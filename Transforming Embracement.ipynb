{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Embracement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 02:53:09.039492 47818327614336 file_utils.py:39] PyTorch version 1.5.1+cu101 available.\n"
     ]
    }
   ],
   "source": [
    "#prelims\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "#standard way to embed\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0., max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], #pe is 3 dimentional, so when we say self.pe[:, :x.size(1)] we set the x.size(1) to the rows of pe, but if x.size(1) is the column shape it will be wrong\n",
    "                             requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_size = ff_size\n",
    "        self.fc1 = nn.Linear(self.d_model, self.ff_size)\n",
    "        self.fc2 = nn.Linear(self.ff_size, self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.leaky_relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    '''\n",
    "    We can think of attention as having a word (query) which we want to look up and find results for.\n",
    "    K and V are considered our \"memory\" where we are looking for the most similar key to the query\n",
    "    Once we find the most similar key, we get its value. Our \"memory\" is all of the previously generated words\n",
    "    so when we pass in x,x,x we are DOT multiplying first word in x (q1) by all words in key, then get a scalar for each,\n",
    "    then pass all scalars through softmax to get a score, then matmul by V to get the average attention vector for all words in x\n",
    "    https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\n",
    "    '''\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h#size of model\n",
    "        self.h = h#number of heads\n",
    "        #could also be d_model, (n_heads * d_k)\n",
    "        self.Wq = nn.Linear(d_model,d_model)\n",
    "        self.Wk = nn.Linear(d_model,d_model)\n",
    "        self.Wv = nn.Linear(d_model,d_model)\n",
    "        self.out = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn_scores = None\n",
    "    def forward(self,q,k,v, mask=None):\n",
    "        '''Figure 2 of paper'''\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        nbatches = q.size(0)\n",
    "        #1)Do all the linear projections in batch from d_model => h x d_k \n",
    "        #project entire model dimentions into h heads\n",
    "        query = self.Wq(q).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        key = self.Wk(k).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        value = self.Wv(v).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        #2) Apply attention on all the projected vectors in batch. \n",
    "        z = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        z = z.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)#a little confused about what contiguous does\n",
    "        return self.out(z)\n",
    "    def attention(self, q,k,v, mask=None, dropout=None):\n",
    "        'Scaled dot product attention'\n",
    "        d_k = q.size(-1)#dimension of each head. We brok original input of 512 into 8 heads where each head is 64\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)#transpose rows and col, where r => c and c=> r\n",
    "        if mask is not None:#ultra important! as we should not attend to padding\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        '''a vector of scores, I believe it softmaxes across keys (words in translation) using same query. \n",
    "        meaning, it will do softmax for each key for every query to get how much attention we must pay to each word'''\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "        self.attn_scores = attn_scores\n",
    "        if dropout is not None:\n",
    "            attn_scores = dropout(attn_scores)\n",
    "        \n",
    "        return torch.matmul(attn_scores, v)# ,attn_scores #spits out z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.!!!\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This layer just calculates self attention + add & norm. This is the first 2 sub blocks of the encoder\n",
    "'''\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, dropout):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.size = size\n",
    "        self.sublayer = SublayerConnection(size, dropout)\n",
    "    def forward(self, x, mask):#this will be our own mask\n",
    "        x = self.sublayer(x, lambda x: self.self_attn(x, x, x, mask))#first half, multihead attn + residual + sum. The lambda is for passing in the layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Modal Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This layer calculates cross attention + add & norm + feedforward + add & norm. This is the Last 4 sub blocks of the decoder\n",
    "'''\n",
    "class CrossModalLayer(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    def __init__(self, size, cross_attn, feed_forward, dropout):\n",
    "        super(CrossModalLayer, self).__init__()\n",
    "        self.cross_attn = cross_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)#these sublayer connections are to create residual connections and layer normalization + dropout\n",
    "        self.size = size\n",
    "    #memory here is the key/val from another modality, opp_mod_mask: opposite modality mask\n",
    "    def forward(self, x, memory, opp_mod_mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.cross_attn(x, memory, memory, opp_mod_mask))# src_mask == encoder-decoder attn\n",
    "        x = self.sublayer[1](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModal(nn.Module):\n",
    "    def __init__(self,self_attn_layer, cross_attn_layer, N):\n",
    "        super(CrossModal, self).__init__()\n",
    "        self.self_attn_layers = clones(self_attn_layer, N)\n",
    "        self.cross_attn_layers = clones(cross_attn_layer, N)\n",
    "        \n",
    "    def forward(self, x, memory, self_mod_mask, opp_mod_mask):#the memory here refers to the output of the encoder.\n",
    "        for self_attn, cross_attn in zip(self.self_attn_layers,self.cross_attn_layers):\n",
    "            x = self_attn(x, self_mod_mask)\n",
    "            x = cross_attn(x, memory, opp_mod_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embracement(nn.Module):\n",
    "    def __init__(self, size, p, model_type):\n",
    "        super(Embracement, self).__init__()\n",
    "        self.c = size\n",
    "        self.p = nn.Parameter(torch.tensor(p))\n",
    "        self.model_type = model_type\n",
    "    def forward(self,X_A, X_B):\n",
    "        #make our random vector of size c and use it as a mask\n",
    "        #r = torch.multinomial(torch.tensor(self.p), self.c, replacement=True).float().to(device)\n",
    "        r = torch.multinomial(self.p, self.c, replacement=True).float().to(device)\n",
    "        r1 = (r == 0.).float().clone().detach().to(device)\n",
    "        r2 = (r == 1.).float().clone().detach().to(device)\n",
    "        #Here we will filter the values\n",
    "        d_prime1 = X_A*r1\n",
    "        d_prime2 = X_B*r2\n",
    "        d_prime = d_prime1 + d_prime2\n",
    "        if self.model_type == 1:#multiplied hidden + embrace\n",
    "            #multiply the 3 vectors together\n",
    "            d_prime = d_prime * X_A * X_B\n",
    "        x = d_prime.view(-1, self.c)\n",
    "        #x = F.softmax(self.fc1(x), dim=0)\n",
    "        return x#this is our z for both modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminalNetwork(nn.Module):#output of model\n",
    "    \"Output layer + softmax\"\n",
    "    def __init__(self, d_model, n_out, dropout = 0.1):#takes in z size, outputs number of classes\n",
    "        super(TerminalNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(d_model, d_model*2)\n",
    "        self.out = nn.Linear(d_model*2, n_out)#shape of embedding -> vocab size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = LayerNorm(d_model*2)\n",
    "    def forward(self, x):\n",
    "        return self.out(self.dropout(self.layernorm(F.leaky_relu(self.hidden(x)))))\n",
    "        #return self.out(x)#F.softmax(self.out(x), dim=-1)#output, Softmax is only needed for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgPredictionHead(nn.Module):\n",
    "    def __init__(self, d_model, v_target_size):\n",
    "        super(ImgPredictionHead, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.v_target_size = v_target_size\n",
    "        \n",
    "        self.layernorm = LayerNorm(d_model)\n",
    "        self.dense = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        self.decoder = nn.Linear(d_model, v_target_size)\n",
    "    def forward(self, hidden_states):\n",
    "        #the transform bit from vilbert#\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.leaky_relu(hidden_states)\n",
    "        hidden_states = self.layernorm(hidden_states)\n",
    "        #the transform bit from vilbert#\n",
    "        #img prediction head#\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LmPredictionHead(nn.Module):\n",
    "    def __init__(self, d_model, embedding_weights):\n",
    "        super(LmPredictionHead, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.layernorm = LayerNorm(d_model)\n",
    "        self.dense = nn.Linear(d_model,d_model)\n",
    "        #prediction head bit#\n",
    "        self.decoder = nn.Linear(embedding_weights.size(1), embedding_weights.size(0), bias=False)\n",
    "        self.decoder.weight = embedding_weights\n",
    "        self.bias = nn.Parameter(torch.zeros(embedding_weights.size(0)))\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        #the transform bit from vilbert#\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.leaky_relu(hidden_states)\n",
    "        hidden_states = self.layernorm(hidden_states)\n",
    "        #the transform bit from vilbert#\n",
    "        #img prediction head#\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, d_model, embedding_weights, v_target_size, dropout):\n",
    "        super(MultimodalPreTrainingHeads, self).__init__()\n",
    "        self.bi_seq_relationship = nn.Linear(d_model, 2)\n",
    "        self.LmPredictions = LmPredictionHead(d_model, embedding_weights)\n",
    "        self.ImgPredictions = ImgPredictionHead(d_model, v_target_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, v_hidden_states, t_hidden_states, v_pooled_output, t_pooled_output):#non pooled inputs, will probably need to change later\n",
    "        prediction_scores_t = self.LmPredictions(t_hidden_states)\n",
    "        prediction_scores_v = self.ImgPredictions(v_hidden_states)\n",
    "        #fuse pooled outputs\n",
    "        pooled_output = self.dropout(t_pooled_output * v_pooled_output)\n",
    "        seq_relationship_score = self.bi_seq_relationship(pooled_output)\n",
    "        return prediction_scores_t, prediction_scores_v, seq_relationship_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertImageEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from image, spatial location (omit now) and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v_feature_size, v_hidden_size, hidden_dropout_prob):\n",
    "        super(BertImageEmbeddings, self).__init__()\n",
    "\n",
    "        self.image_embeddings = nn.Linear(v_feature_size, v_hidden_size)\n",
    "        self.image_location_embeddings = nn.Linear(5, v_hidden_size)\n",
    "        self.LayerNorm = LayerNorm(v_hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, input_loc):\n",
    "\n",
    "        img_embeddings = self.image_embeddings(input_ids)\n",
    "        loc_embeddings = self.image_location_embeddings(input_loc)\n",
    "\n",
    "        # TODO: we want to make the padding_idx == 0, however, with custom initilization, it seems it will have a bias.\n",
    "        # Let's do masking for now\n",
    "        embeddings = self.LayerNorm(img_embeddings + loc_embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from vilbert\n",
    "class BertTextPooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(BertTextPooler, self).__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertImagePooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(BertImagePooler, self).__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this is a nonsequential model, will have two cross modal blocks running at same time\n",
    "I have to share the vector after the self attention portion, maybe within the model I get:\n",
    "2 inputs, each input passes through its own self-attention -> add+norm, then get each vector, and pass it as\n",
    "eachothers key & val as its 'memory', may have to make the module for the crossmodal block to have a \n",
    "self attention method and a cross attention module\n",
    "'''\n",
    "\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    \"Full model definition\"\n",
    "    def __init__(self,crossmodal_A, crossmodal_B, \n",
    "                 embracement, terminal, embed_image, embed_text, img_pooler, txt_pooler, PretrainingHead,\n",
    "                task=0, model_type=1):\n",
    "        super(MultimodalTransformer, self).__init__()\n",
    "        #cross modal transformer sub blocks\n",
    "        self.crossmodal_A = crossmodal_A\n",
    "        self.crossmodal_B = crossmodal_B\n",
    "        self.embrace = embracement\n",
    "        self.terminal = terminal\n",
    "        self.embed_image = embed_image\n",
    "        self.embed_text = embed_text\n",
    "        self.img_pooler = img_pooler\n",
    "        self.txt_pooler = txt_pooler\n",
    "        self.PretrainingHead = PretrainingHead\n",
    "        self.task = task\n",
    "        self.model_type = model_type\n",
    "        if self.task == 0:#pretraining\n",
    "            self.embed_text[0].embed.weight = self.PretrainingHead.LmPredictions.decoder.weight\n",
    "    def forward(self, x_a, x_b, X_A_mask, X_B_mask):#a and b are modalities A and B\n",
    "        #perform self attention on modalities A and B\n",
    "        \n",
    "        X_A = self.embed_image(*x_a)\n",
    "        X_B = self.embed_text(x_b)\n",
    "        #perform cross modal attention\n",
    "        X_A = self.crossmodal_A(X_A, X_B, X_A_mask, X_B_mask)#modality A gets modality B as memory, pass in both masks\n",
    "        X_B = self.crossmodal_B(X_B, X_A, X_B_mask, X_A_mask)#modality B gets modality A as memory\n",
    "        pooled_X_A = self.img_pooler(X_A)\n",
    "        pooled_X_B = self.txt_pooler(X_B)\n",
    "        if self.task == 1:#classification\n",
    "            if self.model_type == 2:#no embrace\n",
    "                Z = pooled_X_A*pooled_X_B#just multiply them  \n",
    "            else:\n",
    "                Z = self.embrace(pooled_X_A, pooled_X_B)#our embraced vector\n",
    "            Z = self.terminal(Z)#give us output\n",
    "        if self.task == 0:#pretraining\n",
    "            prediction_scores_t, prediction_scores_v, seq_relationship_score = self.PretrainingHead(X_A, X_B,pooled_X_A, pooled_X_B )\n",
    "            Z = (prediction_scores_t, prediction_scores_v, seq_relationship_score)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(d_model = 128, ff_size = 128, src_vocab = 2000, n_out = 2, n_head = 1, Nx = 1, dropout = 0.1, task = 1, freeze_FE_layers = False, model_type = 1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_head,d_model)\n",
    "    ff = FeedForward(d_model, ff_size=ff_size)\n",
    "    self_attn_A = SelfAttentionLayer(d_model, c(attn), dropout)\n",
    "    crossmodal_A = CrossModal(self_attn_A, CrossModalLayer(d_model, c(attn), c(ff), dropout=dropout),Nx)\n",
    "    \n",
    "    self_attn_B = SelfAttentionLayer(d_model, c(attn), dropout)\n",
    "    crossmodal_B = CrossModal(self_attn_B, CrossModalLayer(d_model, c(attn), c(ff), dropout=dropout),Nx)\n",
    "    \n",
    "    position = PositionalEncoder(d_model=d_model, dropout=dropout)#max_seq_len must be larger than the longest sequence!\n",
    "    #embeddings are placed in sequential nn so we can train them, we also place the positional encoder so when we pass the input -> emb -> Pos\n",
    "    embed_B = nn.Sequential(Embedder(src_vocab, d_model),c(position))#torch.rand(200, d_model)#this is temporary\n",
    "    embed_A = BertImageEmbeddings(v_feature_size=1024, v_hidden_size=d_model, hidden_dropout_prob=dropout)#v_feature_size is fixed due to feature extractor\n",
    "    txt_pooler = BertTextPooler(d_model)\n",
    "    img_pooler = BertImagePooler(d_model)\n",
    "    if task == 1:#classification\n",
    "        PT_Head = None\n",
    "        tn = TerminalNetwork(d_model, n_out)\n",
    "        if model_type == 2:\n",
    "            embracement = None\n",
    "        else:\n",
    "            embracement = Embracement(d_model, [0.50,0.50], model_type=model_type)\n",
    "    elif task == 0:\n",
    "        PT_Head = MultimodalPreTrainingHeads(d_model, embed_B[0].embed.weight, n_out, dropout)\n",
    "        tn = None\n",
    "        embracement = None\n",
    "    model = MultimodalTransformer(crossmodal_A, crossmodal_B, embracement,\n",
    "                                  tn, embed_image=embed_A, embed_text=embed_B, img_pooler = img_pooler, \n",
    "                                  txt_pooler=txt_pooler,PretrainingHead = PT_Head, task = task, model_type=model_type)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    if freeze_FE_layers:#freeze all layers\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in model.terminal.parameters():#unfreeze terminal network!\n",
    "            p.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Datasets (features must be extracted using the \"RSNA Pneumonia\" notebook from MIMIC-CXR-JPG prior to using):\n",
    " - MIMIC-CXR-JPG: https://physionet.org/content/mimic-cxr-jpg/2.0.0/\n",
    " - MIMIC-CXR (reports): https://physionet.org/content/mimic-cxr/2.0.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset will be coming from already extracted features in .npy files.\n",
    "    Args:\n",
    "        csv_file (string): Contains information about original image and captions\n",
    "        root_dir (string): location to .npy files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, max_seq_length=128, max_image_feats=128):\n",
    "        self.max_image_feats = max_image_feats\n",
    "        self.max_seq_length = max_seq_length#max length of tokens\n",
    "        self.dataset_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.img_feats = pd.DataFrame(list(Path(root_dir).rglob(\"*.npy\")), columns=['feature_path'])#list of features per image\n",
    "        self.img_feats['feature_path'] = self.img_feats['feature_path'].apply(lambda x: str(x))\n",
    "        self.img_feats['dicom_id'] = self.img_feats['feature_path'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "        self.dataset_frame = pd.merge(self.dataset_frame, self.img_feats, on='dicom_id')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "        #one hot encode\n",
    "        #target = self.dataset_frame.y\n",
    "        self.n_classes = self.dataset_frame.y.nunique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_frame)\n",
    "    def random_region(self,image_feat, num_boxes):\n",
    "        \"\"\"\n",
    "        Mask a random region of the image masked\n",
    "        \"\"\"\n",
    "        output_label = []\n",
    "        masked_label = np.zeros((image_feat.shape[0]))\n",
    "\n",
    "        for i in range(num_boxes):\n",
    "            prob = random.random()\n",
    "            # mask token with 15% probability\n",
    "            #of these masked tokens, we mask the features with 90% prob (may be bad for samples with small num of feats)\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.9:\n",
    "                    image_feat[i] = 0\n",
    "                # append current token to output (we will predict these later)\n",
    "                output_label.append(1)\n",
    "            else:\n",
    "                # no masking token (will be ignored by loss function later)\n",
    "                output_label.append(-1)\n",
    "\n",
    "        return image_feat, output_label\n",
    "    \n",
    "    def random_word(self, tokens):\n",
    "        output_label = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.tokenizer.all_special_ids:\n",
    "                continue\n",
    "            prob = random.random()\n",
    "            # mask token with 15% probability\n",
    "\n",
    "            # if is_next == 1 and self.objective != 0:\n",
    "            #     prob = 1 # not sample mask\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = np.random.randint(len(self.tokenizer))\n",
    "                    # torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "\n",
    "                # -> rest 10% randomly keep current token\n",
    "                # append current token to output (we will predict these later)\n",
    "                output_label.append(token)\n",
    "            else:\n",
    "                # no masking token (will be ignored by loss function later)\n",
    "                output_label.append(-1)\n",
    "\n",
    "        return tokens, output_label\n",
    "    def get_random_caption(self):\n",
    "        \"\"\"\n",
    "        Get random caption from another document for nextSentence task.\n",
    "        :return: str, content of one line\n",
    "        \"\"\"\n",
    "        # add the hard negative mining objective here.\n",
    "        rand_doc_idx = random.randint(0, len(self.dataset_frame) - 1)\n",
    "        caption = self.dataset_frame.iloc[rand_doc_idx]['report']\n",
    "\n",
    "        return caption\n",
    "    def random_cap(self, caption):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n",
    "        from one doc. With 50% the second sentence will be a random one from another doc.\n",
    "        :param index: int, index of sample.\n",
    "        :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n",
    "        \"\"\"\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            caption = self.get_random_caption()\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        return caption, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #load features from disk\n",
    "        features = np.load(self.dataset_frame.iloc[idx]['feature_path'], allow_pickle=True).item()\n",
    "        cls_probs = features['cls_prob']\n",
    "        caption = self.dataset_frame.iloc[idx]['report']\n",
    "        next_sentence_label = []\n",
    "        if task == 0:#pretraining\n",
    "            caption, next_sentence_label = self.random_cap(caption)\n",
    "        tokenized = self.tokenizer(text=caption, add_special_tokens=True, max_length=self.max_seq_length,truncation=True,pad_to_max_length=True,)\n",
    "        text = torch.tensor(tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        masked_tokens, masked_lm_labels = self.random_word(tokenized[\"input_ids\"])\n",
    "\n",
    "        masked_text = torch.tensor(masked_tokens, dtype=torch.long)\n",
    "        pad_len = self.max_seq_length-len(masked_lm_labels)\n",
    "        masked_lm_labels = torch.tensor(masked_lm_labels + [-1]*pad_len, dtype=torch.long)\n",
    "        \n",
    "        text_attn_mask = torch.tensor(tokenized[\"attention_mask\"])\n",
    "        #text_attn_mask = torch.stack([torch.tensor(text_attn_mask)], dim=0).squeeze(0)\n",
    "        image_id = features['image_id']\n",
    "        image_w = features['image_width']\n",
    "        image_h = features['image_height']\n",
    "        feature = torch.from_numpy(features['features'])\n",
    "        num_boxes = features['num_boxes']\n",
    "        if task == 0:#pretraining\n",
    "            feature, image_label = self.random_region(feature, num_boxes)#image_label is the \"mask\" for the image.\n",
    "        else:#give image label ones to predict it\n",
    "            image_label =[-1]*num_boxes\n",
    "        \n",
    "        g_feat = torch.sum(feature, dim=0) / num_boxes\n",
    "        g_box = torch.tensor([[0.,0.,0.,0.]])\n",
    "        num_boxes = num_boxes + 1\n",
    "        feature = torch.cat([g_feat.view(1,-1), feature], dim=0)\n",
    "        boxes = torch.from_numpy(features['bbox'])\n",
    "        boxes = torch.cat([g_box,boxes])\n",
    "        image_location = np.zeros((boxes.shape[0], 5), dtype=np.float32)\n",
    "        image_location[:,:4] = boxes\n",
    "        image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "        image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "        image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "        image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "        image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "        g_location = np.array([0,0,1,1,1])\n",
    "        image_location = np.concatenate([np.expand_dims(g_location, axis=0), image_location], axis=0)\n",
    "        \n",
    "        \n",
    "        #the stacked features probably need to be padded!!!!!!!\n",
    "        #padding added\n",
    "        img_feat_padding = torch.zeros((self.max_image_feats-feature.size(0)),feature.size(1))\n",
    "        img_attn_mask = torch.tensor([1]*len(feature) + [0]*len(img_feat_padding))\n",
    "        #img_attn_mask = torch.stack([torch.tensor(img_attn_mask)], dim=0).squeeze(0)\n",
    "        #print('img_attn_mask')\n",
    "        #print(img_attn_mask)\n",
    "        image_label = torch.tensor([-1]+image_label+[-1]*len(img_feat_padding))\n",
    "        \n",
    "        g_cls_prob = sum(cls_probs)/len(cls_probs)#avg probabilies that it is a pneumonia sample?\n",
    "        \n",
    "        feature = torch.cat([feature, img_feat_padding], axis=0)\n",
    "        features = torch.stack([feature], dim=0).float().squeeze(0)\n",
    "        location = torch.tensor(image_location).float()\n",
    "        location = torch.cat([location, torch.zeros(self.max_image_feats-location.size(0),location.size(1))], axis=0)\n",
    "        \n",
    "        cls_dist = torch.cat([torch.tensor([g_cls_prob]).float(), torch.from_numpy(cls_probs), torch.zeros(img_feat_padding.size(0))], axis=0)\n",
    "        \n",
    "        cls_dist = torch.tensor([[1-prob, prob] for prob in cls_dist])\n",
    "        cls_dist = torch.stack([cls_dist], dim=0).float().squeeze(0)\n",
    "        \n",
    "        #image_label = torch.cat([image_label, (img_feat_padding-1)], axis=0)#append -1 for padding as we want the loss to ignore pad\n",
    "        image_label = torch.stack([image_label], dim=0).float().squeeze(0)\n",
    "        #print(location)\n",
    "        spatials = torch.stack([location], dim=0).float().squeeze(0)\n",
    "        #image_mask = torch.stack([torch.tensor(image_mask)], dim=0).byte()\n",
    "        bboxes = torch.cat([boxes, torch.zeros(self.max_image_feats-boxes.size(0),boxes.size(1))], axis=0)\n",
    "        bboxes = torch.stack([bboxes], dim=0).float().squeeze(0)\n",
    "        y = self.dataset_frame.iloc[idx]['y']\n",
    "        ambiguous = self.dataset_frame.iloc[idx]['ambiguous']\n",
    "        output = {'features': features, 'spatials': spatials, 'text': text, 'boxes': bboxes, \n",
    "                  'image_id':image_id,'width':image_w, 'height':image_h, 'text_attn_mask':text_attn_mask, \n",
    "                  'img_attn_mask':img_attn_mask, 'cls_dist':cls_dist,'image_label': image_label,\n",
    "                  'masked_text':masked_text,'masked_lm_labels':masked_lm_labels,'next_sentence_label':next_sentence_label, 'ambiguous':ambiguous, 'y':y}\n",
    "\n",
    "        return output#features, spatials, text, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'useAmbiguous' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-76f6a50f8569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m '''\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0museAmbiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#root dirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/kl533/krauthammer_partition/Weights/transforming_embracement/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'useAmbiguous' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data comes from feature extractor & splits. Feature extractor gives features in .npy file (features, bbox, spatials, class, probs) and \n",
    "csv file containing:\n",
    "    dicom_id\n",
    "    feature_path\n",
    "    report\n",
    "    report_path\n",
    "    Pneumonia/not Pneumonia\n",
    "\n",
    "'''\n",
    "if not useAmbiguous:\n",
    "    #root dirs\n",
    "    root_dir = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/'\n",
    "    mimic_loc = '/home/kl533/krauthammer_partition/mimic-cxr-jpg/mimic-cxr-jpg-2.0.0.physionet.org/'\n",
    "    #load csv into DF\n",
    "    df = pd.read_csv(os.path.join(root_dir, 'img_infos.csv'), index_col=0)\n",
    "    splits = pd.read_csv(os.path.join(mimic_loc, \"mimic-cxr-2.0.0-split.csv\"))\n",
    "\n",
    "    splits = splits[['dicom_id','split']]\n",
    "    df_splits = pd.merge(df, splits, on='dicom_id')\n",
    "    train = df_splits[df_splits['split']=='train']\n",
    "    validate = df_splits[df_splits['split']=='validate']\n",
    "    test = df_splits[df_splits['split']=='test']\n",
    "\n",
    "    #save dataframes into train, valid, test\n",
    "    train.to_csv('/home/kl533/krauthammer_partition/Weights/transforming_embracement/img_infos_TRAIN.csv')\n",
    "    validate.to_csv('/home/kl533/krauthammer_partition/Weights/transforming_embracement/img_infos_VALIDATE.csv')\n",
    "    test.to_csv('/home/kl533/krauthammer_partition/Weights/transforming_embracement/img_infos_TEST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training, valid, test sets and merge with neg 1 annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make grand list of all samples, then get all sample ids, then split via sample ids'''\n",
    "#load ambiguous\n",
    "root_dir = '/gpfs/ysm/home/kl533/Lab/Transformers/transforming_embracement/pneumonia_neg_1_feats/'\n",
    "neg_1 = os.path.join(root_dir, 'img_infos.csv')\n",
    "ambiguous = pd.read_csv(neg_1, index_col=0)\n",
    "ambiguous['ambiguous'] = 1\n",
    "#load unambiguous\n",
    "root_dir = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/'\n",
    "mimic_loc = '/home/kl533/krauthammer_partition/mimic-cxr-jpg/mimic-cxr-jpg-2.0.0.physionet.org/'\n",
    "#load csv into DF\n",
    "unambiguous = pd.read_csv(os.path.join(root_dir, 'img_infos.csv'), index_col=0)\n",
    "unambiguous['ambiguous'] = 0\n",
    "#get all sample ids for full dataset\n",
    "metadata = pd.read_csv(os.path.join(mimic_loc, \"mimic-cxr-2.0.0-metadata.csv\"))\n",
    "full_set = unambiguous.append(ambiguous, sort=False)\n",
    "full_set = pd.merge(full_set, metadata[['dicom_id','subject_id']], on='dicom_id')#gives us all samples with subject ids\n",
    "\n",
    "#split the data apart keeping subject_ids that are ambiguous\n",
    "#since subject ids from ambiguous may also be in unambiguous, we need to remove those subject ids from unabmiguous\n",
    "ambiguous_subject_ids = full_set[full_set['ambiguous'] == 1]['subject_id'].unique().tolist()#get sample ids that are ambiguous\n",
    "unambiguous_subject_ids = full_set[~full_set['subject_id'].isin(ambiguous_subject_ids)]['subject_id'].unique().tolist()\n",
    "random.shuffle(ambiguous_subject_ids)\n",
    "random.shuffle(unambiguous_subject_ids)\n",
    "#make splits, 2 different ones unambiguous splits, and ambiguous splits\n",
    "#want to make sure 80% of ambiguous goes into training subject ids\n",
    "#this is so we have more samples that are ambiguous in our train\n",
    "\n",
    "train_sbjct_id, validate_sbjct_id, test_sbjct_id = np.split(unambiguous_subject_ids, \n",
    "                                                            [int(.8*len(unambiguous_subject_ids)), \n",
    "                                                             int(.9*len(unambiguous_subject_ids))])\n",
    "\n",
    "train_amb_sbjct_id, validate_amb_sbjct_id, test_amb_sbjct_id = np.split(ambiguous_subject_ids, \n",
    "                                                                        [int(.8*len(ambiguous_subject_ids)), \n",
    "                                                                         int(.9*len(ambiguous_subject_ids))])\n",
    "#append the ambiguous and unambiguous subject ids\n",
    "train_sbjct_id = np.append(train_sbjct_id, train_amb_sbjct_id, 0)\n",
    "validate_sbjct_id = np.append(validate_sbjct_id, validate_amb_sbjct_id, 0)\n",
    "test_sbjct_id = np.append(test_sbjct_id, test_amb_sbjct_id, 0)\n",
    "#finally make our split dataframe\n",
    "train = full_set[full_set['subject_id'].isin(train_sbjct_id)]\n",
    "valid = full_set[full_set['subject_id'].isin(validate_sbjct_id)]\n",
    "test = full_set[full_set['subject_id'].isin(test_sbjct_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "checking if subjects from train are in valid and test\n",
      "0\n",
      "0\n",
      "checking if subjects from valid are in test\n",
      "0\n",
      "checking if study_ids from train are in valid and test\n",
      "0\n",
      "0\n",
      "checking if study_id from valid are in test\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check for leakage\n",
    "metadata = pd.read_csv(os.path.join(mimic_loc, \"mimic-cxr-2.0.0-metadata.csv\"))\n",
    "train_dicom_id = train['dicom_id'].tolist()\n",
    "valid_dicom_id = valid['dicom_id'].tolist()\n",
    "test_dicom_id = test['dicom_id'].tolist()\n",
    "print(set(train_dicom_id).intersection(valid_dicom_id))\n",
    "print(set(train_dicom_id).intersection(test_dicom_id))\n",
    "\n",
    "train_subject_id = metadata[metadata['dicom_id'].isin(train_dicom_id)]['subject_id'].tolist()\n",
    "valid_subject_id = metadata[metadata['dicom_id'].isin(valid_dicom_id)]['subject_id'].tolist()\n",
    "test_subject_id = metadata[metadata['dicom_id'].isin(test_dicom_id)]['subject_id'].tolist()\n",
    "print('checking if subjects from train are in valid and test')\n",
    "print(len(set(train_subject_id).intersection(valid_subject_id)))\n",
    "print(len(set(train_subject_id).intersection(test_subject_id)))\n",
    "\n",
    "print('checking if subjects from valid are in test')\n",
    "print(len(set(valid_subject_id).intersection(test_subject_id)))\n",
    "\n",
    "train_study_id = metadata[metadata['dicom_id'].isin(train_dicom_id)]['study_id'].tolist()\n",
    "valid_study_id = metadata[metadata['dicom_id'].isin(valid_dicom_id)]['study_id'].tolist()\n",
    "test_study_id = metadata[metadata['dicom_id'].isin(test_dicom_id)]['study_id'].tolist()\n",
    "print('checking if study_ids from train are in valid and test')\n",
    "print(len(set(train_study_id).intersection(valid_study_id)))\n",
    "print(len(set(train_study_id).intersection(test_study_id)))\n",
    "\n",
    "print('checking if study_id from valid are in test')\n",
    "print(len(set(valid_subject_id).intersection(test_study_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30495"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train['ambiguous'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n",
      "29668\n"
     ]
    }
   ],
   "source": [
    "print(sum(train['y'] == 1))\n",
    "print(sum(train['y'] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train, valid, and test\n",
    "home_dir = '/gpfs/ysm/home/kl533/Lab/Transformers/transforming_embracement/'\n",
    "train.to_csv(os.path.join(home_dir, 'img_infos_TRAIN.csv'))\n",
    "valid.to_csv(os.path.join(home_dir, 'img_infos_VALIDATE.csv'))\n",
    "test.to_csv(os.path.join(home_dir, 'img_infos_TEST.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 02:53:23.493977 47818327614336 tokenization_utils_base.py:1233] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kl533/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0826 02:53:24.095016 47818327614336 tokenization_utils_base.py:1233] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kl533/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0826 02:53:24.731545 47818327614336 tokenization_utils_base.py:1233] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kl533/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "home_dir = '/gpfs/ysm/home/kl533/Lab/Transformers/transforming_embracement/'\n",
    "root_dir = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/'\n",
    "train_csv = os.path.join(home_dir, 'img_infos_TRAIN.csv')\n",
    "valid_csv = os.path.join(home_dir, 'img_infos_VALIDATE.csv')\n",
    "test_csv = os.path.join(home_dir, 'img_infos_TEST.csv')\n",
    "\n",
    "mimic_feature_dataset_train = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=train_csv,root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_train = DataLoader(mimic_feature_dataset_train, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "mimic_feature_dataset_valid = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=valid_csv, root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_valid = DataLoader(mimic_feature_dataset_valid, batch_size=24, shuffle=True, num_workers=4)\n",
    "\n",
    "mimic_feature_dataset_test = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=test_csv, root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_test = DataLoader(mimic_feature_dataset_test, batch_size=24, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_feature_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a model with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=8, Nx=6, d_model=256, ff_size=1024, dropout=0.1, task=task,freeze_FE_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21015102"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get number of parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attn_score = torch.rand(2,2,5,5)\\nattn_mask = torch.tensor([[1,1,1,0,0],[1,1,0,0,0]])\\nprint(attn_score.size())\\nprint(attn_score[0][0])\\nattn_mask = attn_mask.unsqueeze(1).unsqueeze(2)\\nprint(attn_score.size())\\n### Cross attention\\ncross_attn_score = torch.rand(2,2,10,5)#(batch, head, txt_tokens, img_tokens), this is in the text decoders block\\nimg_attn_mask = torch.tensor([[1,1,1,1,0],[1,0,0,0,0]])#b0: 4 feats, b1: 1 feat...\\nprint(cross_attn_score[0][0])\\nprint(cross_attn_score.size())\\nimg_attn_mask = img_attn_mask.unsqueeze(1).unsqueeze(2)\\ncross_attn_score.masked_fill(img_attn_mask == 0, -1e9)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''attn_score = torch.rand(2,2,5,5)\n",
    "attn_mask = torch.tensor([[1,1,1,0,0],[1,1,0,0,0]])\n",
    "print(attn_score.size())\n",
    "print(attn_score[0][0])\n",
    "attn_mask = attn_mask.unsqueeze(1).unsqueeze(2)\n",
    "print(attn_score.size())\n",
    "### Cross attention\n",
    "cross_attn_score = torch.rand(2,2,10,5)#(batch, head, txt_tokens, img_tokens), this is in the text decoders block\n",
    "img_attn_mask = torch.tensor([[1,1,1,1,0],[1,0,0,0,0]])#b0: 4 feats, b1: 1 feat...\n",
    "print(cross_attn_score[0][0])\n",
    "print(cross_attn_score.size())\n",
    "img_attn_mask = img_attn_mask.unsqueeze(1).unsqueeze(2)\n",
    "cross_attn_score.masked_fill(img_attn_mask == 0, -1e9)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Find learning rate. Not implemented yet\\nfrom torch_lr_finder import LRFinder\\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\\nlr_finder.range_test(mimic_feature_dataloader_train, val_loader=mimic_feature_dataloader_valid, end_lr=1, num_iter=100, step_mode=\"linear\")\\nlr_finder.plot(log_lr=False)\\nlr_finder.reset()'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Find learning rate. Not implemented yet\n",
    "from torch_lr_finder import LRFinder\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(mimic_feature_dataloader_train, val_loader=mimic_feature_dataloader_valid, end_lr=1, num_iter=100, step_mode=\"linear\")\n",
    "lr_finder.plot(log_lr=False)\n",
    "lr_finder.reset()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain the model for Visual-Linguistic grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ysm/project/kl533/conda_envs/dlnn/lib/python3.6/site-packages/torch/nn/functional.py:2247: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 13.213\n",
      "[1,   200] loss: 8.788\n",
      "[1,   300] loss: 7.699\n",
      "[1,   400] loss: 7.055\n",
      "[1,   500] loss: 6.689\n",
      "[1,   600] loss: 6.571\n",
      "[1,   700] loss: 6.520\n",
      "[1,   800] loss: 6.500\n",
      "[1,   900] loss: 6.479\n",
      "[1,  1000] loss: 6.402\n",
      "[1,  1100] loss: 6.449\n",
      "[1,  1200] loss: 6.423\n",
      "[1,  1300] loss: 6.434\n",
      "[1,  1400] loss: 6.417\n",
      "[1,  1500] loss: 6.396\n",
      "[2,   100] loss: 6.428\n",
      "[2,   200] loss: 6.374\n",
      "[2,   300] loss: 6.383\n",
      "[2,   400] loss: 6.383\n",
      "[2,   500] loss: 6.367\n",
      "[2,   600] loss: 6.391\n",
      "[2,   700] loss: 6.384\n",
      "[2,   800] loss: 6.361\n",
      "[2,   900] loss: 6.357\n",
      "[2,  1000] loss: 6.317\n",
      "[2,  1100] loss: 6.350\n",
      "[2,  1200] loss: 6.283\n",
      "[2,  1300] loss: 6.201\n",
      "[2,  1400] loss: 6.176\n",
      "[2,  1500] loss: 6.141\n",
      "[3,   100] loss: 6.121\n",
      "[3,   200] loss: 6.019\n",
      "[3,   300] loss: 6.003\n",
      "[3,   400] loss: 5.988\n",
      "[3,   500] loss: 5.956\n",
      "[3,   600] loss: 5.875\n",
      "[3,   700] loss: 5.845\n",
      "[3,   800] loss: 5.824\n",
      "[3,   900] loss: 5.794\n",
      "[3,  1000] loss: 5.774\n",
      "[3,  1100] loss: 5.707\n",
      "[3,  1200] loss: 5.688\n",
      "[3,  1300] loss: 5.969\n",
      "[3,  1400] loss: 5.717\n",
      "[3,  1500] loss: 5.668\n",
      "[4,   100] loss: 5.581\n",
      "[4,   200] loss: 5.629\n",
      "[4,   300] loss: 5.552\n",
      "[4,   400] loss: 5.570\n",
      "[4,   500] loss: 5.524\n",
      "[4,   600] loss: 5.454\n",
      "[4,   700] loss: 5.432\n",
      "[4,   800] loss: 5.365\n",
      "[4,   900] loss: 5.312\n",
      "[4,  1000] loss: 5.248\n",
      "[4,  1100] loss: 5.178\n",
      "[4,  1200] loss: 5.113\n",
      "[4,  1300] loss: 5.035\n",
      "[4,  1400] loss: 5.214\n",
      "[4,  1500] loss: 5.073\n",
      "[5,   100] loss: 4.945\n",
      "[5,   200] loss: 4.901\n",
      "[5,   300] loss: 4.803\n",
      "[5,   400] loss: 4.813\n",
      "[5,   500] loss: 4.716\n",
      "[5,   600] loss: 4.632\n",
      "[5,   700] loss: 4.604\n",
      "[5,   800] loss: 4.630\n",
      "[5,   900] loss: 4.566\n",
      "[5,  1000] loss: 4.501\n",
      "[5,  1100] loss: 4.495\n",
      "[5,  1200] loss: 4.435\n",
      "[5,  1300] loss: 4.455\n",
      "[5,  1400] loss: 4.433\n",
      "[5,  1500] loss: 4.468\n",
      "[6,   100] loss: 4.324\n",
      "[6,   200] loss: 4.315\n",
      "[6,   300] loss: 4.260\n",
      "[6,   400] loss: 4.264\n",
      "[6,   500] loss: 4.225\n",
      "[6,   600] loss: 4.182\n",
      "[6,   700] loss: 4.214\n",
      "[6,   800] loss: 4.187\n",
      "[6,   900] loss: 4.155\n",
      "[6,  1000] loss: 4.081\n",
      "[6,  1100] loss: 4.088\n",
      "[6,  1200] loss: 4.104\n",
      "[6,  1300] loss: 4.047\n",
      "[6,  1400] loss: 4.012\n",
      "[6,  1500] loss: 3.982\n",
      "[7,   100] loss: 3.977\n",
      "[7,   200] loss: 3.919\n",
      "[7,   300] loss: 3.895\n",
      "[7,   400] loss: 3.906\n",
      "[7,   500] loss: 3.854\n",
      "[7,   600] loss: 3.862\n",
      "[7,   700] loss: 3.860\n",
      "[7,   800] loss: 3.977\n",
      "[7,   900] loss: 3.827\n",
      "[7,  1000] loss: 3.800\n",
      "[7,  1100] loss: 3.730\n",
      "[7,  1200] loss: 3.752\n",
      "[7,  1300] loss: 3.717\n",
      "[7,  1400] loss: 3.681\n",
      "[7,  1500] loss: 3.679\n",
      "[8,   100] loss: 3.694\n",
      "[8,   200] loss: 3.676\n",
      "[8,   300] loss: 3.620\n",
      "[8,   400] loss: 3.600\n",
      "[8,   500] loss: 3.620\n",
      "[8,   600] loss: 3.584\n",
      "[8,   700] loss: 3.705\n",
      "[8,   800] loss: 3.613\n",
      "[8,   900] loss: 3.570\n",
      "[8,  1000] loss: 3.553\n",
      "[8,  1100] loss: 3.521\n",
      "[8,  1200] loss: 3.510\n",
      "[8,  1300] loss: 3.472\n",
      "[8,  1400] loss: 3.471\n",
      "[8,  1500] loss: 3.466\n",
      "[9,   100] loss: 3.431\n",
      "[9,   200] loss: 3.445\n",
      "[9,   300] loss: 3.375\n",
      "[9,   400] loss: 3.375\n",
      "[9,   500] loss: 3.342\n",
      "[9,   600] loss: 3.328\n",
      "[9,   700] loss: 3.307\n",
      "[9,   800] loss: 3.312\n",
      "[9,   900] loss: 3.342\n",
      "[9,  1000] loss: 3.270\n",
      "[9,  1100] loss: 3.274\n",
      "[9,  1200] loss: 3.277\n",
      "[9,  1300] loss: 3.232\n",
      "[9,  1400] loss: 3.242\n",
      "[9,  1500] loss: 3.224\n",
      "[10,   100] loss: 3.190\n",
      "[10,   200] loss: 3.188\n",
      "[10,   300] loss: 3.129\n",
      "[10,   400] loss: 3.172\n",
      "[10,   500] loss: 3.125\n",
      "[10,   600] loss: 3.104\n",
      "[10,   700] loss: 3.165\n",
      "[10,   800] loss: 3.096\n",
      "[10,   900] loss: 3.197\n",
      "[10,  1000] loss: 3.095\n",
      "[10,  1100] loss: 3.037\n",
      "[10,  1200] loss: 3.055\n",
      "[10,  1300] loss: 3.070\n",
      "[10,  1400] loss: 3.043\n",
      "[10,  1500] loss: 2.975\n",
      "[11,   100] loss: 3.024\n",
      "[11,   200] loss: 2.978\n",
      "[11,   300] loss: 2.936\n",
      "[11,   400] loss: 2.988\n",
      "[11,   500] loss: 2.939\n",
      "[11,   600] loss: 2.922\n",
      "[11,   700] loss: 2.956\n",
      "[11,   800] loss: 2.905\n",
      "[11,   900] loss: 2.888\n",
      "[11,  1000] loss: 2.898\n",
      "[11,  1100] loss: 2.887\n",
      "[11,  1200] loss: 2.831\n",
      "[11,  1300] loss: 2.875\n",
      "[11,  1400] loss: 2.840\n",
      "[11,  1500] loss: 2.797\n",
      "[12,   100] loss: 2.804\n",
      "[12,   200] loss: 2.828\n",
      "[12,   300] loss: 2.820\n",
      "[12,   400] loss: 2.775\n",
      "[12,   500] loss: 2.761\n",
      "[12,   600] loss: 2.764\n",
      "[12,   700] loss: 2.768\n",
      "[12,   800] loss: 2.750\n",
      "[12,   900] loss: 2.775\n",
      "[12,  1000] loss: 2.734\n",
      "[12,  1100] loss: 2.697\n",
      "[12,  1200] loss: 2.701\n",
      "[12,  1300] loss: 2.686\n",
      "[12,  1400] loss: 2.694\n",
      "[12,  1500] loss: 2.698\n",
      "[13,   100] loss: 2.682\n",
      "[13,   200] loss: 2.659\n",
      "[13,   300] loss: 2.639\n",
      "[13,   400] loss: 2.644\n",
      "[13,   500] loss: 2.616\n",
      "[13,   600] loss: 2.626\n",
      "[13,   700] loss: 2.611\n",
      "[13,   800] loss: 2.616\n",
      "[13,   900] loss: 2.560\n",
      "[13,  1000] loss: 2.597\n",
      "[13,  1100] loss: 2.571\n",
      "[13,  1200] loss: 2.569\n",
      "[13,  1300] loss: 2.577\n",
      "[13,  1400] loss: 2.531\n",
      "[13,  1500] loss: 2.526\n",
      "[14,   100] loss: 2.535\n",
      "[14,   200] loss: 2.519\n",
      "[14,   300] loss: 2.520\n",
      "[14,   400] loss: 2.530\n",
      "[14,   500] loss: 2.439\n",
      "[14,   600] loss: 2.525\n",
      "[14,   700] loss: 2.512\n",
      "[14,   800] loss: 2.513\n",
      "[14,   900] loss: 2.485\n",
      "[14,  1000] loss: 2.457\n",
      "[14,  1100] loss: 2.512\n",
      "[14,  1200] loss: 2.481\n",
      "[14,  1300] loss: 2.458\n",
      "[14,  1400] loss: 2.467\n",
      "[14,  1500] loss: 2.430\n",
      "[15,   100] loss: 2.415\n",
      "[15,   200] loss: 2.420\n",
      "[15,   300] loss: 2.453\n",
      "[15,   400] loss: 2.452\n",
      "[15,   500] loss: 2.383\n",
      "[15,   600] loss: 2.409\n",
      "[15,   700] loss: 2.388\n",
      "[15,   800] loss: 2.380\n",
      "[15,   900] loss: 2.391\n",
      "[15,  1000] loss: 2.420\n",
      "[15,  1100] loss: 2.360\n",
      "[15,  1200] loss: 2.363\n",
      "[15,  1300] loss: 2.361\n",
      "[15,  1400] loss: 2.394\n",
      "[15,  1500] loss: 2.375\n",
      "[16,   100] loss: 2.345\n",
      "[16,   200] loss: 2.399\n",
      "[16,   300] loss: 2.333\n",
      "[16,   400] loss: 2.322\n",
      "[16,   500] loss: 2.364\n",
      "[16,   600] loss: 2.342\n",
      "[16,   700] loss: 2.321\n",
      "[16,   800] loss: 2.306\n",
      "[16,   900] loss: 2.366\n",
      "[16,  1000] loss: 2.252\n",
      "[16,  1100] loss: 2.299\n",
      "[16,  1200] loss: 2.291\n",
      "[16,  1300] loss: 2.295\n",
      "[16,  1400] loss: 2.289\n",
      "[16,  1500] loss: 2.266\n",
      "[17,   100] loss: 2.295\n",
      "[17,   200] loss: 2.258\n",
      "[17,   300] loss: 2.245\n",
      "[17,   400] loss: 2.251\n",
      "[17,   500] loss: 2.263\n",
      "[17,   600] loss: 2.231\n",
      "[17,   700] loss: 2.278\n",
      "[17,   800] loss: 2.270\n",
      "[17,   900] loss: 2.239\n",
      "[17,  1000] loss: 2.208\n",
      "[17,  1100] loss: 2.236\n",
      "[17,  1200] loss: 2.233\n",
      "[17,  1300] loss: 2.219\n",
      "[17,  1400] loss: 2.213\n",
      "[17,  1500] loss: 2.212\n",
      "[18,   100] loss: 2.259\n",
      "[18,   200] loss: 2.204\n",
      "[18,   300] loss: 2.220\n",
      "[18,   400] loss: 2.211\n",
      "[18,   500] loss: 2.194\n",
      "[18,   600] loss: 2.200\n",
      "[18,   700] loss: 2.202\n",
      "[18,   800] loss: 2.208\n",
      "[18,   900] loss: 2.177\n",
      "[18,  1000] loss: 2.191\n",
      "[18,  1100] loss: 2.201\n",
      "[18,  1200] loss: 2.151\n",
      "[18,  1300] loss: 2.163\n",
      "[18,  1400] loss: 2.163\n",
      "[18,  1500] loss: 2.179\n",
      "[19,   100] loss: 2.141\n",
      "[19,   200] loss: 2.167\n",
      "[19,   300] loss: 2.153\n",
      "[19,   400] loss: 2.152\n",
      "[19,   500] loss: 2.158\n",
      "[19,   600] loss: 2.140\n",
      "[19,   700] loss: 2.133\n",
      "[19,   800] loss: 2.114\n",
      "[19,   900] loss: 2.129\n",
      "[19,  1000] loss: 2.153\n",
      "[19,  1100] loss: 2.118\n",
      "[19,  1200] loss: 2.122\n",
      "[19,  1300] loss: 2.133\n",
      "[19,  1400] loss: 2.112\n",
      "[19,  1500] loss: 2.126\n",
      "[20,   100] loss: 2.086\n",
      "[20,   200] loss: 2.086\n",
      "[20,   300] loss: 2.076\n",
      "[20,   400] loss: 2.136\n",
      "[20,   500] loss: 2.090\n",
      "[20,   600] loss: 2.097\n",
      "[20,   700] loss: 2.072\n",
      "[20,   800] loss: 2.092\n",
      "[20,   900] loss: 2.086\n",
      "[20,  1000] loss: 2.078\n",
      "[20,  1100] loss: 2.121\n",
      "[20,  1200] loss: 2.062\n",
      "[20,  1300] loss: 2.091\n",
      "[20,  1400] loss: 2.096\n",
      "[20,  1500] loss: 2.071\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "tasks = {'masked_pretraining': 0, 'classification': 1}\n",
    "\n",
    "vis_criterion = nn.KLDivLoss()\n",
    "tex_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "training_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "tokenizer = mimic_feature_dataset_train.tokenizer\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    torch.cuda.empty_cache()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(mimic_feature_dataloader_train, 0):\n",
    "        # get the inputs;\n",
    "        features = data['features'].to(device)\n",
    "        spatials = data['spatials'].to(device)\n",
    "        text = data['text'].to(device)\n",
    "        masked_text = data['masked_text'].to(device)\n",
    "        masked_lm_labels = data['masked_lm_labels'].to(device)\n",
    "        labels = data['y'].to(device)\n",
    "        img_attn_mask = data['img_attn_mask'].to(device)\n",
    "        text_attn_mask = data['text_attn_mask'].to(device)\n",
    "        image_target = data['cls_dist'].to(device)\n",
    "        image_label = data['image_label'].to(device)\n",
    "        next_sentence_label = data['next_sentence_label'].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model([features, spatials], masked_text, img_attn_mask, text_attn_mask)\n",
    "        prediction_scores_t, prediction_scores_v, seq_relationship_score = outputs\n",
    "        prediction_scores_v = prediction_scores_v[:, 1:]\n",
    "        \n",
    "        img_loss = vis_criterion(F.log_softmax(prediction_scores_v, dim=2), image_target[:,1:])\n",
    "        \n",
    "        masked_img_loss = torch.sum(\n",
    "                    img_loss * (image_label == 1).unsqueeze(2).float()\n",
    "                ) / max(torch.sum((image_label == 1)), 1)\n",
    "        masked_lm_loss = tex_criterion(\n",
    "                prediction_scores_t.view(-1, tokenizer.vocab_size),\n",
    "                masked_lm_labels.view(-1),\n",
    "            )\n",
    "\n",
    "        masked_lm_loss = masked_lm_loss.unsqueeze(0)\n",
    "        masked_img_loss = masked_img_loss.unsqueeze(0)\n",
    "        \n",
    "        masked_img_loss = masked_img_loss * 1#args.img_weight how much we weigh the image?\n",
    "        next_sentence_loss = tex_criterion(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "        loss = masked_lm_loss + masked_img_loss + next_sentence_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            #save the training losses\n",
    "            training_losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "    #Check point each epoch\n",
    "    \n",
    "    PATH = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/weights/transforming_embracement_pretrained_VLM_epoch_{}.pth'.format(epoch)\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 23:05:05\n"
     ]
    }
   ],
   "source": [
    "#start: 8:09pm\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights\n",
    "PATH = './weights/MATE_Pretrained_VLM.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aca171f81d0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weights\n",
    "PATH = './weights/MATE_Pretrained_VLM.pth'\n",
    "model.load_state_dict(torch.load(PATH), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mimic_feature_dataset_train.tokenizer\n",
    "\n",
    "for i, data in enumerate(mimic_feature_dataloader_train, 0):\n",
    "    # get the inputs;\n",
    "    features = data['features'].to(device)\n",
    "    spatials = data['spatials'].to(device)\n",
    "    text = data['text'].to(device)\n",
    "    masked_text = data['masked_text'].to(device)\n",
    "    masked_lm_labels = data['masked_lm_labels'].to(device)\n",
    "    labels = data['y'].to(device)\n",
    "    img_attn_mask = data['img_attn_mask'].to(device)\n",
    "    text_attn_mask = data['text_attn_mask'].to(device)\n",
    "    image_target = data['cls_dist'].to(device)\n",
    "    image_label = data['image_label'].to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)\n",
    "outputs = model([features, spatials], masked_text, img_attn_mask, text_attn_mask)\n",
    "prediction_scores_t, prediction_scores_v, seq = outputs\n",
    "prediction_scores_v = prediction_scores_v[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2345,    -1,    -1,    -1,    -1,    -1,    -1,    -1,  2474,    -1,\n",
       "           -1,    -1,    -1,    -1,  1024,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,  7274,    -1,    -1,    -1, 16342,    -1,    -1,    -1,    -1,\n",
       "           -1, 11457,  7831,    -1,    -1,    -1,    -1,  1035,  1035,    -1,\n",
       "           -1,    -1,  5776,  2003,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "         7192,    -1,    -1,    -1, 18994,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,  1996,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1, 12032,    -1,    -1,    -1,    -1, 21163,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,  1996,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1, 22307,    -1,    -1,\n",
       "         6728,  6305,  3012,  2089,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,  2003,  2464,  1012,    -1,    -1, 20414,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,  1999,  1996,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1, 10548,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,  4246,    -1,    -1,    -1,    -1,\n",
       "        22307, 11522,    -1,  6728,    -1,    -1,    -1,    -1,  8823,    -1,\n",
       "           -1,    -1,    -1,    -1,  2003,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "           -1,    -1], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 2\n",
    "masked_lm_labels[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] final report examination : chest ( pa and lat ) indication : history : _ _ _ m with dyspnea, fatigue technique : chest pa and lateral comparison : chest radiograph _ _ _ findings : patient is status post median sternotomy, mediastinal clips, and mitral valve repair. mild cardiomegaly is re - demonstrated. the aortic knob is calcified. mediastinal and hilar contours are unchanged. pulmonary vasculature is not engorged. lungs remain hyperinflated. blunting of the costophrenic angles on the lateral view posteriorly suggests trace bilateral pleural effusions, decreased in size from the prior exam. patchy retrocardiac opacity may reflect atelectasis, but infection is difficult to exclude. no pneumothorax is seen. moderate multilevel degenerative changes are noted in the thoracic spine. impression : decreased size of small bilateral pleural effusions. patchy retrocardiac opacity could reflect atelectasis though infection is difficult to exclude. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data['text'][num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] report examination : chest ( pa and [MASK]t ) indication : history [MASK] _ _ _ m with d [MASK]pnea, [MASK] technique : chest pa and [MASK] [MASK] : chest radiograph [MASK] _ _ findings : [MASK] [MASK] status post median sternotomy, mediastinal clips, and mitral valve [MASK]. mild card [MASK]egaly is re - demonstrated. [MASK] aortic knob is calcified. mediasti [MASK] and hilar con [MASK]s are unchanged. pulmonary vasculature is not engorged. lungs remain hyperinflated. blunting of the costophrenic angles on [MASK] lateral view posteriorly suggests trace bilateral pleural effusions, decreased in size from the prior exam. patchy [MASK]cardiac [MASK] [MASK] [MASK] [MASK] reflect atelectasis, but infection is difficult to exclude. no pneumothorax [MASK] [MASK] [MASK] moderate multi [MASK]el degenerative changes are noted [MASK] [MASK] thoracic spine. impression : [MASK] size of small bilateral pleural e [MASK]usions. patchy [MASK]cardiac [MASK]acity could reflect [MASK]lectasis though infection is difficult to exclude. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode (masked_text[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final report examination : chest ( pa and lat ) indication : _ : _ _ _ _ with recentyspnea /.. : chest pa and lateral comparison : chest radiograph _ _ _ findings : cough.. post median sternotomy. mediastinal silhouette, with mitral fracture replacement. mild cardiomiomly, re - demonstrated. the aortic arch. calcified. thestinal and hilar contours are noted. pulmonary vasculature is not engorgement and lungs are hyperinflated. blunting of the costophrenic angle seen the lateral right.ly. a left pleural effusion have likely in size from the prior exam. patchy retrocardiac ate. may could likely atelectasis, however there is also to exclude pneumonia no pneumothorax.... oslev os degenerative changes are noted. the thoracic spine. impression : normal normal. the left pleural effff. patchy retrocardiac op lungity may be ate atectasis, infection is difficult to exclude..,eureur ate ate....remsti the. bony.. ate bony....... bony. bi....se.se.......eur.... os. os..ph bony. the.. the un........ stern...... the the os.... the... are...........se bony..seeur cal bony. the..se theci theci. the. the thesti. os. the.. un of.se.. bony the.sti the.eurcibas os.. un. the.. the.. unark pl bony. bony the the of the os of. the. the bony the.. the the. the the.alar.seal.. ateci suggest.. pl bony....... the..... ate ate per the the. the os. osci intact... os bi suggest....iom.... the. cal. and.. con os the os the.ark....... the os bonyse.. ate the.. stern.......eur.. overly thestiiom.. bony'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.max(prediction_scores_t[num], dim=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0038, 0.9962],\n",
       "        [0.2553, 0.7447],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_target[0,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4167,  0.7209],\n",
       "        [-2.3142, -0.8793],\n",
       "        [ 5.5470, -6.9916],\n",
       "        [ 6.7048, -7.3473],\n",
       "        [ 7.7279, -4.8800],\n",
       "        [ 7.9359, -4.8009],\n",
       "        [ 6.6318, -4.7139],\n",
       "        [ 7.5034, -5.0953],\n",
       "        [ 7.5055, -5.1582]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores_v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weights\n",
    "PATH = './weights/MATE_Pretrained_VLM.pth'\n",
    "model.load_state_dict(torch.load(PATH), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model([features, spatials], masked_text, img_attn_mask, text_attn_mask)\n",
    "prediction_scores_t, prediction_scores_v = outputs\n",
    "prediction_scores_v = prediction_scores_v[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 30522])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores_t[3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final report chest radiographs history : increased increased., with known metastatic pancreati cancer. comparisons : radiographs from _ _ _ and chest ct chest _ _ _. technique : chest, portable and and and.graphs : the cardiac, mediastinal and hilar contours appear unchanged. a similar to of the the upper mediastinal contours unchanged, is not to be tortuosity of the right glenumees artery on the prior study. patch patch opacities in the upper lung, alike defined opacity in the rightula are unchanged to toive atelectasis or scarring. the the vague opacity seen the frontal view, more more pronounced than on the prior chestgraph, however, which appear similar similar to the similar view view, the multiple ct chest. linear atelectasis, also.. degentic changes are similar to to thor theic spine. degenerative changes, seen may to lung, particularly in thes. the bones are toner demized. impression : increasedy right basilar opacitiess with,, is consistent with atelectasis however however is present, difficult to completely excluded, however. 2 pneumonia exclude however pneumonia however... exclude... pneumonia.,sis considered,sississis history in. setting. pneumonia. of pneumonia of....,...tion setting,...sis.,, considered... considered of ofsis..ities ofsis.sissis.sis,, of of of at..... of.., reflect of.. of. of in in, considered, of of of of in...,. of of considered of...... in.. to. of ofities consideredscu of of history of. of in. of of. of. with ofities of the.tic. of of.. seen. and.... reflect. the of,ities, the.. and.. the. of.......,..,,.,. of. of.. of.. of. defined.. the.. the, defined..... the considered of...sis with.. theac in to.ac......ac the the of. in...........................'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.max(prediction_scores_t[3], dim=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2267, -0.6401],\n",
       "        [-2.1018, -0.8257],\n",
       "        [-1.8775, -1.1504],\n",
       "        [-1.4759, -1.7652],\n",
       "        [ 0.0485, -3.5444],\n",
       "        [-0.8324, -2.8117],\n",
       "        [-1.6613, -1.5677],\n",
       "        [-1.8468, -1.3681],\n",
       "        [-1.2258, -2.2282]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores_v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './weights/MATE_Pretrained_VLM.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b26836ab9e8>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 1#classification\n",
    "#model_type = 0#embrace\n",
    "model_type = 1#multiplied + embrace\n",
    "#model_type = 2#multiplied\n",
    "model = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=8, Nx=6, d_model=256, \n",
    "                  ff_size=1024, dropout=0.35, task=task, freeze_FE_layers=False, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (self_attn_layers): ModuleList(\n",
       "      (0): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (self_attn_layers): ModuleList(\n",
       "      (0): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (out): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=256, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.35, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 256)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weights\n",
    "PATH = './weights/MATE_Pretrained_VLM.pth'\n",
    "model.load_state_dict(torch.load(PATH), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.748\n",
      "[1,   200] loss: 0.711\n",
      "[1,   300] loss: 0.660\n",
      "[1,   400] loss: 0.670\n",
      "[1,   500] loss: 0.682\n",
      "[1,   600] loss: 0.687\n",
      "[1,   700] loss: 0.599\n",
      "[1,   800] loss: 0.599\n",
      "[1,   900] loss: 0.506\n",
      "[1,  1000] loss: 0.529\n",
      "[1,  1100] loss: 0.531\n",
      "[1,  1200] loss: 0.525\n",
      "[1,  1300] loss: 0.575\n",
      "[1,  1400] loss: 0.493\n",
      "[1,  1500] loss: 0.554\n",
      "Accuracy of the network on the 24389 training samples: 71.75775964574193\n",
      "Training AUC: 0.708\n",
      "Accuracy of the network on the 3105 valid samples: 80.61191626409018\n",
      "Valid AUC: 0.818\n",
      "Validation loss: 0.5006108373144235\n",
      "[2,   100] loss: 0.482\n",
      "[2,   200] loss: 0.478\n",
      "[2,   300] loss: 0.505\n",
      "[2,   400] loss: 0.463\n",
      "[2,   500] loss: 0.482\n",
      "[2,   600] loss: 0.443\n",
      "[2,   700] loss: 0.515\n",
      "[2,   800] loss: 0.484\n",
      "[2,   900] loss: 0.455\n",
      "[2,  1000] loss: 0.440\n",
      "[2,  1100] loss: 0.516\n",
      "[2,  1200] loss: 0.520\n",
      "[2,  1300] loss: 0.512\n",
      "[2,  1400] loss: 0.442\n",
      "[2,  1500] loss: 0.399\n",
      "Accuracy of the network on the 24389 training samples: 82.42240354258067\n",
      "Training AUC: 0.770\n",
      "Accuracy of the network on the 3105 valid samples: 85.76489533011272\n",
      "Valid AUC: 0.839\n",
      "Validation loss: 0.44513251863104025\n",
      "[3,   100] loss: 0.386\n",
      "[3,   200] loss: 0.385\n",
      "[3,   300] loss: 0.438\n",
      "[3,   400] loss: 0.429\n",
      "[3,   500] loss: 0.460\n",
      "[3,   600] loss: 0.455\n",
      "[3,   700] loss: 0.400\n",
      "[3,   800] loss: 0.384\n",
      "[3,   900] loss: 0.460\n",
      "[3,  1000] loss: 0.453\n",
      "[3,  1100] loss: 0.409\n",
      "[3,  1200] loss: 0.412\n",
      "[3,  1300] loss: 0.388\n",
      "[3,  1400] loss: 0.464\n",
      "[3,  1500] loss: 0.420\n",
      "Accuracy of the network on the 24389 training samples: 84.85792775431547\n",
      "Training AUC: 0.815\n",
      "Accuracy of the network on the 3105 valid samples: 84.92753623188406\n",
      "Valid AUC: 0.864\n",
      "Validation loss: 0.41532381905453064\n",
      "[4,   100] loss: 0.406\n",
      "[4,   200] loss: 0.397\n",
      "[4,   300] loss: 0.426\n",
      "[4,   400] loss: 0.363\n",
      "[4,   500] loss: 0.404\n",
      "[4,   600] loss: 0.415\n",
      "[4,   700] loss: 0.368\n",
      "[4,   800] loss: 0.384\n",
      "[4,   900] loss: 0.375\n",
      "[4,  1000] loss: 0.398\n",
      "[4,  1100] loss: 0.406\n",
      "[4,  1200] loss: 0.383\n",
      "[4,  1300] loss: 0.343\n",
      "[4,  1400] loss: 0.396\n",
      "[4,  1500] loss: 0.417\n",
      "Accuracy of the network on the 24389 training samples: 86.33810324326541\n",
      "Training AUC: 0.844\n",
      "Accuracy of the network on the 3105 valid samples: 88.1159420289855\n",
      "Valid AUC: 0.875\n",
      "Validation loss: 0.39952136975007396\n",
      "[5,   100] loss: 0.330\n",
      "[5,   200] loss: 0.386\n",
      "[5,   300] loss: 0.336\n",
      "[5,   400] loss: 0.389\n",
      "[5,   500] loss: 0.361\n",
      "[5,   600] loss: 0.338\n",
      "[5,   700] loss: 0.383\n",
      "[5,   800] loss: 0.317\n",
      "[5,   900] loss: 0.371\n",
      "[5,  1000] loss: 0.362\n",
      "[5,  1100] loss: 0.373\n",
      "[5,  1200] loss: 0.321\n",
      "[5,  1300] loss: 0.301\n",
      "[5,  1400] loss: 0.383\n",
      "[5,  1500] loss: 0.340\n",
      "Accuracy of the network on the 24389 training samples: 87.64196974045676\n",
      "Training AUC: 0.864\n",
      "Accuracy of the network on the 3105 valid samples: 87.21417069243157\n",
      "Valid AUC: 0.886\n",
      "Validation loss: 0.4644428918015551\n",
      "[6,   100] loss: 0.265\n",
      "[6,   200] loss: 0.330\n",
      "[6,   300] loss: 0.293\n",
      "[6,   400] loss: 0.366\n",
      "[6,   500] loss: 0.307\n",
      "[6,   600] loss: 0.309\n",
      "[6,   700] loss: 0.314\n",
      "[6,   800] loss: 0.293\n",
      "[6,   900] loss: 0.286\n",
      "[6,  1000] loss: 0.352\n",
      "[6,  1100] loss: 0.308\n",
      "[6,  1200] loss: 0.297\n",
      "[6,  1300] loss: 0.349\n",
      "[6,  1400] loss: 0.309\n",
      "[6,  1500] loss: 0.280\n",
      "Accuracy of the network on the 24389 training samples: 88.52761490836032\n",
      "Training AUC: 0.880\n",
      "Accuracy of the network on the 3105 valid samples: 87.66505636070853\n",
      "Valid AUC: 0.891\n",
      "Validation loss: 0.44480531570113063\n",
      "[7,   100] loss: 0.319\n",
      "[7,   200] loss: 0.282\n",
      "[7,   300] loss: 0.240\n",
      "[7,   400] loss: 0.252\n",
      "[7,   500] loss: 0.253\n",
      "[7,   600] loss: 0.275\n",
      "[7,   700] loss: 0.277\n",
      "[7,   800] loss: 0.278\n",
      "[7,   900] loss: 0.308\n",
      "[7,  1000] loss: 0.245\n",
      "[7,  1100] loss: 0.287\n",
      "[7,  1200] loss: 0.250\n",
      "[7,  1300] loss: 0.266\n",
      "[7,  1400] loss: 0.284\n",
      "[7,  1500] loss: 0.293\n",
      "Accuracy of the network on the 24389 training samples: 89.83148140555168\n",
      "Training AUC: 0.892\n",
      "Accuracy of the network on the 3105 valid samples: 85.99033816425121\n",
      "Valid AUC: 0.893\n",
      "Validation loss: 0.5767061604666252\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[8,   100] loss: 0.234\n",
      "[8,   200] loss: 0.219\n",
      "[8,   300] loss: 0.216\n",
      "[8,   400] loss: 0.196\n",
      "[8,   500] loss: 0.184\n",
      "[8,   600] loss: 0.252\n",
      "[8,   700] loss: 0.203\n",
      "[8,   800] loss: 0.198\n",
      "[8,   900] loss: 0.218\n",
      "[8,  1000] loss: 0.221\n",
      "[8,  1100] loss: 0.210\n",
      "[8,  1200] loss: 0.171\n",
      "[8,  1300] loss: 0.184\n",
      "[8,  1400] loss: 0.162\n",
      "[8,  1500] loss: 0.197\n",
      "Accuracy of the network on the 24389 training samples: 91.68887613268276\n",
      "Training AUC: 0.905\n",
      "Accuracy of the network on the 3105 valid samples: 86.53784219001611\n",
      "Valid AUC: 0.898\n",
      "Validation loss: 0.5830736477427435\n",
      "[9,   100] loss: 0.201\n",
      "[9,   200] loss: 0.210\n",
      "[9,   300] loss: 0.190\n",
      "[9,   400] loss: 0.232\n",
      "[9,   500] loss: 0.151\n",
      "[9,   600] loss: 0.174\n",
      "[9,   700] loss: 0.230\n",
      "[9,   800] loss: 0.184\n",
      "[9,   900] loss: 0.141\n",
      "[9,  1000] loss: 0.173\n",
      "[9,  1100] loss: 0.192\n",
      "[9,  1200] loss: 0.185\n",
      "[9,  1300] loss: 0.178\n",
      "[9,  1400] loss: 0.177\n",
      "[9,  1500] loss: 0.201\n",
      "Accuracy of the network on the 24389 training samples: 92.42281356349174\n",
      "Training AUC: 0.915\n",
      "Accuracy of the network on the 3105 valid samples: 86.98872785829307\n",
      "Valid AUC: 0.903\n",
      "Validation loss: 0.5193914355698807\n",
      "[10,   100] loss: 0.182\n",
      "[10,   200] loss: 0.214\n",
      "[10,   300] loss: 0.147\n",
      "[10,   400] loss: 0.151\n",
      "[10,   500] loss: 0.153\n",
      "[10,   600] loss: 0.160\n",
      "[10,   700] loss: 0.162\n",
      "[10,   800] loss: 0.219\n",
      "[10,   900] loss: 0.148\n",
      "[10,  1000] loss: 0.185\n",
      "[10,  1100] loss: 0.151\n",
      "[10,  1200] loss: 0.195\n",
      "[10,  1300] loss: 0.156\n",
      "[10,  1400] loss: 0.189\n",
      "[10,  1500] loss: 0.207\n",
      "Accuracy of the network on the 24389 training samples: 92.61962360080364\n",
      "Training AUC: 0.922\n",
      "Accuracy of the network on the 3105 valid samples: 85.92592592592592\n",
      "Valid AUC: 0.906\n",
      "Validation loss: 0.5456775560232774\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-06.\n",
      "[11,   100] loss: 0.159\n",
      "[11,   200] loss: 0.169\n",
      "[11,   300] loss: 0.145\n",
      "[11,   400] loss: 0.177\n",
      "[11,   500] loss: 0.189\n",
      "[11,   600] loss: 0.150\n",
      "[11,   700] loss: 0.210\n",
      "[11,   800] loss: 0.165\n",
      "[11,   900] loss: 0.172\n",
      "[11,  1000] loss: 0.161\n",
      "[11,  1100] loss: 0.181\n",
      "[11,  1200] loss: 0.175\n",
      "[11,  1300] loss: 0.171\n",
      "[11,  1400] loss: 0.175\n",
      "[11,  1500] loss: 0.164\n",
      "Accuracy of the network on the 24389 training samples: 92.64012464635697\n",
      "Training AUC: 0.928\n",
      "Accuracy of the network on the 3105 valid samples: 86.53784219001611\n",
      "Valid AUC: 0.908\n",
      "Validation loss: 0.5477044838502835\n",
      "[12,   100] loss: 0.208\n",
      "[12,   200] loss: 0.172\n",
      "[12,   300] loss: 0.159\n",
      "[12,   400] loss: 0.211\n",
      "[12,   500] loss: 0.156\n",
      "[12,   600] loss: 0.174\n",
      "[12,   700] loss: 0.187\n",
      "[12,   800] loss: 0.141\n",
      "[12,   900] loss: 0.182\n",
      "[12,  1000] loss: 0.159\n",
      "[12,  1100] loss: 0.186\n",
      "[12,  1200] loss: 0.151\n",
      "[12,  1300] loss: 0.173\n",
      "[12,  1400] loss: 0.156\n",
      "[12,  1500] loss: 0.190\n",
      "Accuracy of the network on the 24389 training samples: 92.70982820123827\n",
      "Training AUC: 0.933\n",
      "Accuracy of the network on the 3105 valid samples: 86.95652173913044\n",
      "Valid AUC: 0.910\n",
      "Validation loss: 0.5747115462173348\n",
      "[13,   100] loss: 0.175\n",
      "[13,   200] loss: 0.191\n",
      "[13,   300] loss: 0.155\n",
      "[13,   400] loss: 0.209\n",
      "[13,   500] loss: 0.195\n",
      "[13,   600] loss: 0.161\n",
      "[13,   700] loss: 0.176\n",
      "[13,   800] loss: 0.178\n",
      "[13,   900] loss: 0.169\n",
      "[13,  1000] loss: 0.185\n",
      "[13,  1100] loss: 0.155\n",
      "[13,  1200] loss: 0.175\n",
      "[13,  1300] loss: 0.138\n",
      "[13,  1400] loss: 0.192\n",
      "[13,  1500] loss: 0.170\n",
      "Accuracy of the network on the 24389 training samples: 92.89843782032884\n",
      "Training AUC: 0.937\n",
      "Accuracy of the network on the 3105 valid samples: 86.89210950080515\n",
      "Valid AUC: 0.912\n",
      "Validation loss: 0.6075522880750992\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-07.\n",
      "[14,   100] loss: 0.153\n",
      "[14,   200] loss: 0.189\n",
      "[14,   300] loss: 0.180\n",
      "[14,   400] loss: 0.148\n",
      "[14,   500] loss: 0.167\n",
      "[14,   600] loss: 0.168\n",
      "[14,   700] loss: 0.174\n",
      "[14,   800] loss: 0.161\n",
      "[14,   900] loss: 0.169\n",
      "[14,  1000] loss: 0.219\n",
      "[14,  1100] loss: 0.170\n",
      "[14,  1200] loss: 0.160\n",
      "[14,  1300] loss: 0.182\n",
      "[14,  1400] loss: 0.142\n",
      "[14,  1500] loss: 0.176\n",
      "Accuracy of the network on the 24389 training samples: 93.05014555742343\n",
      "Training AUC: 0.940\n",
      "Accuracy of the network on the 3105 valid samples: 87.27858293075684\n",
      "Valid AUC: 0.914\n",
      "Validation loss: 0.5185966708711319\n",
      "[15,   100] loss: 0.172\n",
      "[15,   200] loss: 0.214\n",
      "[15,   300] loss: 0.186\n",
      "[15,   400] loss: 0.171\n",
      "[15,   500] loss: 0.171\n",
      "[15,   600] loss: 0.145\n",
      "[15,   700] loss: 0.166\n",
      "[15,   800] loss: 0.170\n",
      "[15,   900] loss: 0.163\n",
      "[15,  1000] loss: 0.207\n",
      "[15,  1100] loss: 0.129\n",
      "[15,  1200] loss: 0.197\n",
      "[15,  1300] loss: 0.175\n",
      "[15,  1400] loss: 0.142\n",
      "[15,  1500] loss: 0.156\n",
      "Accuracy of the network on the 24389 training samples: 92.9148386567715\n",
      "Training AUC: 0.943\n",
      "Accuracy of the network on the 3105 valid samples: 86.02254428341385\n",
      "Valid AUC: 0.915\n",
      "Validation loss: 0.5707319557949435\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "import torch.optim as optim\n",
    "tokenizer = mimic_feature_dataset_train.tokenizer\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2)\n",
    "\n",
    "#apply weights for ambiguous samples\n",
    "ambiguity_count = mimic_feature_dataset_train.dataset_frame.groupby('ambiguous').count()['dicom_id'].to_numpy()\n",
    "ambiguity_weights = 1/ambiguity_count\n",
    "\n",
    "\n",
    "\n",
    "training_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "training_label_list = []\n",
    "valid_label_list = []\n",
    "\n",
    "training_output_list = []\n",
    "valid_output_list = []\n",
    "\n",
    "training_aucs = []\n",
    "valid_aucs = []\n",
    "\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "    torch.cuda.empty_cache()\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for i, data in enumerate(mimic_feature_dataloader_train, 0):\n",
    "        # get the inputs;\n",
    "        features = data['features'].to(device)\n",
    "        spatials = data['spatials'].to(device)\n",
    "        text = data['text'].to(device)\n",
    "        labels = data['y'].to(device)\n",
    "        img_attn_mask = data['img_attn_mask'].to(device)\n",
    "        text_attn_mask = data['text_attn_mask'].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model([features, spatials], text, img_attn_mask, text_attn_mask)\n",
    "        sample_weight = torch.tensor(ambiguity_weights[data['ambiguous']]).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = (loss * sample_weight / sample_weight.sum()).sum()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            #save the training losses\n",
    "            training_losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "    #############get our accuracy for train################\n",
    "        training_label_list.extend(labels.tolist())\n",
    "        training_output_list.extend(outputs.tolist())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the {} training samples: {}'.format(len(mimic_feature_dataset_train), (100 * correct / total)))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(training_label_list, np.asarray(training_output_list)[:,1])\n",
    "    AUC = metrics.auc(fpr, tpr)\n",
    "    training_aucs.append(AUC)\n",
    "    print('Training AUC: {:.03f}'.format(AUC))\n",
    "    #############get our accuracy for train################\n",
    "    #Check point each epoch\n",
    "    PATH = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/weights/transforming_embracement_leaky_varEMB_epoch_{}.pth'.format(epoch)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_data in mimic_feature_dataloader_valid:\n",
    "            val_features = val_data['features'].to(device)\n",
    "            val_spatials = val_data['spatials'].to(device)\n",
    "            val_text = val_data['text'].to(device)\n",
    "            val_labels = val_data['y'].to(device)\n",
    "            val_img_attn_mask = val_data['img_attn_mask'].to(device)\n",
    "            val_text_attn_mask = val_data['text_attn_mask'].to(device)\n",
    "            val_outputs = model([val_features, val_spatials], val_text, val_img_attn_mask, val_text_attn_mask)\n",
    "            val_sample_weight = torch.tensor(ambiguity_weights[val_data['ambiguous']]).to(device)\n",
    "            val_weighted_loss = (criterion(val_outputs, val_labels) * val_sample_weight / val_sample_weight.sum()).sum()\n",
    "            val_loss += val_weighted_loss\n",
    "            #get our accuracy for valid\n",
    "            valid_label_list.extend(val_labels.tolist())\n",
    "            valid_output_list.extend(val_outputs.tolist())\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "        print('Accuracy of the network on the {} valid samples: {}'.format(len(mimic_feature_dataset_valid), (100 * val_correct / val_total)))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(valid_label_list, np.asarray(valid_output_list)[:,1])\n",
    "        val_AUC = metrics.auc(fpr, tpr)\n",
    "        valid_aucs.append(val_AUC)\n",
    "        print('Valid AUC: {:.03f}'.format(val_AUC))\n",
    "        #get our accuracy for valid\n",
    "\n",
    "    #reduce LR\n",
    "    valid_losses.append(val_loss / len(mimic_feature_dataloader_valid))#save losses    \n",
    "    print('Validation loss: {}'.format(val_loss / len(mimic_feature_dataloader_valid)))\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b4d0bbec358>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b4d03c292b0>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights\n",
    "PATH = './weights/MATE_Classification_Full_model_type_{}.pth'.format(model_type)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (self_attn_A): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (self_attn_B): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (out): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.35, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 512)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weights\n",
    "PATH = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/weights/transforming_embracement_leaky_varEMB_epoch_9.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "#model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'spatials', 'text', 'boxes', 'image_id', 'width', 'height', 'text_attn_mask', 'img_attn_mask', 'cls_dist', 'image_label', 'masked_text', 'masked_lm_labels', 'next_sentence_label', 'ambiguous', 'y'])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = {0:'MATE Vanilla', 1: 'MATE Pooled hidden x Embraced', 2: 'MATE Pooled hidden'}\n",
    "amb_types = {True: 'Ambiguous Only Subset', False: 'Full Test'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "task = 1#classification\n",
    "#model_type = 0#embrace\n",
    "model_type = 1#multiplied + embrace\n",
    "#model_type = 2#multiplied\n",
    "model = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=8, Nx=6, d_model=256, \n",
    "                  ff_size=1024, dropout=0.35, task=task, freeze_FE_layers=False, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (self_attn_layers): ModuleList(\n",
       "      (0): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (self_attn_layers): ModuleList(\n",
       "      (0): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SelfAttentionLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (out): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=256, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.35, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 256)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "PATH = './weights/MATE_Classification_Full_model_type_{}.pth'.format(model_type)\n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 82 test samples: 59.75609756097561\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_list = []\n",
    "output_list = []\n",
    "dicom_id_list = []\n",
    "sample_num = 0\n",
    "only_amb = True\n",
    "tokenizer = mimic_feature_dataset_train.tokenizer\n",
    "with torch.no_grad():\n",
    "    for data in mimic_feature_dataloader_test:\n",
    "        dicom_id_list.extend(data['image_id'])\n",
    "        features = data['features'].to(device)\n",
    "        spatials = data['spatials'].to(device)\n",
    "        text = data['text'].to(device)\n",
    "        labels = data['y'].to(device)\n",
    "        img_attn_mask = data['img_attn_mask'].to(device)\n",
    "        text_attn_mask = data['text_attn_mask'].to(device)\n",
    "        outputs = model([features, spatials], text,img_attn_mask, text_attn_mask)\n",
    "        #outputs = model([torch.rand_like(features), torch.rand_like(spatials)], text, img_attn_mask, text_attn_mask)#noise\n",
    "        #outputs = model([features, spatials], torch.randint_like(text, 30000), img_attn_mask, text_attn_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred_amb = pd.DataFrame({'outputs':outputs.cpu(),'labels':labels.cpu(),'pred':predicted.cpu(), 'amb':data['ambiguous'].tolist()})\n",
    "        if only_amb:\n",
    "            mask = pred_amb['amb'] == 1\n",
    "            if sum(mask) > 0:\n",
    "                output_list.extend(pred_amb[mask]['outputs'].tolist())                \n",
    "                labels = torch.tensor(pred_amb[mask]['labels'].tolist())\n",
    "                label_list.extend(labels.tolist())\n",
    "                predicted = torch.tensor(pred_amb[mask]['pred'].tolist())\n",
    "                sample_num+=labels.size(0)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        else:\n",
    "            output_list.extend(outputs.tolist())\n",
    "            label_list.extend(labels.tolist())\n",
    "            sample_num+=labels.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the {} test samples: {}'.format(sample_num, (100 * correct / total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe = OneHotEncoder()\n",
    "#label_list = ohe.fit_transform(np.asarray(label_list).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(label_list, F.softmax(torch.tensor(output_list), dim=1)[:,1])\n",
    "AUC = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7266542980828695"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Micro AUC because there is only one class!\n",
    "plt.figure(figsize=(10,10))\n",
    "lw = 5\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % AUC)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC - {} - {}'.format(model_types[model_type], amb_types[only_amb]))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.7878787878787878, 0.6117647058823529, None)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(label_list, np.argmax(output_list, axis=1), average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.47      0.58        49\n",
      "           1       0.50      0.79      0.61        33\n",
      "\n",
      "    accuracy                           0.60        82\n",
      "   macro avg       0.63      0.63      0.60        82\n",
      "weighted avg       0.66      0.60      0.59        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_list, np.argmax(F.softmax(torch.as_tensor(output_list), dim=1), axis=1).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(zip(np.argmax(output_list, axis=1), label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = counts[1,1]\n",
    "tn = counts[0,0]\n",
    "fp = counts[1,0]\n",
    "fn = counts[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5357142857142857\n",
      "0.9090909090909091\n",
      "0.6741573033707865\n"
     ]
    }
   ],
   "source": [
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*((precision*recall)/(precision+recall))\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = F.softmax(torch.as_tensor(output_list),dim=1)[:,1].tolist()\n",
    "plt.hist(x, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_csv)\n",
    "valid = pd.read_csv(valid_csv)\n",
    "test = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "checking if subjects from train are in valid and test\n",
      "0\n",
      "0\n",
      "checking if subjects from valid are in test\n",
      "0\n",
      "checking if subjects from train are in valid and test\n",
      "0\n",
      "0\n",
      "checking if study_id from valid are in test\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check for leakage\n",
    "mimic_loc = '/home/kl533/krauthammer_partition/mimic-cxr-jpg/mimic-cxr-jpg-2.0.0.physionet.org/'\n",
    "metadata = pd.read_csv(os.path.join(mimic_loc, \"mimic-cxr-2.0.0-metadata.csv\"))\n",
    "train_dicom_id = train['dicom_id'].tolist()\n",
    "valid_dicom_id = valid['dicom_id'].tolist()\n",
    "test_dicom_id = test['dicom_id'].tolist()\n",
    "print(set(train_dicom_id).intersection(valid_dicom_id))\n",
    "print(set(train_dicom_id).intersection(test_dicom_id))\n",
    "\n",
    "train_subject_id = metadata[metadata['dicom_id'].isin(train_dicom_id)]['subject_id'].tolist()\n",
    "valid_subject_id = metadata[metadata['dicom_id'].isin(valid_dicom_id)]['subject_id'].tolist()\n",
    "test_subject_id = metadata[metadata['dicom_id'].isin(test_dicom_id)]['subject_id'].tolist()\n",
    "print('checking if subjects from train are in valid and test')\n",
    "print(len(set(train_subject_id).intersection(valid_subject_id)))\n",
    "print(len(set(train_subject_id).intersection(test_subject_id)))\n",
    "\n",
    "print('checking if subjects from valid are in test')\n",
    "print(len(set(valid_subject_id).intersection(test_subject_id)))\n",
    "\n",
    "train_study_id = metadata[metadata['dicom_id'].isin(train_dicom_id)]['study_id'].tolist()\n",
    "valid_study_id = metadata[metadata['dicom_id'].isin(valid_dicom_id)]['study_id'].tolist()\n",
    "test_study_id = metadata[metadata['dicom_id'].isin(test_dicom_id)]['study_id'].tolist()\n",
    "print('checking if subjects from train are in valid and test')\n",
    "print(len(set(train_study_id).intersection(valid_study_id)))\n",
    "print(len(set(train_study_id).intersection(test_study_id)))\n",
    "\n",
    "print('checking if study_id from valid are in test')\n",
    "print(len(set(valid_subject_id).intersection(test_study_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b28e6463a075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dicom_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dicom_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dicom_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dicom_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalid_dicom_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dicom_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dicom_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dicom_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train_dicom_id = train['dicom_id'].tolist()\n",
    "test_dicom_id = test['dicom_id'].tolist()\n",
    "valid_dicom_id = validate['dicom_id'].tolist()\n",
    "set(train_dicom_id).intersection(valid_dicom_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(os.path.join(mimic_loc, \"mimic-cxr-2.0.0-metadata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subject_id = metadata[metadata['dicom_id'].isin(train_dicom_id)]['subject_id'].tolist()\n",
    "test_subject_id = metadata[metadata['dicom_id'].isin(test_dicom_id)]['subject_id'].tolist()\n",
    "valid_subject_id = metadata[metadata['dicom_id'].isin(valid_dicom_id)]['subject_id'].tolist()\n",
    "set(train_subject_id).intersection(valid_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_study_id = metadata[metadata['dicom_id'].isin(train_dicom_id)]['study_id'].tolist()\n",
    "test_study_id = metadata[metadata['dicom_id'].isin(test_dicom_id)]['study_id'].tolist()\n",
    "valid_study_id = metadata[metadata['dicom_id'].isin(valid_dicom_id)]['study_id'].tolist()\n",
    "set(train_subject_id).intersection(valid_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800506</td>\n",
       "      <td>213014.531</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800506</td>\n",
       "      <td>213014.531</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800626</td>\n",
       "      <td>165500.312</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800626</td>\n",
       "      <td>165500.312</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2705</td>\n",
       "      <td>2539</td>\n",
       "      <td>21800723</td>\n",
       "      <td>80556.875</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2906</td>\n",
       "      <td>2258</td>\n",
       "      <td>21800723</td>\n",
       "      <td>80556.875</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>10000032</td>\n",
       "      <td>56699142</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800805</td>\n",
       "      <td>234424.765</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21321015</td>\n",
       "      <td>84047.984</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b79e55c3-735ce5ac-64412506-cdc9ea79-f1af521f</td>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21321015</td>\n",
       "      <td>84047.984</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dcfeeac4-1597e318-d0e6736a-8b2c2238-47ac3f1b</td>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21321015</td>\n",
       "      <td>84047.984</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0c4eb1e1-b801903c-bcebe8a4-3da9cd3c-3b94a27c</td>\n",
       "      <td>10000898</td>\n",
       "      <td>50771383</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21880312</td>\n",
       "      <td>125501.842</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2a280266-c8bae121-54d75383-cac046f4-ca37aa16</td>\n",
       "      <td>10000898</td>\n",
       "      <td>50771383</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21880312</td>\n",
       "      <td>125501.842</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8959e402-2175d68d-edba5a6c-baab51c3-9359f700</td>\n",
       "      <td>10000898</td>\n",
       "      <td>54205396</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21880113</td>\n",
       "      <td>140127.546</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9e7a6aae-2580e589-6212d336-9813ebbd-a9239a34</td>\n",
       "      <td>10000898</td>\n",
       "      <td>54205396</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21880113</td>\n",
       "      <td>140127.546</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b75df1bd-0f22d631-52d73526-2ae7b85a-d843b39d</td>\n",
       "      <td>10000898</td>\n",
       "      <td>54205396</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21880113</td>\n",
       "      <td>140127.546</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738</td>\n",
       "      <td>10000935</td>\n",
       "      <td>50578979</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2870</td>\n",
       "      <td>2402</td>\n",
       "      <td>21871016</td>\n",
       "      <td>123945.421</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3be619d1-506a66cf-ff1ab8a1-2efb77bb-fe7d59fc</td>\n",
       "      <td>10000935</td>\n",
       "      <td>51178377</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21870823</td>\n",
       "      <td>191426.062</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9b314ad7-fbcb0422-6db62dfc-732858d0-a5527d8b</td>\n",
       "      <td>10000935</td>\n",
       "      <td>51178377</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21870823</td>\n",
       "      <td>191426.062</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6a5c3985-7764bdd0-ec5a6a74-af78bcaa-4ca33ec3</td>\n",
       "      <td>10000935</td>\n",
       "      <td>55697293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LL</td>\n",
       "      <td>2022</td>\n",
       "      <td>1840</td>\n",
       "      <td>21870226</td>\n",
       "      <td>85443.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>left lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>c50494f1-90e2bff5-e9189550-1a4562fd-6ab5204c</td>\n",
       "      <td>10000935</td>\n",
       "      <td>55697293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>2022</td>\n",
       "      <td>1840</td>\n",
       "      <td>21870226</td>\n",
       "      <td>85443.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8e3f2822-0c1d4b71-2a265bbf-5b96e531-ccf5fa30</td>\n",
       "      <td>10000935</td>\n",
       "      <td>56164612</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21870711</td>\n",
       "      <td>111436.562</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ad13cf84-62c34a01-a01b9e87-2581a359-83bbc046</td>\n",
       "      <td>10000935</td>\n",
       "      <td>56164612</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21870711</td>\n",
       "      <td>111436.562</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6fa5997e-b1dfecf8-3c174666-8815c84a-32db59ff</td>\n",
       "      <td>10000935</td>\n",
       "      <td>56522600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LL</td>\n",
       "      <td>2022</td>\n",
       "      <td>1736</td>\n",
       "      <td>21860730</td>\n",
       "      <td>155005.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>left lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>f1adcae3-2921c0a8-5d9652f9-4191ecd7-f2a96f35</td>\n",
       "      <td>10000935</td>\n",
       "      <td>56522600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>2022</td>\n",
       "      <td>1736</td>\n",
       "      <td>21860730</td>\n",
       "      <td>155005.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>88498b37-c21dc7ba-bc202800-b517a62d-f7ac5bcf</td>\n",
       "      <td>10000935</td>\n",
       "      <td>58219844</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21871010</td>\n",
       "      <td>125551.703</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6ad03ed1-97ee17ee-9cf8b320-f7011003-cd93b42d</td>\n",
       "      <td>10000980</td>\n",
       "      <td>50985099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>2022</td>\n",
       "      <td>1870</td>\n",
       "      <td>21910917</td>\n",
       "      <td>130354.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6b360eca-17d2ae1a-19126084-78e9c85d-9800d216</td>\n",
       "      <td>10000980</td>\n",
       "      <td>50985099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LL</td>\n",
       "      <td>2022</td>\n",
       "      <td>1870</td>\n",
       "      <td>21910917</td>\n",
       "      <td>130354.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>left lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>943486a3-b3fa9ff7-50f5a769-7a62fcbb-f39b6da4</td>\n",
       "      <td>10000980</td>\n",
       "      <td>51967283</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21890627</td>\n",
       "      <td>64348.484</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>96f9a77c-59b47dfb-0cac64db-6538421a-a6b135e2</td>\n",
       "      <td>10000980</td>\n",
       "      <td>54577367</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21910613</td>\n",
       "      <td>84554.234</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cfb03587-782edf6c-1bf392e1-98196cd5-365d69e8</td>\n",
       "      <td>10000980</td>\n",
       "      <td>54577367</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21910613</td>\n",
       "      <td>84554.234</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377080</th>\n",
       "      <td>af21a5e8-fcfe93ac-84b1c519-61a17957-1bbeccb6</td>\n",
       "      <td>19999068</td>\n",
       "      <td>52793893</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21610824</td>\n",
       "      <td>61741.000</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377081</th>\n",
       "      <td>86292c07-43af46ea-0649f4b2-feecce77-91866b79</td>\n",
       "      <td>19999068</td>\n",
       "      <td>54061983</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21610826</td>\n",
       "      <td>220516.000</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377082</th>\n",
       "      <td>027ce998-e2d551f7-a8a283f4-72d4b80e-d849d761</td>\n",
       "      <td>19999068</td>\n",
       "      <td>57399028</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21610829</td>\n",
       "      <td>51136.000</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377083</th>\n",
       "      <td>f43d99e3-05307f9a-81e228f8-ea2eb9c2-0af14ecd</td>\n",
       "      <td>19999068</td>\n",
       "      <td>58313972</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21610825</td>\n",
       "      <td>162956.000</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377084</th>\n",
       "      <td>4ae61651-0f1f0738-07991a0b-42f9e4d2-74cb632c</td>\n",
       "      <td>19999068</td>\n",
       "      <td>59390811</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21610827</td>\n",
       "      <td>53735.000</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377085</th>\n",
       "      <td>458b41b5-6c8bfc44-a4dff92c-254519d0-9c40e805</td>\n",
       "      <td>19999068</td>\n",
       "      <td>59937395</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21610825</td>\n",
       "      <td>112914.000</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377086</th>\n",
       "      <td>8dc9f5e1-14887015-8db378ef-2fd4441a-d45ee0f3</td>\n",
       "      <td>19999156</td>\n",
       "      <td>50847545</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21230127</td>\n",
       "      <td>145009.656</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377087</th>\n",
       "      <td>bd4eb73d-09c65a7e-797c197f-ae864491-8d258918</td>\n",
       "      <td>19999156</td>\n",
       "      <td>50847545</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21230127</td>\n",
       "      <td>145009.656</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377088</th>\n",
       "      <td>9c44a9e5-7a8fcfa1-cb6b451f-1d1ac993-5312b1c2</td>\n",
       "      <td>19999270</td>\n",
       "      <td>55274188</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21821112</td>\n",
       "      <td>104527.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377089</th>\n",
       "      <td>f5cbd087-0490844c-4e1d38f7-154dc31a-4e99dbd1</td>\n",
       "      <td>19999270</td>\n",
       "      <td>55274188</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21821112</td>\n",
       "      <td>104527.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377090</th>\n",
       "      <td>bf6e3bdb-4ec92b48-340d4e48-7a00234a-1b76dae7</td>\n",
       "      <td>19999270</td>\n",
       "      <td>56267753</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21831203</td>\n",
       "      <td>135727.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377091</th>\n",
       "      <td>f83a9135-334e26ff-6419b6b9-1457634a-aaed76a3</td>\n",
       "      <td>19999270</td>\n",
       "      <td>56267753</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2140</td>\n",
       "      <td>1760</td>\n",
       "      <td>21831203</td>\n",
       "      <td>135727.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377092</th>\n",
       "      <td>c8bbb9ff-ecb81ef7-a1a6cecf-f535bd20-bd512ba0</td>\n",
       "      <td>19999287</td>\n",
       "      <td>50000173</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21970808</td>\n",
       "      <td>31549.890</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377093</th>\n",
       "      <td>ef49bac7-16939860-5a4f182e-c568720f-e0c9d278</td>\n",
       "      <td>19999287</td>\n",
       "      <td>50574077</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21970805</td>\n",
       "      <td>51156.546</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377094</th>\n",
       "      <td>50373d1b-8df0d15f-5d0047f4-578bc509-4f2b48f0</td>\n",
       "      <td>19999287</td>\n",
       "      <td>51885769</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21970726</td>\n",
       "      <td>25712.640</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377095</th>\n",
       "      <td>46510b80-411ac511-fe6ffab2-d7dfdc76-dff1a762</td>\n",
       "      <td>19999287</td>\n",
       "      <td>52519175</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21970807</td>\n",
       "      <td>90637.796</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377096</th>\n",
       "      <td>f7e95a22-cb958055-47114ddf-38532ef4-b4c172d5</td>\n",
       "      <td>19999287</td>\n",
       "      <td>52519175</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21970807</td>\n",
       "      <td>90637.796</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377097</th>\n",
       "      <td>431c5823-ea54bba2-f21af1d4-45887776-9de7edd1</td>\n",
       "      <td>19999287</td>\n",
       "      <td>53255195</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21970803</td>\n",
       "      <td>194419.125</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377098</th>\n",
       "      <td>5a5eddf4-b64e5e49-f6e9c8bc-d6409b00-015470ea</td>\n",
       "      <td>19999287</td>\n",
       "      <td>53282218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>2022</td>\n",
       "      <td>1606</td>\n",
       "      <td>21961127</td>\n",
       "      <td>112117.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377099</th>\n",
       "      <td>9d1b4abe-52d55ff8-25dd0af8-bae63de6-0f2e36e5</td>\n",
       "      <td>19999287</td>\n",
       "      <td>53282218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LL</td>\n",
       "      <td>2022</td>\n",
       "      <td>1776</td>\n",
       "      <td>21961127</td>\n",
       "      <td>112117.000</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>left lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377100</th>\n",
       "      <td>2eb70dfe-52fa728e-a36e09be-ec0ed3cf-0a2ea7f0</td>\n",
       "      <td>19999287</td>\n",
       "      <td>58938059</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21970805</td>\n",
       "      <td>93746.343</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377101</th>\n",
       "      <td>53e9b6d0-5d5317f5-f1a4c031-01d40558-fd14a425</td>\n",
       "      <td>19999376</td>\n",
       "      <td>57540554</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21450731</td>\n",
       "      <td>45417.656</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377102</th>\n",
       "      <td>ee9155f3-944c056b-c76c73d0-3f792f2c-92ae461e</td>\n",
       "      <td>19999442</td>\n",
       "      <td>58497551</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21481128</td>\n",
       "      <td>133244.093</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377103</th>\n",
       "      <td>16b6c70f-6d36bd77-89d2fef4-9c4b8b0a-79c69135</td>\n",
       "      <td>19999442</td>\n",
       "      <td>58708861</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21481119</td>\n",
       "      <td>224703.375</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377104</th>\n",
       "      <td>3fcd0406-9b111603-feae7033-96632b3a-111333e5</td>\n",
       "      <td>19999733</td>\n",
       "      <td>57132437</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21520708</td>\n",
       "      <td>224550.171</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377105</th>\n",
       "      <td>428e2c18-5721d8f3-35a05001-36f3d080-9053b83c</td>\n",
       "      <td>19999733</td>\n",
       "      <td>57132437</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21520708</td>\n",
       "      <td>224550.171</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377106</th>\n",
       "      <td>58c403aa-35ff8bd9-73e39f54-8dc9cc5d-e0ec3fa9</td>\n",
       "      <td>19999733</td>\n",
       "      <td>57132437</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21520708</td>\n",
       "      <td>224550.171</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377107</th>\n",
       "      <td>58766883-376a15ce-3b323a28-6af950a0-16b793bd</td>\n",
       "      <td>19999987</td>\n",
       "      <td>55368167</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21451104</td>\n",
       "      <td>51448.218</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377108</th>\n",
       "      <td>7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08</td>\n",
       "      <td>19999987</td>\n",
       "      <td>58621812</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21451102</td>\n",
       "      <td>202809.234</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377109</th>\n",
       "      <td>1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e</td>\n",
       "      <td>19999987</td>\n",
       "      <td>58971208</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21451103</td>\n",
       "      <td>50507.625</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Recumbent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>377110 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            dicom_id  subject_id  study_id  \\\n",
       "0       02aa804e-bde0afdd-112c0b34-7bc16630-4e384014    10000032  50414267   \n",
       "1       174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962    10000032  50414267   \n",
       "2       2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab    10000032  53189527   \n",
       "3       e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c    10000032  53189527   \n",
       "4       68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714    10000032  53911762   \n",
       "5       fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818    10000032  53911762   \n",
       "6       ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c    10000032  56699142   \n",
       "7       096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4    10000764  57375967   \n",
       "8       b79e55c3-735ce5ac-64412506-cdc9ea79-f1af521f    10000764  57375967   \n",
       "9       dcfeeac4-1597e318-d0e6736a-8b2c2238-47ac3f1b    10000764  57375967   \n",
       "10      0c4eb1e1-b801903c-bcebe8a4-3da9cd3c-3b94a27c    10000898  50771383   \n",
       "11      2a280266-c8bae121-54d75383-cac046f4-ca37aa16    10000898  50771383   \n",
       "12      8959e402-2175d68d-edba5a6c-baab51c3-9359f700    10000898  54205396   \n",
       "13      9e7a6aae-2580e589-6212d336-9813ebbd-a9239a34    10000898  54205396   \n",
       "14      b75df1bd-0f22d631-52d73526-2ae7b85a-d843b39d    10000898  54205396   \n",
       "15      d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738    10000935  50578979   \n",
       "16      3be619d1-506a66cf-ff1ab8a1-2efb77bb-fe7d59fc    10000935  51178377   \n",
       "17      9b314ad7-fbcb0422-6db62dfc-732858d0-a5527d8b    10000935  51178377   \n",
       "18      6a5c3985-7764bdd0-ec5a6a74-af78bcaa-4ca33ec3    10000935  55697293   \n",
       "19      c50494f1-90e2bff5-e9189550-1a4562fd-6ab5204c    10000935  55697293   \n",
       "20      8e3f2822-0c1d4b71-2a265bbf-5b96e531-ccf5fa30    10000935  56164612   \n",
       "21      ad13cf84-62c34a01-a01b9e87-2581a359-83bbc046    10000935  56164612   \n",
       "22      6fa5997e-b1dfecf8-3c174666-8815c84a-32db59ff    10000935  56522600   \n",
       "23      f1adcae3-2921c0a8-5d9652f9-4191ecd7-f2a96f35    10000935  56522600   \n",
       "24      88498b37-c21dc7ba-bc202800-b517a62d-f7ac5bcf    10000935  58219844   \n",
       "25      6ad03ed1-97ee17ee-9cf8b320-f7011003-cd93b42d    10000980  50985099   \n",
       "26      6b360eca-17d2ae1a-19126084-78e9c85d-9800d216    10000980  50985099   \n",
       "27      943486a3-b3fa9ff7-50f5a769-7a62fcbb-f39b6da4    10000980  51967283   \n",
       "28      96f9a77c-59b47dfb-0cac64db-6538421a-a6b135e2    10000980  54577367   \n",
       "29      cfb03587-782edf6c-1bf392e1-98196cd5-365d69e8    10000980  54577367   \n",
       "...                                              ...         ...       ...   \n",
       "377080  af21a5e8-fcfe93ac-84b1c519-61a17957-1bbeccb6    19999068  52793893   \n",
       "377081  86292c07-43af46ea-0649f4b2-feecce77-91866b79    19999068  54061983   \n",
       "377082  027ce998-e2d551f7-a8a283f4-72d4b80e-d849d761    19999068  57399028   \n",
       "377083  f43d99e3-05307f9a-81e228f8-ea2eb9c2-0af14ecd    19999068  58313972   \n",
       "377084  4ae61651-0f1f0738-07991a0b-42f9e4d2-74cb632c    19999068  59390811   \n",
       "377085  458b41b5-6c8bfc44-a4dff92c-254519d0-9c40e805    19999068  59937395   \n",
       "377086  8dc9f5e1-14887015-8db378ef-2fd4441a-d45ee0f3    19999156  50847545   \n",
       "377087  bd4eb73d-09c65a7e-797c197f-ae864491-8d258918    19999156  50847545   \n",
       "377088  9c44a9e5-7a8fcfa1-cb6b451f-1d1ac993-5312b1c2    19999270  55274188   \n",
       "377089  f5cbd087-0490844c-4e1d38f7-154dc31a-4e99dbd1    19999270  55274188   \n",
       "377090  bf6e3bdb-4ec92b48-340d4e48-7a00234a-1b76dae7    19999270  56267753   \n",
       "377091  f83a9135-334e26ff-6419b6b9-1457634a-aaed76a3    19999270  56267753   \n",
       "377092  c8bbb9ff-ecb81ef7-a1a6cecf-f535bd20-bd512ba0    19999287  50000173   \n",
       "377093  ef49bac7-16939860-5a4f182e-c568720f-e0c9d278    19999287  50574077   \n",
       "377094  50373d1b-8df0d15f-5d0047f4-578bc509-4f2b48f0    19999287  51885769   \n",
       "377095  46510b80-411ac511-fe6ffab2-d7dfdc76-dff1a762    19999287  52519175   \n",
       "377096  f7e95a22-cb958055-47114ddf-38532ef4-b4c172d5    19999287  52519175   \n",
       "377097  431c5823-ea54bba2-f21af1d4-45887776-9de7edd1    19999287  53255195   \n",
       "377098  5a5eddf4-b64e5e49-f6e9c8bc-d6409b00-015470ea    19999287  53282218   \n",
       "377099  9d1b4abe-52d55ff8-25dd0af8-bae63de6-0f2e36e5    19999287  53282218   \n",
       "377100  2eb70dfe-52fa728e-a36e09be-ec0ed3cf-0a2ea7f0    19999287  58938059   \n",
       "377101  53e9b6d0-5d5317f5-f1a4c031-01d40558-fd14a425    19999376  57540554   \n",
       "377102  ee9155f3-944c056b-c76c73d0-3f792f2c-92ae461e    19999442  58497551   \n",
       "377103  16b6c70f-6d36bd77-89d2fef4-9c4b8b0a-79c69135    19999442  58708861   \n",
       "377104  3fcd0406-9b111603-feae7033-96632b3a-111333e5    19999733  57132437   \n",
       "377105  428e2c18-5721d8f3-35a05001-36f3d080-9053b83c    19999733  57132437   \n",
       "377106  58c403aa-35ff8bd9-73e39f54-8dc9cc5d-e0ec3fa9    19999733  57132437   \n",
       "377107  58766883-376a15ce-3b323a28-6af950a0-16b793bd    19999987  55368167   \n",
       "377108  7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08    19999987  58621812   \n",
       "377109  1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e    19999987  58971208   \n",
       "\n",
       "       PerformedProcedureStepDescription ViewPosition  Rows  Columns  \\\n",
       "0                     CHEST (PA AND LAT)           PA  3056     2544   \n",
       "1                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "2                     CHEST (PA AND LAT)           PA  3056     2544   \n",
       "3                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "4                    CHEST (PORTABLE AP)           AP  2705     2539   \n",
       "5                    CHEST (PORTABLE AP)           AP  2906     2258   \n",
       "6                    CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "7                     CHEST (PA AND LAT)           AP  2544     3056   \n",
       "8                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "9                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "10                    CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "11                    CHEST (PA AND LAT)           PA  2544     3056   \n",
       "12                    CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "13                    CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "14                    CHEST (PA AND LAT)           PA  3056     2544   \n",
       "15                   CHEST (PORTABLE AP)           AP  2870     2402   \n",
       "16                    CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "17                    CHEST (PA AND LAT)           AP  3056     2544   \n",
       "18                                   NaN           LL  2022     1840   \n",
       "19                                   NaN           PA  2022     1840   \n",
       "20                    CHEST (PA AND LAT)           AP  2544     3056   \n",
       "21                    CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "22                                   NaN           LL  2022     1736   \n",
       "23                                   NaN           PA  2022     1736   \n",
       "24                   CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "25                                   NaN           PA  2022     1870   \n",
       "26                                   NaN           LL  2022     1870   \n",
       "27                   CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "28                    CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "29                    CHEST (PA AND LAT)           AP  2544     3056   \n",
       "...                                  ...          ...   ...      ...   \n",
       "377080               CHEST (PORTABLE AP)          NaN  2140     1760   \n",
       "377081               CHEST (PORTABLE AP)          NaN  2140     1760   \n",
       "377082               CHEST (PORTABLE AP)          NaN  2140     1760   \n",
       "377083               CHEST (PORTABLE AP)          NaN  2140     1760   \n",
       "377084               CHEST (PORTABLE AP)          NaN  2140     1760   \n",
       "377085               CHEST (PORTABLE AP)          NaN  2140     1760   \n",
       "377086                CHEST (PA AND LAT)           PA  3056     2544   \n",
       "377087                CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "377088                CHEST (PA AND LAT)          NaN  2140     1760   \n",
       "377089                CHEST (PA AND LAT)          NaN  2140     1760   \n",
       "377090                CHEST (PA AND LAT)          NaN  2140     1760   \n",
       "377091                CHEST (PA AND LAT)          NaN  2140     1760   \n",
       "377092               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "377093               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377094               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "377095               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377096               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377097               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377098                               NaN           PA  2022     1606   \n",
       "377099                               NaN           LL  2022     1776   \n",
       "377100               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "377101               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "377102               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377103               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377104                CHEST (PA AND LAT)           PA  3056     2544   \n",
       "377105                CHEST (PA AND LAT)           PA  3056     2544   \n",
       "377106                CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "377107               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377108               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "377109               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "\n",
       "        StudyDate   StudyTime ProcedureCodeSequence_CodeMeaning  \\\n",
       "0        21800506  213014.531                CHEST (PA AND LAT)   \n",
       "1        21800506  213014.531                CHEST (PA AND LAT)   \n",
       "2        21800626  165500.312                CHEST (PA AND LAT)   \n",
       "3        21800626  165500.312                CHEST (PA AND LAT)   \n",
       "4        21800723   80556.875               CHEST (PORTABLE AP)   \n",
       "5        21800723   80556.875               CHEST (PORTABLE AP)   \n",
       "6        21800805  234424.765               CHEST (PORTABLE AP)   \n",
       "7        21321015   84047.984                CHEST (PA AND LAT)   \n",
       "8        21321015   84047.984                CHEST (PA AND LAT)   \n",
       "9        21321015   84047.984                CHEST (PA AND LAT)   \n",
       "10       21880312  125501.842                CHEST (PA AND LAT)   \n",
       "11       21880312  125501.842                CHEST (PA AND LAT)   \n",
       "12       21880113  140127.546                CHEST (PA AND LAT)   \n",
       "13       21880113  140127.546                CHEST (PA AND LAT)   \n",
       "14       21880113  140127.546                CHEST (PA AND LAT)   \n",
       "15       21871016  123945.421               CHEST (PORTABLE AP)   \n",
       "16       21870823  191426.062                CHEST (PA AND LAT)   \n",
       "17       21870823  191426.062                CHEST (PA AND LAT)   \n",
       "18       21870226   85443.000                CHEST (PA AND LAT)   \n",
       "19       21870226   85443.000                CHEST (PA AND LAT)   \n",
       "20       21870711  111436.562                CHEST (PA AND LAT)   \n",
       "21       21870711  111436.562                CHEST (PA AND LAT)   \n",
       "22       21860730  155005.000                CHEST (PA AND LAT)   \n",
       "23       21860730  155005.000                CHEST (PA AND LAT)   \n",
       "24       21871010  125551.703               CHEST (PORTABLE AP)   \n",
       "25       21910917  130354.000                CHEST (PA AND LAT)   \n",
       "26       21910917  130354.000                CHEST (PA AND LAT)   \n",
       "27       21890627   64348.484               CHEST (PORTABLE AP)   \n",
       "28       21910613   84554.234                CHEST (PA AND LAT)   \n",
       "29       21910613   84554.234                CHEST (PA AND LAT)   \n",
       "...           ...         ...                               ...   \n",
       "377080   21610824   61741.000               CHEST (PORTABLE AP)   \n",
       "377081   21610826  220516.000               CHEST (PORTABLE AP)   \n",
       "377082   21610829   51136.000               CHEST (PORTABLE AP)   \n",
       "377083   21610825  162956.000               CHEST (PORTABLE AP)   \n",
       "377084   21610827   53735.000               CHEST (PORTABLE AP)   \n",
       "377085   21610825  112914.000               CHEST (PORTABLE AP)   \n",
       "377086   21230127  145009.656                CHEST (PA AND LAT)   \n",
       "377087   21230127  145009.656                CHEST (PA AND LAT)   \n",
       "377088   21821112  104527.000                CHEST (PA AND LAT)   \n",
       "377089   21821112  104527.000                CHEST (PA AND LAT)   \n",
       "377090   21831203  135727.000                CHEST (PA AND LAT)   \n",
       "377091   21831203  135727.000                CHEST (PA AND LAT)   \n",
       "377092   21970808   31549.890               CHEST (PORTABLE AP)   \n",
       "377093   21970805   51156.546               CHEST (PORTABLE AP)   \n",
       "377094   21970726   25712.640               CHEST (PORTABLE AP)   \n",
       "377095   21970807   90637.796               CHEST (PORTABLE AP)   \n",
       "377096   21970807   90637.796               CHEST (PORTABLE AP)   \n",
       "377097   21970803  194419.125               CHEST (PORTABLE AP)   \n",
       "377098   21961127  112117.000                CHEST (PA AND LAT)   \n",
       "377099   21961127  112117.000                CHEST (PA AND LAT)   \n",
       "377100   21970805   93746.343               CHEST (PORTABLE AP)   \n",
       "377101   21450731   45417.656               CHEST (PORTABLE AP)   \n",
       "377102   21481128  133244.093               CHEST (PORTABLE AP)   \n",
       "377103   21481119  224703.375               CHEST (PORTABLE AP)   \n",
       "377104   21520708  224550.171                CHEST (PA AND LAT)   \n",
       "377105   21520708  224550.171                CHEST (PA AND LAT)   \n",
       "377106   21520708  224550.171                CHEST (PA AND LAT)   \n",
       "377107   21451104   51448.218               CHEST (PORTABLE AP)   \n",
       "377108   21451102  202809.234               CHEST (PORTABLE AP)   \n",
       "377109   21451103   50507.625               CHEST (PORTABLE AP)   \n",
       "\n",
       "       ViewCodeSequence_CodeMeaning PatientOrientationCodeSequence_CodeMeaning  \n",
       "0                  postero-anterior                                      Erect  \n",
       "1                           lateral                                      Erect  \n",
       "2                  postero-anterior                                      Erect  \n",
       "3                           lateral                                      Erect  \n",
       "4                  antero-posterior                                        NaN  \n",
       "5                  antero-posterior                                      Erect  \n",
       "6                  antero-posterior                                        NaN  \n",
       "7                  antero-posterior                                      Erect  \n",
       "8                           lateral                                      Erect  \n",
       "9                           lateral                                      Erect  \n",
       "10                          lateral                                      Erect  \n",
       "11                 postero-anterior                                      Erect  \n",
       "12                          lateral                                      Erect  \n",
       "13                          lateral                                      Erect  \n",
       "14                 postero-anterior                                      Erect  \n",
       "15                 antero-posterior                                      Erect  \n",
       "16                          lateral                                      Erect  \n",
       "17                 antero-posterior                                      Erect  \n",
       "18                     left lateral                                      Erect  \n",
       "19                 postero-anterior                                      Erect  \n",
       "20                 antero-posterior                                      Erect  \n",
       "21                          lateral                                      Erect  \n",
       "22                     left lateral                                      Erect  \n",
       "23                 postero-anterior                                      Erect  \n",
       "24                 antero-posterior                                      Erect  \n",
       "25                 postero-anterior                                      Erect  \n",
       "26                     left lateral                                      Erect  \n",
       "27                 antero-posterior                                      Erect  \n",
       "28                          lateral                                      Erect  \n",
       "29                 antero-posterior                                      Erect  \n",
       "...                             ...                                        ...  \n",
       "377080                          NaN                                        NaN  \n",
       "377081                          NaN                                        NaN  \n",
       "377082                          NaN                                        NaN  \n",
       "377083                          NaN                                        NaN  \n",
       "377084                          NaN                                        NaN  \n",
       "377085                          NaN                                        NaN  \n",
       "377086             postero-anterior                                      Erect  \n",
       "377087                      lateral                                      Erect  \n",
       "377088                          NaN                                        NaN  \n",
       "377089                          NaN                                        NaN  \n",
       "377090                          NaN                                        NaN  \n",
       "377091                          NaN                                        NaN  \n",
       "377092             antero-posterior                                      Erect  \n",
       "377093             antero-posterior                                      Erect  \n",
       "377094             antero-posterior                                      Erect  \n",
       "377095             antero-posterior                                        NaN  \n",
       "377096             antero-posterior                                      Erect  \n",
       "377097             antero-posterior                                      Erect  \n",
       "377098             postero-anterior                                      Erect  \n",
       "377099                 left lateral                                      Erect  \n",
       "377100             antero-posterior                                      Erect  \n",
       "377101             antero-posterior                                      Erect  \n",
       "377102             antero-posterior                                        NaN  \n",
       "377103             antero-posterior                                      Erect  \n",
       "377104             postero-anterior                                      Erect  \n",
       "377105             postero-anterior                                      Erect  \n",
       "377106                      lateral                                      Erect  \n",
       "377107             antero-posterior                                      Erect  \n",
       "377108             antero-posterior                                      Erect  \n",
       "377109             antero-posterior                                  Recumbent  \n",
       "\n",
       "[377110 rows x 12 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52e964a2-7fa373f2-9b88b009-123e8817-48889d6b\n",
      "tensor(0)\n",
      "[CLS] final report history : copd with dyspnea on exertion. findings : in comparison with study of _ _ _, there is again evidence of severe copd with apparent bullous changes in the apices. old healed rib fractures are noted on the right. however, there is no evidence of acute pneumonia or vascular congestion at this time. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "num = 10\n",
    "print(data['image_id'][num])\n",
    "print(data['y'][num])\n",
    "print(mimic_feature_dataset_neg_1.tokenizer.decode(data['text'][num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(test_csv, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WET READ: ___ ___ ___ 7:52 AM\n",
      "  \n",
      "  \n",
      "  \n",
      "  Compared with the chest radiograph from earlier on the same date, there is\n",
      "  worsening opacification of the right lung field, in keeping with history of\n",
      "  worsening hemothorax and bloody pleural fluid from the chest catheter.  There\n",
      "  has been no change to the position of the right pleural catheter.  The trachea\n",
      "  is now midline; previously it was deviated slightly to the right due to volume\n",
      "  loss.  Left upper lung is grossly clear.\n",
      "  \n",
      "  The above findings were communicated via telephone by Dr. ___ to Dr.\n",
      "  ___ at 20:17 on ___, ___ min after discovery.\n",
      " WET READ VERSION #1 ___ ___ ___ 8:19 PM\n",
      "  Compared with the chest radiograph from earlier on the same date, there is\n",
      "  worsening opacification of the right lung field, in keeping with history of\n",
      "  worsening hemothorax and bloody pleural fluid from the chest catheter.  There\n",
      "  has been no change to the position of the right pleural catheter.  The trachea\n",
      "  is now midline; previously it was deviated slightly to the right due to volume\n",
      "  loss.  Left upper lung is grossly clear.\n",
      "  \n",
      "  The above findings were communicated via telephone by Dr. ___ to Dr.\n",
      "  ___ at 20:17 on ___, ___ min after discovery.\n",
      " ______________________________________________________________________________\n",
      "                                 FINAL REPORT\n",
      " EXAMINATION:  CHEST (PORTABLE AP)\n",
      " \n",
      " INDICATION:  ___ year old man with worsening anemia and chest tube with bloody\n",
      " pleural fluid.  // evaluate for worsening effusion, consolidation     \n",
      " evaluate for worsening effusion, consolidation\n",
      " \n",
      " COMPARISON:  Comparison to ___ at 09:38\n",
      " \n",
      " FINDINGS: \n",
      " \n",
      " Portable semi-erect chest radiograph ___ at 19:49 is submitted.\n",
      " \n",
      " IMPRESSION: \n",
      " \n",
      " Interval increase in size of large layering right pleural collection\n",
      " consistent with hemothorax given history of bloody pleural fluid coming from\n",
      " the chest tube. Right chest tube remains unchanged in position.  There is now\n",
      " slight shift to the mediastinal and cardiac structures to the left.  There is\n",
      " improved aeration at the left base with interval appearance of patchy opacity\n",
      " in the left mid lung which may reflect an area of atelectasis, although\n",
      " pneumonia cannot be excluded.  No pneumothorax.  Heart remains enlarged.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_df[train_df['ambiguous'] == 1]['report'].head(5).tolist()[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model on -1 samples (Ambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data comes from feature extractor & splits. Feature extractor gives features in .npy file (features, bbox, spatials, class, probs) and \n",
    "csv file containing:\n",
    "    dicom_id\n",
    "    feature_path\n",
    "    report\n",
    "    report_path\n",
    "    Pneumonia/not Pneumonia\n",
    "'''\n",
    "#root dirs\n",
    "root_dir = '/gpfs/ysm/home/kl533/Lab/Transformers/transforming_embracement/pneumonia_neg_1_feats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_1 = os.path.join(root_dir, 'img_infos.csv')\n",
    "neg_1_df = pd.read_csv(neg_1)\n",
    "#splits = splits[['dicom_id','split']]\n",
    "neg_1_splits = pd.merge(neg_1_df, splits, on='dicom_id')\n",
    "neg_1_train = neg_1_splits[neg_1_splits['split']=='train']\n",
    "neg_1_validate = neg_1_splits[neg_1_splits['split']=='validate']\n",
    "neg_1_test = neg_1_splits[neg_1_splits['split']=='test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_1_train) + len(neg_1_validate) + len(neg_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_feature_dataset_neg_1 = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=neg_1,root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_neg_1 = DataLoader(mimic_feature_dataset_neg_1, batch_size=24, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=8, Nx=6, d_model=512, ff_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (self_attn_A): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (self_attn_B): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 512)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weights\n",
    "PATH = './weights/transforming_embracement_multiplied_leaky.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 827 test samples: 53.32527206771463\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_list = []\n",
    "output_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in mimic_feature_dataloader_neg_1:\n",
    "        features = data['features'].to(device)\n",
    "        spatials = data['spatials'].to(device)\n",
    "        text = data['text'].to(device)\n",
    "        labels = data['y'].to(device)\n",
    "        img_attn_mask = data['img_attn_mask'].to(device)\n",
    "        text_attn_mask = data['text_attn_mask'].to(device)\n",
    "        outputs = model([features, spatials], text,img_attn_mask, text_attn_mask)\n",
    "        #outputs = model([torch.rand_like(features), torch.rand_like(spatials)], text, img_attn_mask, text_attn_mask)#noise\n",
    "        #outputs = model([features, spatials], torch.randint_like(text, 30000), img_attn_mask, text_attn_mask)\n",
    "        output_list.extend(outputs.tolist())\n",
    "        label_list.extend(labels.tolist())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Accuracy of the network on the {} test samples: {}'.format(len(mimic_feature_dataset_neg_1), (100 * correct / total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor(output_list), 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99cee6c2-0967d1e5-39dd3ad3-1dbb8bb4-1774e25f'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] final report examination : chest ( portable ap ) indication : _ _ _ f with sob and altered mental status, eval pna vs edema comparison : _ _ _ findings : ap portable upright view of the chest. cardiomegaly appears unchanged with bilateral small pleural effusions and lower lobe atelectasis. there is hilar congestion. airspace opacities in the right mid to lower lung could reflect pneumonia. no large pneumothorax. densely calcified tracheobronchial tree appears slightly kinked along the superior mediastinum though this is similar to the prior exam. bony structures appear grossly intact. impression : cardiomegaly with hilar congestion, small bilateral effusions and probable pneumonia in the right mid to lower lung. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 737, 0: 90})"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(torch.max(F.softmax(torch.tensor(output_list), dim=1), 1)[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(F.softmax(outputs.data, dim=1), 1)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71544da2-e9222499-7012cc6e-1d843708-358250c9\n",
      "tensor(0)\n",
      "[CLS] final report reason for examination : evaluation of the patient with history of alcoholism and delirium tremens after extubation. ap radiograph of the chest was reviewed in comparison to _ _ _. right internal jugular line tip is at the level of mid svc. heart size and mediastinum are stable. there is interval improvement of right basal consolidation. still present bilateral pleural effusions are noted. improvement in the consolidation is most likely consistent with improvement of infection / aspiration. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-5167ebe260ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmimic_feature_dataset_neg_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "print(data['image_id'][num])\n",
    "print(data['y'][num])\n",
    "print(mimic_feature_dataset_neg_1.tokenizer.decode(data['text'][num]))\n",
    "print(data[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7978, 0.4434, 0.0230,  ..., 0.2872, 0.0277, 0.0000],\n",
       "        [1.2022, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],\n",
       "        [0.2769, 1.0015, 0.0000,  ..., 0.7345, 0.0000, 0.0000],\n",
       "        [0.9143, 0.3289, 0.0691,  ..., 0.0000, 0.0832, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['features'][10][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] final report indication : history of breast cancer and malignant effusion, presenting with fever and hypoxia to _ _ _ %. assess for pleural effusion and / or pneumonia. comparison : chest radiograph from _ _ _. findings : a single frontal portable radiograph of the chest was acquired. small - to - moderate bilateral pleural effusions are increased compared to the prior study from _ _ _. consolidative opacities at both lung bases likely reflect compressive atelectasis and pleural effusions, although concomitant infection at either lung base is certainly possible. there is engorgement of the pulmonary vasculature with mild interstitial pulmonary edema. background emphysematous changes are redemonstrated. the heart size is top normal. the descending thoracic aorta is slightly tortuous, as before. there is no pneumothorax. impression : 1. increased moderate bilateral pleural effusions with bilateral lower lobe consolidative opacities, likely compressive atelectasis given the adjacent effusions, although infection at either lung base is certainly possible. 2. mild interstitial pulmonary edema. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_feature_dataset_neg_1.tokenizer.decode(text[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe = OneHotEncoder()\n",
    "#label_list = ohe.fit_transform(np.asarray(label_list).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(label_list, F.softmax(torch.tensor(output_list), dim=1)[:,1])\n",
    "AUC = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.603477740750235"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Micro AUC because there is only one class!\n",
    "plt.figure(figsize=(10,10))\n",
    "lw = 5\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % AUC)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('MATE ROC w/ Multiplied Hidden states for Pneumonia')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.13      0.22       498\n",
      "           1       0.42      0.95      0.58       329\n",
      "\n",
      "    accuracy                           0.45       827\n",
      "   macro avg       0.61      0.54      0.40       827\n",
      "weighted avg       0.65      0.45      0.36       827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_list, np.argmax(F.softmax(torch.as_tensor(output_list), dim=1), axis=1).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = [0.840, 0.719, 0.702, 0.669, 0.634, 0.613, 0.592, 0.567, 0.475, 0.408, 0.380, 0.353, 0.304, 0.308, 0.286, 0.282, 0.266, 0.264, 0.267, 0.264, 0.259, 0.254, 0.240, 0.238, 0.223, 0.237, 0.243, 0.227, 0.219, 0.221, 0.217, 0.216, 0.211, 0.200, 0.208, 0.203, 0.211, 0.192, 0.190, 0.203, 0.199, 0.207, 0.186, 0.205, 0.193, 0.196, 0.201, 0.188, 0.193, 0.186, 0.184, 0.199, 0.195, 0.187, 0.184, 0.191, 0.196, 0.196, 0.187, 0.199, 0.180, 0.188, 0.201, 0.199, 0.185, 0.198, 0.191, 0.197, 0.180, 0.192, 0.194, 0.197, 0.186, 0.197, 0.189, 0.190, 0.185, 0.204, 0.200, 0.195, 0.192, 0.189, 0.199, 0.189, 0.196, 0.192, 0.203, 0.178, 0.175, 0.196, 0.197, 0.202, 0.195, 0.195, 0.196, 0.182, 0.190, 0.180, 0.195, 0.201, 0.186, 0.195, 0.195, 0.193, 0.187, 0.193, 0.190, 0.196, 0.181, 0.205, 0.187, 0.196, 0.199, 0.183, 0.187, 0.201, 0.193, 0.189, 0.207, 0.192, 0.194, 0.195, 0.195, 0.196, 0.191, 0.180, 0.202, 0.187, 0.194, 0.190, 0.190, 0.192, 0.193, 0.192, 0.202, 0.194, 0.186, 0.185, 0.199, 0.196, 0.184, 0.195, 0.195, 0.197, 0.189, 0.195, 0.196, 0.200, 0.189, 0.197, 0.189, 0.186, 0.189, 0.196, 0.184, 0.194, 0.182, 0.197, 0.206, 0.178, 0.200, 0.198, 0.182, 0.205, 0.196, 0.202, 0.193, 0.189, 0.194, 0.186, 0.198, 0.181, 0.185, 0.192, 0.195, 0.192, 0.191, 0.189, 0.204, 0.198, 0.192, 0.191, 0.185, 0.193, 0.195, 0.195, 0.192, 0.177, 0.190, 0.197, 0.188, 0.195, 0.193, 0.179, 0.205, 0.191, 0.191, 0.202, 0.191, 0.197]\n",
    "valid_losses = [0.6551180481910706, 0.5125117301940918, 0.35630908608436584, 0.327761709690094, 0.3096001148223877, 0.25478067994117737, 0.31744229793548584, 0.27848029136657715, 0.2852853238582611, 0.2723208963871002, 0.26577574014663696, 0.26251327991485596, 0.2957403361797333, 0.2850898504257202, 0.28291958570480347, 0.2838857173919678, 0.2858005464076996, 0.26507440209388733, 0.25818362832069397, 0.2840954065322876, 0.26475095748901367, 0.246206596493721, 0.29353299736976624, 0.25475946068763733, 0.26366496086120605, 0.26031923294067383, 0.2550439238548279, 0.310778945684433, 0.26753565669059753, 0.25684309005737305, 0.25910377502441406, 0.27806609869003296, 0.27536746859550476, 0.32196709513664246, 0.236615389585495, 0.26282182335853577, 0.2900902330875397, 0.25510773062705994, 0.2310592085123062, 0.2458401322364807, 0.27914029359817505, 0.26502206921577454, 0.26396599411964417, 0.24301722645759583, 0.251845121383667, 0.2969387173652649, 0.2520844042301178, 0.2482447624206543, 0.2943507432937622, 0.24974343180656433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(r'(?<=Validation loss: ).*', re.MULTILINE)\n",
    "p.match(\"Validation loss: 44fd\")\n",
    "print(p.match(\"Validation loss: f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aacf212c6a0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aacf2334ef0>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEDCAYAAADX1GjKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3zV9d338dcnkzCSQFghgGwB2Yog4AYvAUVw1r3q1vvqsK2ttva67tZ6W7Wt1VZbaRH3AIpaHFBBRfYeYcoeCZCQAdk53/uPcxJDyDhJTnJykvfz8TiPwDm/k/PJD3Le5zt/5pxDRESat7BgFyAiIsGnMBAREYWBiIgoDEREBIWBiIgAEcEuoDbMrAhvkGUFuxYRkRASC3icc6e991soTi01Mw9gcXFxwS5FRCRkZGZmAjjn3Gm9QiHZMgCy4uLi4jIyMoJdh4hIyIiPjyczM7PCHhWNGYiIiMJAREQUBiIigsJARERQGIiICAoDERGhmYWBc441+47zu0+2UFTsCXY5IiKNRqiuM6iVlKw8rv7LEgDG9m7PBf06BLkiEZHGoVm1DBLjYhjZoy0AH64/FORqREQaj2YVBgBXDu0CwGebUsgrLA5yNSIijUOzC4NJgxMJDzOy84tYtO1osMsREWkUml0YtG8dzZjeCQB8tEFdRSIi0AzDAGCKr6voP1tSOZlfFORqRESCr1mGwWVndSYqPIy8Qg/zk1ODXY6ISNA1yzCIi4nkojO900o1q0hEpJmGAcCUYd6uoq+2HyUjpyDI1YiIBFezDYNL+3eiZVQ4RR7HJ5tSgl2OiEhQNdswiIkKZ8LATgB8uE5dRSLSvDXbMIDvZhUt251GalZekKsREQmeZh0G5/ftQFxMJM7BvzccDnY5IiJB06zDICoijEmDOwOaVSQizVuzDgOAK4d4u4rW7c9gX1pOkKsREQmOZh8Go3ol0LFNNKDtKUSk+Wr2YRAeZkwekgjAR+oqEpFmqtmHAXw3q2hrSjbbU7ODXI2ISMNTGADDusXTrV0MoDUHItI8KQwAMysdSP5w/SGcc0GuSESkYSkMfEr2KtqXnsOGA5lBrkZEpGEpDHz6d46lX6fWgNYciEjzozAoo6Sr6OMNhyj2qKtIRJoPhUEZV/pmFaVm5bNid3qQqxERaTgKgzJ6tG/F0K5xgLqKRKR5URiUM2VYEuDtKsorLA5yNSIiDUNhUM7UYV2IDDey84r4VBe9EZFmwq8wMLPWZvaCmR02s1wzW2VmU/x43h4zc5Xctta9/MBLaB3N+AHei968u3J/kKsREWkY/rYM5gA3A08Ak4FkYI6ZTarmedOA88rd7vU99q8aV9tArh/ZDYClu9LYm3YyyNWIiNS/asPA94Y/Hvi+c266c+4L4HZgKfBcVc91zq11zi0rewMG+x7+Rx1rrzcX9O1AYlwLAN5fdSDI1YiI1D9/WgbTgExgbskdzrtfw2tAfzMb6O+LmVkUcBOw2Dm3vYa1NpjwMOPas7sC8MHqA1pzICJNnj9hMAhIds55yt2/oczj/poKJNCIWwUlrjvb21WUkpXHVzuOBrkaEZH65U8YJAAVrcBKL/O4v+4CTgDvVXWQmWVUdQPiavCatdI9oSXn9fL+aO9pIFlEmjh/B5Cr6ifxqw/FzLoCE4B3nXMhMSp7g28gecGWVNJO5Ae5GhGR+uNPGKRR8af/dr6v/u7bcIfv9artInLOxVd1wzuGUe8uH9SZNi0iKCx2zFl7sCFeUkQkKPwJg83AADMrf2zJrKBN1X0DMzO8YbDVObekRhUGUYvIcKb6ViS/u3K/rnMgIk2WP2EwB4gHrix3/23ANudcsh/f40KgNyEwcFze9ed4u4p2HDnBuv0ZQa5GRKR++BMG84CFwHQzu8vMLjazGcA44CclB5nZIjOr7KPzXUARMLOO9Ta4QUmxDEiMBeC9VRpIFpGmqdow8K0pmAq8AzwFfAIMAa52zn1U3fPNrA1wDTDPOZdat3IbnplxwzneNQcfrT9MTkFRkCsSEQk8v2YTOeeynHMPO+c6O+daOOdGOOf+Ve6Yi5xzVsFzs51zrZxzVwWq6IY2dXgSUeFhnMgvYt5GbV4nIk2Pdi31Q3zLKC47y7t5ndYciEhTpDDwU8magxV70tl19ESQqxERCSyFgZ/G9m5PUnwMAO9p8zoRaWIUBn4KCzOu8w0kz1pzgKLi8ls1iYiELoVBDVx7dlfM4Gh2Pou2afM6EWk6FAY10LVtS8b1aQ/Au1pzICJNiMKghkpWJH+x9QipWXlBrkZEJDAUBjV02VmdSGgVRbHHMWPJnmCXIyISEAqDGoqOCOf2MT0AeGPZXrLzCoNbkIhIACgMauG2886gZVQ42XlFvLV8X7DLERGpM4VBLcS3jOJ7I7sDMH3xbvKLioNckYhI3SgMaunu83sSEWYcyc5n7tpDwS5HRKROFAa1lBQfw5ShXQB4+atv8Xh04RsRCV0Kgzq498JeAOw6epL5W0Jud24RkVIKgzro3zmWi8/sAMDLX36ry2KKSMhSGNTR/Rf2BmDtvgxW7jke5GpERGpHYVBH5/Zsx/Du8YC3dSAiEooUBnVkZtx3gbd18MXWI2xLyQ5yRSIiNacwCIDLBnaiV4dWALzylVoHIhJ6FAYBEBZm3HeBd2bRh+sOcTAjN8gViYjUjMIgQKYOT6Jjm2iKPI5/LN4d7HJERGpEYRAg0RHh3DWuJwBvr9hHRk5BkCsSEfGfwiCAbhrVnTbREeQUFPP60r3BLkdExG8KgwCKbRHJzaPPAGDGkj3kFWoDOxEJDQqDALtrbA+iwsNIO1nA+6sPBLscERG/KAwCrGNsC6YO925g94GukywiIUJhUA+mDe8KwPoDmRw4nhPkakREqqcwqAfn9mxHQqsoAD7dlBLkakREqqcwqAfhYcZlZ3UGYN7Gw0GuRkSkegqDejJpsDcM1uzLICUzL8jViIhUTWFQT0b3SiC+ZSQAn25S60BEGjeFQT2JDA/jsoGdAJincQMRaeQUBvVo4uBEAFbuSedItrqKRKTx8isMzKy1mb1gZofNLNfMVpnZFD+fa2Z2r5mtNrMcM8sws2VmNqZupTd+Y3u3p02LCJyDzzbrGski0nj52zKYA9wMPAFMBpKBOWY2yY/nvgo8A8wCJvm+zzygVY2rDTFREWFMGODtKvpEs4pEpBGLqO4A3xv+eOBq59wc330LgV7Ac3jf2Ct77jXAHcA459zSMg/9uw41h5SJgxOZvfYgy3enk3Yin4TW0cEuSUTkNP60DKYBmcDckjuccw54DehvZgOreO4jwFflgqBZOb9ve1pFhVPsccxPVleRiDRO/oTBICDZOecpd/+GMo+fxswigdHARjN7ysxSzazIzDab2e1VvaBvXKHSGxDnR92NQovIcC4doFlFItK4+RMGCUB6Bfenl3m8sudFA7cDVwEPAxOBjcAMM7unZqWGrpIFaEt2HtNFb0SkUfJ3ANnV4rGS790CmOSce985Nx+4EVgJ/KrSb+hcfFU3vN1WIePCfh2JiQynSF1FItJI+RMGaVT86b+d72tFrQaA43iDYqtzrvSyX77xhk+BrmbWsQa1hqyYqHAu7t8BgE/UVSQijZA/YbAZGGBm5Y8d7Pu6qaInOedygZ2VfE/zfS0/DtFkTRzkXYC2eMcxsvIKg1yNiMip/AmDOUA8cGW5+28Dtjnnkqt47my8QdKj5A4zM7xjB7ucc8dqVG0Iu7h/R6Ijwigo9vDFliPBLkdE5BT+hME8YCEw3czuMrOLzWwGMA74SclBZrbIzMqPH/weSAU+NbMbzWwi8D5wNvCLQPwAoaJ1dAQX9vN2FWlbaxFpbKoNA18f/1TgHeAp4BNgCN5FaB9V89w04Hy8M4j+greVcQYwzTn3bt1KDz2TfHsVLdp+lBP5RUGuRkTkO9WuQAZwzmXhnRr6cBXHXFTJ/XuA62pRW5NzyYCORIWHUVDkYeHWI1w5tEuwSxIRAbRraYOKbRHJuL7tAfhE1zgQkUZEYdDAJg7yLkBbuPUouQXFQa5GRMRLYdDAJgzsRESYkVtYzJfbNatIRBoHhUEDi28ZxZg+3q6ieRu1AE1EGgeFQRBM8nUV/WdLqhagiUijoDAIgsvO6kxMZDgnC4r5/afbgl2OiIjCIBjatYriRxP6AfDG8r2s3lvZ9k4iIg1DYRAkd47twaCkWJyDx2ZtJL9IM4tEJHgUBkESER7G01cPITzM2HHkBC8v2hXskkSkGVMYBNGgpDjuHtcTgJcW7mTnkewgVyQizZXCIMh+OL4f3drFUFDs4eezN+LxVHUdIRGR+qEwCLKYqHCemua9NMTKPcd5e+W+IFckIs2RwqAROL9vB64engTA0/O2kpqVF+SKRKS5URg0Ek9cMZB2raLIzi/i1x9uDnY5ItLMKAwaiXatovjlFQMA73WSP9+srSpEpOEoDBqRqcOSON+3xfWv5m4mW1tViEgDURg0ImbGb6cOpkVkGClZeTyjrSpEpIEoDBqZ7gkty21VcTzIFYlIc6AwaITuGtuTAYnerSo+WL0/2OWISDOgMGiEIsLDmDCgIwDbUrQqWUTqn8KgkerXuQ0A21NP4JxWJYtI/VIYNFJndvKGwYn8Ig5lahGaiNQvhUEj1aN9KyLDDYDt6ioSkXqmMGikIsPD6N2hNQDbUhUGIlK/FAaNWD9fV5FaBiJS3xQGjdiZvkFktQxEpL4pDBqxkpbBjiMnKNZ1DkSkHikMGrGSGUUFRR72pp0McjUi0pQpDBqxrm1jiIkMB2C7uopEpB4pDBqxsDCjXyffjKKUE0GuRkSaMoVBI1c6o0gtAxGpRwqDRk4zikSkIfgVBmbW2sxeMLPDZpZrZqvMbIofz/u1mbkKbrqMl59KWga7j50kv6g4yNWISFMV4edxc4ARwE+B3cAdwBwzu9I5N8+P508AynZ6F9SkyOaspGVQ7HHsOnqSAYmxQa5IRJqiasPAzCYB44GrnXNzfPctBHoBzwH+hMEq51xGXQptrjq2iSYuJpLM3EK2p2YrDESkXvjTTTQNyATmltzhvHsqvwb0N7OB9VSb4L0UZsl6A13bQETqiz9hMAhIds55yt2/oczj1dliZsW+MYe/m1nHGlXZzPXr7J1eqhlFIlJf/BkzSAC2V3B/epnHK/Mt8AtgLd5xgrF4xx0uNbOznXMVXuDXzKrrUoqr5vEmpbRloDAQkXri7wByVRvjVPqYc+71cnd9YWbLgM+Bh4Df+Pn6zVrJjKL96bmczC+iVbS//2wiIv7x510ljYo//bfzfU2v4LFKOefmm9lh4Lwqjomv6nv4Wg7NpnVQEgbg3bRuWLcqT4+ISI35M2awGRhgZuWPHez7uqmWr1t+DEIq0bZVFB3bRAO6toGI1A9/wmAOEA9cWe7+24BtzrnkmrygmV0GdAKW1eR5zZ1WIotIffKnm2gesBCYbmYJeBed3Q6MA64qOcjMFgEXOueszH1rgZnANqAQGAM8CuwEXgrMj9A89OvUhq93HNOMIhGpF9WGgXPOmdlU4CnfLR5IxrsI7aNqnr4VeBDoAkQC+4FXgf+rRWg1o7UGIlKf/JqW4pzLAh723So75qIK7rux1pXJKfr5uomOZOdz/GQBbVtFBbkiEWlKtGtpiOjbsXXpn9VVJCKBpjAIEa2iI+jWLgZQGIhI4CkMQohWIotIfVEYhJDSq57pEpgiEmAKgxBSdq2Bd+NYEZHAUBiEkJKWQWZuIUey84NcjYg0JQqDENKrQyvCw7xr+rTeQEQCSWEQQqIjwunZvhWgGUUiElgKgxCjlcgiUh8UBiGmdEaRWgYiEkAKgxBzZuklME/g8WhGkYgEhsIgxJS0DHILizlwPDfI1YhIU6EwCDFnJLQiKsL7z6aVyCISKAqDEBMeZqWb1mncQEQCRWEQgjSjSEQCTWEQgkqubaCWgYgEisIgBJW0DL49eoLCYk+QqxGRpkBhEIJKWgaFxY49x04GuRoRaQoUBiGoS1wLWkd7r1iqGUUiEggKgxBkZvTr5JtRpEFkEQkAhUGIKnttAxGRulIYhKh+ml4qIgGkMAhRAxJjAdiTlsNjszaQV1gc5IpEJJQpDELUqJ7tmDwkEYB3Vu7nhleWcihDexWJSO0oDEKUmfHijcP5yX+diRmsP5DJlX9ezNJv04JdmoiEIIVBCDMzHrq4DzPuPJe4mEjSThZwy/TlvPr1LpzT9tYi4j+FQRNwYb8OfPzIOAYkxlLscfzm31v4wbvryC3QOIKI+Edh0ER0a9eS2Q+MYeqwLgDMXXeIaX/5hn1pOUGuTERCgcKgCYmJCucPNwzjV1cMJDzM2JqSzRV//lob2olItRQGTYyZcde4nrz5/VEktIoiK6+IV7/eFeyyRKSRUxg0UaN7JfDIJX0A+Dw5VbubikiVFAZN2MTB3nUIGTmFLNGUUxGpgsKgCesU24JzzmgLwLwNh4NcjYg0Zn6FgZm1NrMXzOywmeWa2Sozm1KTFzKvL8zMmdkfa1eu1NQkX+vgs+QUdRWJSKX8bRnMAW4GngAmA8nAHDObVIPXugfoX7PypK4mDu4MeLuKlu1SV5GIVKzaMPC94Y8Hvu+cm+6c+wK4HVgKPOfPi5hZEvAM8EgdapVaSIyLYUT3eADmbVRXUbA553hr+T7W7Dse7FJETuFPy2AakAnMLbnDefc6eA3ob2YD/fgefwW+cs7NqlWVUielXUWbUylSV1FQfbzhML+Ys5E7/rFCK8SlUfEnDAYByc658u8iG8o8XikzuxG4GHjI36LMLKOqGxDn7/eS78Ig/WQBy3enB7ma5u3TzSkAZOUV8XlySpCrEfmOP2GQAFT0DpJe5vEKmVl74E/A4865/TUvTwKhS3wMw31dRf8OcFfRNzuPcfkfv+KlhTsD+n2bovyiYr7cdrT07x+sPhDEaqQ+FBV7yCssJjuvkPSTBRzJyuNgRi57jp1k55Fsdh872Wg3kYzw87iqqq/qsReA3cCLflcEOOfiq3pcrYOamzQokbX7MvhsUwr/O+UsIsLrPqv4880pPPzWWgqKPWxN2cZ5vRMY0b1tAKptmpbtSudEflHp37/ZeYyUzDw6x7UIYlUSCM45Hn5rrV8fts7v256/3XoOMVHhDVCZ//x5R0ij4k//7XxfK+x3MLMJwA3AT4FYM4s3s5I3+Wjf3/0NI6mjkllFaScLWLGn7l1Fs9cc4IE311BQZgziybmbKfY0zk89jcF8X7fQkK5xtGsVhcfBnLUHg1yVBMKK3el+t7q/3nGMO2esIKegqPqDG5A/YbAZGGBm5Y8d7Pu6qZLnneX7/ouA42VuAPf7/jy+JsVK7XVt25Kh3QIzq+j1pXv40XvrKfY4BiXF8upt5wCw8WAm765Ub2BFnHMsSD4CwMRBiUwZ6t1ddtaaA42220D893ff/l9Du8Yx+8ExfPzIOD79wfks+NGFfPWTi1ny2CWsfHw8/3vVWYC3lXjHP1ZyMr/xBII/YTAHiAeuLHf/bcA251xyJc/7AO/AcfkbwCzfn1fUtGCpvcm+1sGnm1Jr9QneOcdLC3fyy7mbATi3Rzveumc04wd24tqzuwLwzGdbOX6yIHBFNxGbDmaRkpUHwIQy52vnkRNsOJAZzNKarKy8QuauO8gDb6xm8JOfcclzi3jhPzvYnx7Ybd13HjnBgi3eoH/got6M6N6WQUlx9O8cS5+Oreme0JIu8TF0aBPNbef14Klp3s/RK/akc/s/VpzSdRhM/nTTzAMWAtPNLAHvGMDtwDjgqpKDzGwRcKFzzgCccweA00bIzAzggHNuUR1rlxqaOCiRp+Zt5diJfFbsTue83pWO/Z/GOcfTn27llS+9n4Au7NeBl285u7Tf82eX9+ezTSlk5BTy3Pxt/Gbq4Kq+XbNT0kXUs30rendoBcCZndqwLTWbWWsOlLbapG6OnchnfnIqn25KYcm3xygs/u5DT/bRIp6fv53n52/n3B7tmDYiiUmDE4mLiazTa05f7P2dOCOhJRMGdq72+JtGdSc8DB6bvZFVe49z2/TlvHbXubRpUbc66qraloFvTcFU4B3gKeATYAhwtXPuo/otTwKpW7uWDOnqHXf/ZJP/XUXFHsfj/9pUGgSTByfy99tOHQDr0CaaH07oB8Cby/ex6aA+7ZY13/fJccLATpgZZsY1ZycB8OH6Q+QXac1BbWXkFDB98W6uf3kpI3+7gJ/P3siX249SWOyIighj/IBOPH31YO45vycd20QD3k/lP5+9kZG/XcCDb65mfnIqBUU1X4NzNDufWWu84z7fH9eT8DDz63k3jOzOM9cMwQzW7Mvg1ukryMorrPHrB5KFYn+lmWXExcXFZWRkBLuUkPPyl9/y9Cdb6dAmmmU/v7Ta/7yFxR4efX89c9cdAuD6c7ryu6uHVPi8wmIPk1/4mu2pJxjRPZ4P7h9DmJ+/HE3Z/vQczn9mIQDv338eI3t4514cycpj9O/+g8fBy7eM4PJBicEsMySt2XecB99YU9oFB9A6OoJL+nfk8kGdubBfB1pFf9cBUuxxfLPzGHPWHuTTTSnkFn4Xwp1io5l51yjO7NzG79d//vNtvPDFTtq2jGTJY5fWeIbQrNUHePSD9TjnHW+YefeoOrdUqhIfH09mZmZmRTM2tWtpMzPJ94ZzNDufVdXMKsotKOb+11eXBsFdY3vydCVBABAZHsavp3gHyNbsy9BMGZ8FW1IBaNcq6pSptx1jW3BBvw5Aw685WLLzGJc8u4gZ3+xu0NcNFOccry/dww2vLCUlK4+WUeF8b2Q3/nnnSFb/cjwv3DicSYMTTwkCgPAw44J+HfjDDcNY9cR4nr9+KOf3bU+YQWpWPj94d53fLYTcgmJmLtsLwK2jz6jVVNFrzu7KH64fRpjB+gOZ3PLqcjJygjPmpqmdzUz3hJYMSopl08EsPtmUwqheFY8bZOQUcPdrq1i91zsB7L8v7csPxvctGfOp1Jje7bliSCIfbzjM7z7ZyoSzOhEb5L7QYCsJg0v6dzwtSK8Z0ZVF246yaNtRjp3Ip33r6HqvZ19aDg+8uYbM3EJ+O28LY/q0p18n/z8NB1tuQTGPz9nIbN+HjT4dW/PyLWfTp2PrGn2fVtERXD2iK1eP6Mrqvelc9/JSthzO4sWFO/mRr8uzKh+s3k9GTiFREWHcel6P2vwoAEwdnoQZ/PDddWw8mMnEP31Nv05taN86mg5tomnfOsr3teQWRduWUQFvdSsMmqFJgxN9YXCYX10x8LT/VIczc7lt+gp2HDmBGTx5xUDuGNvT7+//+OQB/GfLEY6dyOdPC3bwyyv82b6qacrMLWT5Lm8LbMLATqc9PmFgJ9q0iCA7r4i56w5x9zj/z3Nt5BQUce/rq8jM9fZPFxY7Hp+zkXfvPS8kuvT2pp3kvtdXszXFe13vyUMSeeaaIae1AGrq7DPace8FvXn5y295aeFOLhvYiUFJla9rLfY4Xl3sbVVdMyKJDm3qFuJXDUsiPMz473fWcTgzj8OZeVUef/GZHfjnnefW6TXLUzdRMzTZt1dRalY+q8vtnrnzSDbX/GUJO46cIDLceOF7w2sUBODdKfWRS72X3JyxZA/bU7MDU3gIWrTtCEUeR3REGOf3bX/a4y0iw7myZM1BPXcVOef42ayNbE3JJiLM+OF476fflXuO896qxr8+ZEFyKlf8eTFbU7IJDzOemDyAF28cXucgKPGD8X3p27E1xR7Hj99bX+Wg/vzkFPameaeo3j2uV0Be/4ohXZj70Fh+PrE/95zfk2nDkxjXpz39O7ehfesoyjbKE+qhBamWQTN0RkIrzuoSy+ZDWfx7w+HSAc21+45z54yVZOQU0ioqnFduPYdxFbyB+ePucT15f9UBdh87yZNzN/PWPaMq7GI6kpXHuv0Z7DhygomDOtOrQ82a+oGUV1jMkax8uraNCdin5PnJ3i6icX3a0zKq4l+3a0Z05a3l+0g+nMWWw1kMSIwNyGuXN33xbj5a7x3/eXzyAO4c25M9aSeZs/YgT83bwqUDOtX5E259KPY4/rhgO3/+wrv/VfvW0bx00/BKuzhrq0VkOM9eN5Sr/7qEbanZvPCfHfzkvyq+BMvfvvLOrBs/oGONu6eqMigprtIWSbHHkX6ygKPZ+fWylYXCoJmaNDiRzYey+HRTCr+6YiBf7jjKg2+sIbewmIRWUcy481wGd6399k/REeE8eeVA7vjnSpbuSuPjDYe5dEBHNh3MYt3+46zbn8G6fRkcKtMc/uc3e/jXQ2Po2rZlIH7EamXmFrJm73FW7kln5Z501h/IpKDIQ1J8DFOGdWHqsKQazSwpr6DIU7oxXUVdRCVGdI+nZ/tW7D52klmrD/BEPXSrLdl5jKfmbQHg6uFJ3DGmB+ANhS+2HvGOH/w7mT9+b3jAX7sujp8s4P+8s5avdxwD4Jwz2vLSzSPoFFs/+zkN7RbP/Rf24qWF3/LXRd9y2cDOp60BWbUnnTX7vDMZ7zk/MK0Cf4SHGR3aRNdbYGtqaTO1+9hJLn52EeCdJTRz6R6KPI6ubWN4/e5R9GzfKiCvc8/MVcxPTiU6Iowij6tw5XNUeBhmkF/koW/H1nzwwJh6mV6XdiKfJd+m+d78j7M1JYvq/vv379yGqcOTmDK0C13iY2r0el9tP8pt/1iBGSz/xaV0bFP5G9iLX+zg2c+30751NMt+fklANhIsceB4DlNe/Ib0kwWc1SWWWQ+MoUXkd58s3125j5/N2gjA63efy/l9OwTsteti44FM7n9jNQczcgG4c2wPfjFpAJEBPDcVyS8qZsqfv2FbajZ9Orbm40fGnXK+7p25is+TUxnaNY5/PTS22kkVjUlVU0sVBs3YxD99zZbDWaV/79+5DTPvOpeOAfzUtT89h/HPf0l+mel6vdq3Yli3eIZ2i2dYt3gGJMayaq93aX5hseO8Xgm8dte5REUE5pe+sNjD9MW7+eOC7eQVnjptMDzMGNQllnN6tGNkj3Z0bRvD55tT+Ne6Q+wrs22BmXf7janDk5g8JNGvGVK/mruJmUv3Mrx7PHMeHFvlsQczchn3/77AOfjHHedwSf/KWxI1kVdYzLUvL2HTwSzatrOdnZ4AAA3sSURBVIzkw4fH0a3dqS0vj8dxw9+WsnLPcc5IaMlnP7jglDe/YHh35T5+OXczBUUeYiLDefqawVw1LKnBXn/TwUyueukbij2O+y7sxc8nDgBg19ETXPr8lzgHL940nCuGdGmwmgJBYSAVKvk0CjCqZzv+fvs59TINdPmuNNbsy2Bgl1iGdo0jvmVUhcfNWXuAH767HoCrRyTx3HVD6/ypa+2+4/x89sbS2ScxkeEM7x7PyB7tOLdnO4Z1i69wANI5x9r9Gcxde5CPNxwmrcx+S4lxLXjvvvNOe1Mt//yxT3/Bocw8fnr5mTx4UZ9qa73p78tY8m0akwcn8tLNI2rx055ew4/fX8/sNQcJM5h516hKx4B2pGYz6YWvKSx2PHxxHx79rzPr/Pq1kVdYzP98tJm3V3gHtHsktOTlW8+mf+f6GUepSsmCsjCDDx4Yw4jubXl8zkbeXL6Prm1jWPToRQFtwTWEqsJAYwbN2PUju/Hp5hQGJsbyv1cNqrdPg6N6Jfg12DdteFf2p+fy/PztzF5zkG5tW5ZucVFT2XmF/P6zbby+bC/OeT/Z335eD358WT+/9oAxM0Z0b8uI7m154oqBLN55jLlrD/LJphQOZ+Zx6/TlvH//mEr7bzcfyiodD5kwwL9P+deM6MqSb9OYn5xKZk4hcS3rFswzl+5ltm+rhJ9d3r/KyQB9O7Xhvgt68+LCnbzy1bdMGdalwdceHMzI5YE3Vpdu3Dd+QCeev2Fo0NapPHxJXz5PTmVrSjaPvr+e1+8eVbo48PvjeoZcEFRHLQNpVJxz/PSDDbzv+6V79rqhpTt8+vv8zzan8OSHm0nNygdgQGIsv7t6MMMCsBncit3p3Dp9OflFHgYkxvLOvaMrHN94fv52XvjPDnoktGThoxf51cI5mV/EyN8uIKegmN9MHcQto8+oU503/X0ZRR7H5CGJvHjj8GpryCss5r/++BV703IY2aNtndceZOYWcu/MVRzJzmdo1ziGdYtnWPe2DEhsQ3TEqR88Fu84xiNvr+F4TiFm8OhlZ/LAhb2DvvZh86FMrnrxG4o8ji5xLTiUmUdcTCRLHrskYFNaG5JaBhIyzIynrh7M4cw8Fu88xmOzNpAY14Kxfaqf4nooI5dfzd1cuuI3JjKcH07oy11jA/cp7tye7fjrLSO4d+ZqthzO4vuvrWTmXaNOm+q3wDeltGRjOn+0io5g4qBEZq05wKw1B2odBsmHsrjv9VUUeRxndmrD768d4lcNLSLD+c3UQdw6fQUr9xzn/dX7uWFk91rVUFTs4eG31pRec3v3sZP8y7etSVR4GAO7xDKsWzzDu8ezLy2HPyzYjsdB25aR/Ol7w0u36Qi2s7rE8cglffnDgu2lLb1bRncPySCoTtNq50iTEBkexl9uGcGZndpQ5HHc/8bqCheuOefYnprN60v38NBbaxj//JelQXDRmR34/IcXcO8FvQPenL+kfyeevW4o4F2w9dBbaygsc8W3A8dzSPYNzI/3s4uoRMlOpmv3ZbDhQM1bvpsOZnLTq8s4nlNIu1ZRvHLr2ZWub6jI+X07MHWYd1C0ZLvz2nhq3tbS6aC3n3cGkwcnkuSbjVVQ7GHd/gxmLNnDf7+zjufme4NgSNc4PnpkXKMJghIPXtybs7p4xyyiwsO4vQ5bTzRm6iaSRutQRi5TX/qGI9n5JMXHMPvBMaSfLGD5rjSW705nxe70UwZ2wbsg6ddTBjJ5cGK9T/l7bckenvzQe6GfqcO68Pz1wwgLM2Z8s5tff5RM25aRrHx8fI3CyONxXPTsIval59AqKpw/3DCMy86qfo988E7FvGX6cjJzC2nfOoq37hldq37/YyfyufS5L8nMLWTqsC41Xnvwzop9PDbbO1X1/gt789jE7xZuHcnOY92+DO86k/0ZbDiQSU5BETeM7MaTV54V9FlMldl55AQ/+WA9kwcn8v0GXFsQaJpNJCFr08FMrn9lKTkFxYQZVHSBtvatoxnVqx2jeyUwZWiXet0CuLw/LtjOHxfsAOCOMT148sqB3Dp9BYt3HuOaEV157vqhNf6emw5mcs/MVaX70/xoQj8evrhPlf3nGw5kcMury8nKK6J962jevmcUfeswAFz2Df2W0d351RVn+TXVd8XudG5+dRmFxY5L+3fkb7edU+U26cUeR35RcY1aL1J7CgMJaQu3HuH7M1eVLljrFBvNqJ4JpQHQq32roC38cc7xPx8lM2PJHgDuOb8n//zGu4Dv5VvO5vJB/n2qL+9Idh4PvLGmdNfYiYM68+x1Qyvsq163P4Nbpy8nO6+IDm2iefue0XXeIsHjcTz01ho+2eS9QtuI7vH89Zazq1z5uz89h6te8i5u69epNbMeGBP0q3fJqRQGEvJW7klnb1oO55zRljMSWjaqVZ8ej3c+f9nrN0RFhLH2lxPqNNCYX1TMrz/8bs59/85t+Nut59A94bv1DWv2Hef26SvIzi+iY5to3r53NL0DtL+Tx+N44YsdpS2fDm2i+cvNI0r3sirrRH4R1/51CVtTsmnbMpK5D407pU5pHHRxGwl5I3u049qzu9IjiK2AyoSFGc9cO4RL+ncsvW9cn/Z1nnESHRHOU9MG83+nDiIizNiaks2UlxbzzU7vwOzqvenc5guCzrEtePe+8wIWBOD9uX4wvh/Tbz+HNi0iOJqdz41/W8bMpXso+yHS43H88N11pbuh/uXmsxUEIUgtA5EAyS0o5s4ZK1i2K71OXUQVWbYrjQffXEP6yQLCw4w7x/Tg7RX7OFlQTGJcC96+ZzQ9ArSfVEV2HzvJfa+vYnvqCcC7QvypaYNpERnO7z/byksLvwXgqWmDuWlU7aajSv1TN5FIA/F4HKnZeSTG1WxTO38cOJ7DvTNXl05bBUiKj+Hte0Y3yCfxk/lF/HTWBv694TAAZ3WJZeqwJH7r2w319vPO4H+uGlTvdUjtKQxEmojcgmJ+8sF6Pt5wmKT4GN65d3SVeyQFmnOOv3+9i6c/2XrKzK5xfdoz486RTW6LhqZGYSDShDjnWH8gk94dWgVtts43O4/x8Fve7SN6JLTkXw+NrXQDQmk8FAYiEnCHMnKZt/EwVwzpQue4+rnYjASW9iYSkYDrEh8T0qtx5VTq4BMREYWBiIgoDEREBIWBiIigMBARERQGIiJC6K4z8AAWFxcX7FJEREJGZmYmgHPOndYQCNUwKMLbqsmq7tgKlCRIZuAqavZ0TgNL5zPwdE69YgGPc+60NWYhGQZ1YWYZABWtwJPa0TkNLJ3PwNM5rZ7GDERERGEgIiIKAxERQWEgIiIoDEREBIWBiIigMBAREZrhOgMRETmdWgYiIqIwEBERhYGIiNCMwsDMWpvZC2Z22MxyzWyVmU0Jdl2hwMy6mtmfzGyxmZ0wM2dmF1Vy7E1mtt7M8szsgJk9bWYtGrjkRs3MLjWzGWa2zcxyfOdptpkNruDYCWa2zPd/9oiZvWJm2l+nHDMbY2afmdlB3/+9o2b2hZlNrOBYndMKNJswAOYANwNPAJOBZGCOmU0KalWhoQ9wI3AC+E9lB5nZLcCbwDfAROAp4CFgRv2XGFLuB7oDf8B7nn7k+/tKMxtdcpAvcOcB+4ErgUeBKcC/zaw5/e76oy2wDfgxcDlwL5APzDOz75UcpHNaBedck78BkwAHTCtznwGLgS3Brq+x34CwMn+e6juXF5U7Jhw4DMwtd/89vuNHBfvnaCw3oGMF98UDx4FZZe5bAawtd/4n+M7nDcH+ORr7DYjA+6b/hc5p9bfmkoTT8O5jPrfkDuf9X/Aa0N/MBgarsFDgnPP4cdhooDPec1rWm0AhcE2g6wpVzrkjFdyXAewAugKYWRIwEni97Pl3zs0HDqLzWS3nXBHe3/tC0DmtTnMJg0FAcgVvahvKPC51U3ION5W90zmXA3yLznGVzKwD3nNUcv4qPJ8+G9H5rJCZhZlZhJl1MbP/Afrh7Y4DndMqNZcwSADSK7g/vczjUjcl57Cy86xzXAkzM+BveH8fn/XdrfNZO+/hbQkcBH4AXO+c+9T3mM5pFZpLGIC3T7A2j0nNVHYudY4r93u8YzH3O+e2lHtM57Nmfgqci3dQeB7wnpndWO4YndMKNJcwSKPi1G/n+1rRJwWpmTTf18rOs85xBczst3hnwPy3c25GmYd0PmvBObfLObfSOfeRc+5G4DPgJd9MIZ3TKjSXMNgMDKhg6ljJvO6K+hClZjb7vp7S72pmLYHe6Byfxsz+F/gF8FPn3AvlHq7wfPoMRufTXyvwTjvtgM5plZpLGMzBO3XvynL33wZsc84lN3xJTc4yIAW4tdz9NwKRwOwGr6gRM7MngV8Cv3TO/b784865A8Aq4OayH2LM7FIgCZ3PavnGYi4CMoA0ndOqRQS7gAYyD1gITDezBGA3cDswDrgqmIWFCjO71vfHkb6vF5pZe+Ckc+4T51yRmT0GzDCzF4EPgAHA/wM+cM4ta/iqGycz+zHwa+BjYEHZhWZAvnNure/PPwM+B942s78BXfCez+XA+w1XceNnZm8Ce4HVwDEgEe/v+CXAI75ppqBzWrlgL3RoqBsQC7yI99NrHrAGmBrsukLlhndwraLbnnLH3YJ3ml4+3hkdzwAxwa6/Md2ARTU4n5fjfaPKA44CfwfaBvtnaGw34GFgKd5xgSLf18+AKys4Vue0gpuuZyAiIs1mzEBERKqgMBAREYWBiIgoDEREBIWBiIigMBARERQGIiKCwkBERFAYiIgI8P8BVwMsbnMZgxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load('./pneumonia_neg_1_feats/004fc013-89cd05df-e53d1c3b-7a82b0c2-87348252.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97748375, 0.88057345], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.item()['cls_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02251625, 0.97748375],\n",
       "       [0.11942655, 0.88057345]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = []\n",
    "for prob in f.item()['cls_prob']:\n",
    "    probs.append([1-prob, prob])\n",
    "np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02251625, 0.97748375],\n",
       "       [0.11942655, 0.88057345]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1-prob, prob] for prob in f.item()['cls_prob']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = './weights/transforming_embracement_leaky.pth'\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=8, Nx=6, d_model=512, ff_size=1024, dropout=0.1, task=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (self_attn_A): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (self_attn_B): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 512)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './weights/MATE_Pretrained_VLM.pth'\n",
    "model.load_state_dict(torch.load(PATH),strict=False)\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only hear to find original files\n",
    "df = pd.read_csv('mimic_outputs.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(mimic_feature_dataloader_test):\n",
    "        features = batch['features'].to(device)\n",
    "        spatials = batch['spatials'].to(device)\n",
    "        text = batch['text'].to(device)\n",
    "        labels = batch['y'].to(device)\n",
    "        img_attn_mask = batch['img_attn_mask'].to(device)\n",
    "        text_attn_mask = batch['text_attn_mask'].to(device)\n",
    "        \n",
    "        bboxs = batch['boxes']\n",
    "        image_id = batch['image_id']\n",
    "        width = batch['width']\n",
    "        height = batch['height']\n",
    "        if i > 3:\n",
    "            #model.to(device)\n",
    "            model.eval()\n",
    "            outputs = model([features, spatials], text, img_attn_mask, text_attn_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "#im_path = str(df[df['dicom_id'] == image_id[0]]['path'].values[0])\n",
    "#im = np.array(Image.open(im_path), dtype=np.uint8)\n",
    "\n",
    "def show_img():\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1,figsize=(10,10))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    max_val = max(head[token_num][idx])\n",
    "    for ix, (box, score) in enumerate(zip(bboxs[idx], head[token_num][idx])):\n",
    "        #print(box)\n",
    "        x0,y0, x1, y1 = box     \n",
    "        rect = patches.Rectangle((x0,y0),x1-x0,y1-y0,linewidth=3,edgecolor=cm.hot(score.item()/max_val),facecolor='none',label='float(score)', alpha = score.item()/max_val)\n",
    "        \n",
    "        ax.add_artist(rect)\n",
    "        rx, ry = rect.get_xy()\n",
    "        cx = rx + rect.get_width()/2.0\n",
    "        cy = ry + rect.get_height()/2.0\n",
    "        ax.annotate('{0:.2f}'.format(score.item()) , (cx, cy), color='w', weight='bold', \n",
    "                    fontsize=12, ha='center', va='center')     \n",
    "        \n",
    "        # Add the patch to the Axes\n",
    "        #ax.add_patch(rect)\n",
    "    fig.colorbar(cm.ScalarMappable(cmap=cm.hot), ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossModalLayer(\n",
       "  (cross_attn): MultiHeadedAttention(\n",
       "    (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (feed_forward): FeedForward(\n",
       "    (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sublayer): ModuleList(\n",
       "    (0): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "    (1): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crossmodal_B.cross_attn_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc3147e6b6043d5bdc024f23ea5096a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Image:', options=('Select Image', 0, 1, 2, 3, 4, 5, 6, 7, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "img_num = 0\n",
    "btn_clr = {'active':'lightgreen', 'inactive':'lightgray'}\n",
    "#text\n",
    "tokenizer = mimic_feature_dataset_test.tokenizer\n",
    "txt = tokenizer.decode(text[img_num])\n",
    "lookup = { v:k for (k,v) in enumerate(txt.split(' '))} \n",
    "\n",
    "#image\n",
    "im_path = str(df[df['dicom_id'] == image_id[0]]['path'].values[0])\n",
    "im = np.array(Image.open(im_path), dtype=np.uint8)\n",
    "global boxes, target, layer_num, head_num, head\n",
    "layer_num = 0\n",
    "batch_num = 0\n",
    "head_num = 0\n",
    "boxes = []#init_buttons(txt)\n",
    "bboxs = batch['boxes'][img_num]\n",
    "global token_num\n",
    "token_num = 0\n",
    "head = model.crossmodal_B.cross_attn_layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "\n",
    "#functions\n",
    "def dd_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        if change['new'] == 'Select Image':\n",
    "            return 0\n",
    "        global im, head, txt, lookup, boxes, bboxs, target, img_num\n",
    "        img_num = change['new']\n",
    "        im_path = str(df[df['dicom_id'] == image_id[img_num]]['path'].values[0])\n",
    "        im = np.array(Image.open(im_path), dtype=np.uint8)\n",
    "        head = model.crossmodal_B.cross_attn_layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "        txt = tokenizer.decode(batch['text'][img_num])\n",
    "        lookup = { v:k for (k,v) in enumerate(txt.split(' '))}\n",
    "        boxes = init_buttons(txt)\n",
    "        btn_container.children = boxes\n",
    "        bboxs = batch['boxes'][img_num]\n",
    "        target.value=\"Ground Truth: {}, Predicted: {}\".format(batch['y'][img_num], predicted[img_num])\n",
    "        with output:\n",
    "            clear_output()\n",
    "            show_img()\n",
    "            #display(target,widgets.GridBox(boxes, layout=widgets.Layout(grid_template_columns=\"repeat(10, auto)\")))\n",
    "def on_button_clicked(b):\n",
    "    global idx, boxes, token_num\n",
    "    tok_id = lookup[b.description]#looks up the word to get the token id        \n",
    "    token_num = tok_id\n",
    "    idx = (-head[token_num]).argsort().cpu()\n",
    "    clear_output()\n",
    "    #color red when selected!\n",
    "    for i in range(len(boxes)):\n",
    "        if boxes[i].description == b.description:\n",
    "            boxes[i].style.button_color = btn_clr['active']\n",
    "        else:\n",
    "            boxes[i].style.button_color = btn_clr['inactive']\n",
    "    btn_container.children = boxes\n",
    "    with output:\n",
    "        clear_output()\n",
    "        show_img()\n",
    "def init_buttons(txt):\n",
    "    #set up buttons\n",
    "    #text = set(txt.split(' '))\n",
    "    boxes = []\n",
    "    tokens = txt.split(' ')\n",
    "    stopwords = ['[PAD]', '[SEP]', '_', ':', '.', ]\n",
    "    tokens = set(tokens)\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    for token in tokens:\n",
    "        b = widgets.Button(\n",
    "            description=token,\n",
    "            layout=widgets.Layout(width='auto'))\n",
    "        b.style.button_color = 'lightgray'\n",
    "        b.on_click(on_button_clicked)\n",
    "        boxes.append(b)\n",
    "    return boxes\n",
    "\n",
    "def layerDD_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global layer_num, head, head_num\n",
    "        layer_num = change['new']\n",
    "        head = model.crossmodal_B.cross_attn_layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "        with output:\n",
    "            clear_output()\n",
    "            show_img()\n",
    "\n",
    "def headDD_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global head_num, head, layer_num\n",
    "        head_num = change['new']\n",
    "        head = model.crossmodal_B.cross_attn_layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "        with output:\n",
    "            clear_output()\n",
    "            show_img()\n",
    "#dropdown\n",
    "dd = widgets.Dropdown(\n",
    "    options=['Select Image']+[x for x in range(len(batch['y']))],\n",
    "    description='Image:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "layerDD = widgets.Dropdown(\n",
    "    options=[x for x in range(len(model.crossmodal_B.cross_attn_layers))],\n",
    "    description='Layer:',\n",
    "    disabled=False,\n",
    ")\n",
    "headDD = widgets.Dropdown(\n",
    "    options=[x for x in range(len(model.crossmodal_B.cross_attn_layers[layer_num].cross_attn.attn_scores[img_num]))],\n",
    "    description='Head:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "target = widgets.Label(value=\"Ground Truth: {}, Predicted: {}\".format(batch['y'][img_num], predicted[img_num])\n",
    ")\n",
    "#output\n",
    "output = widgets.Output()\n",
    "dd_container = widgets.HBox(children=[dd, layerDD, headDD], layout=widgets.Layout(width='100%'))\n",
    "btn_container = widgets.GridBox(boxes, layout=widgets.Layout(grid_template_columns=\"repeat(5, auto)\"))\n",
    "btn_fig_container = widgets.HBox(children=[btn_container, output], layout=widgets.Layout(height='700px'))\n",
    "container = widgets.VBox(children=[dd_container,target,btn_fig_container])\n",
    "\n",
    "#display\n",
    "display(container)\n",
    "\n",
    "dd.observe(dd_on_change,names = \"value\", type = \"change\")\n",
    "layerDD.observe(layerDD_on_change,names = \"value\", type = \"change\")\n",
    "headDD.observe(headDD_on_change,names = \"value\", type = \"change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(layer_num)\n",
    "print(head_num)\n",
    "print(img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9640, 0.6248, 0.0131, 0.8533, 0.7309, 0.8748, 0.1154, 0.6604, 0.5496,\n",
      "        0.2828], device='cuda:0')\n",
      "tensor([0.9864, 0.8722, 0.8620, 0.6079, 0.0801, 0.7999, 0.4206, 0.2023, 0.6078,\n",
      "        0.9604], device='cuda:0')\n",
      "tensor([[0.9864, 0.6248, 0.8620, 0.6079, 0.0801, 0.7999, 0.4206, 0.2023, 0.6078,\n",
      "         0.9604]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_ = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=2, Nx=2, d_model=10, ff_size=20)\n",
    "a = torch.rand(10).to('cuda')\n",
    "b = torch.rand(10).to('cuda')\n",
    "c = model_.embrace(a, b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 == 1\n",
      "doesnt work\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert 1 == 0, '1 == 1'\n",
    "    print('works')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('doesnt work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([0, 10, 3, 0], dtype=torch.float)\n",
    "torch.multinomial(weights, 2,replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6147, 0.3810])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6371, 0.4745])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnn",
   "language": "python",
   "name": "dlnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
