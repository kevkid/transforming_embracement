{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Embracement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 16:38:02.717482 47629321225088 file_utils.py:39] PyTorch version 1.5.1+cu101 available.\n"
     ]
    }
   ],
   "source": [
    "#prelims\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "#standard way to embed\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0., max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], #pe is 3 dimentional, so when we say self.pe[:, :x.size(1)] we set the x.size(1) to the rows of pe, but if x.size(1) is the column shape it will be wrong\n",
    "                             requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_size = ff_size\n",
    "        self.fc1 = nn.Linear(self.d_model, self.ff_size)\n",
    "        self.fc2 = nn.Linear(self.ff_size, self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.leaky_relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    '''\n",
    "    We can think of attention as having a word (query) which we want to look up and find results for.\n",
    "    K and V are considered our \"memory\" where we are looking for the most similar key to the query\n",
    "    Once we find the most similar key, we get its value. Our \"memory\" is all of the previously generated words\n",
    "    so when we pass in x,x,x we are DOT multiplying first word in x (q1) by all words in key, then get a scalar for each,\n",
    "    then pass all scalars through softmax to get a score, then matmul by V to get the average attention vector for all words in x\n",
    "    https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\n",
    "    '''\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h#size of model\n",
    "        self.h = h#number of heads\n",
    "        #could also be d_model, (n_heads * d_k)\n",
    "        self.Wq = nn.Linear(d_model,d_model)\n",
    "        self.Wk = nn.Linear(d_model,d_model)\n",
    "        self.Wv = nn.Linear(d_model,d_model)\n",
    "        self.out = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn_scores = None\n",
    "    def forward(self,q,k,v, mask=None):\n",
    "        '''Figure 2 of paper'''\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        nbatches = q.size(0)\n",
    "        #1)Do all the linear projections in batch from d_model => h x d_k \n",
    "        #project entire model dimentions into h heads\n",
    "        query = self.Wq(q).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        key = self.Wk(k).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        value = self.Wv(v).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        #2) Apply attention on all the projected vectors in batch. \n",
    "        z = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        z = z.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)#a little confused about what contiguous does\n",
    "        return self.out(z)\n",
    "    def attention(self, q,k,v, mask=None, dropout=None):\n",
    "        'Scaled dot product attention'\n",
    "        d_k = q.size(-1)#dimension of each head. We brok original input of 512 into 8 heads where each head is 64\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)#transpose rows and col, where r => c and c=> r\n",
    "        if mask is not None:#ultra important! as we should not attend to padding\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        '''a vector of scores, I believe it softmaxes across keys (words in translation) using same query. \n",
    "        meaning, it will do softmax for each key for every query to get how much attention we must pay to each word'''\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "        self.attn_scores = attn_scores\n",
    "        if dropout is not None:\n",
    "            attn_scores = dropout(attn_scores)\n",
    "        \n",
    "        return torch.matmul(attn_scores, v)# ,attn_scores #spits out z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.!!!\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This layer just calculates self attention + add & norm. This is the first 2 sub blocks of the encoder\n",
    "'''\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, dropout):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.size = size\n",
    "        self.sublayer = SublayerConnection(size, dropout)\n",
    "    def forward(self, x, mask):#this will be our own mask\n",
    "        x = self.sublayer(x, lambda x: self.self_attn(x, x, x, mask))#first half, multihead attn + residual + sum. The lambda is for passing in the layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Modal Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This layer calculates cross attention + add & norm + feedforward + add & norm. This is the Last 4 sub blocks of the decoder\n",
    "'''\n",
    "class CrossModalLayer(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    def __init__(self, size, cross_attn, feed_forward, dropout):\n",
    "        super(CrossModalLayer, self).__init__()\n",
    "        self.cross_attn = cross_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)#these sublayer connections are to create residual connections and layer normalization + dropout\n",
    "        self.size = size\n",
    "    #memory here is the key/val from another modality, opp_mod_mask: opposite modality mask\n",
    "    def forward(self, x, memory, opp_mod_mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.cross_attn(x, memory, memory, opp_mod_mask))# src_mask == encoder-decoder attn\n",
    "        x = self.sublayer[1](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModal(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(CrossModal, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, opp_mod_mask):#the memory here refers to the output of the encoder.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, opp_mod_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embracement(nn.Module):\n",
    "    def __init__(self, size, p):\n",
    "        super(Embracement, self).__init__()\n",
    "        self.c = size\n",
    "        self.p = nn.Parameter(torch.tensor(p))\n",
    "    def forward(self,X_A, X_B):\n",
    "        #make our random vector of size c and use it as a mask\n",
    "        #r = torch.multinomial(torch.tensor(self.p), self.c, replacement=True).float().to(device)\n",
    "        r = torch.multinomial(self.p, self.c, replacement=True).float().to(device)\n",
    "        r1 = (r == 0.).float().clone().detach().to(device)\n",
    "        r2 = (r == 1.).float().clone().detach().to(device)\n",
    "        #Here we will filter the values\n",
    "        d_prime1 = X_A*r1\n",
    "        d_prime2 = X_B*r2\n",
    "        d_prime = d_prime1 + d_prime2\n",
    "        #multiply the 3 vectors together\n",
    "        d_prime = d_prime * X_A * X_B\n",
    "        x = d_prime.view(-1, self.c)\n",
    "        #x = F.softmax(self.fc1(x), dim=0)\n",
    "        return x#this is our z for both modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminalNetwork(nn.Module):#output of model\n",
    "    \"Output layer + softmax\"\n",
    "    def __init__(self, d_model, n_out, dropout = 0.1):#takes in z size, outputs number of classes\n",
    "        super(TerminalNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(d_model, int(d_model/2))\n",
    "        self.out = nn.Linear(int(d_model/2), n_out)#shape of embedding -> vocab size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = LayerNorm(int(d_model/2))\n",
    "    def forward(self, x):\n",
    "        return self.out(self.dropout(self.layernorm(F.leaky_relu(self.hidden(x)))))\n",
    "        #return self.out(x)#F.softmax(self.out(x), dim=-1)#output, Softmax is only needed for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertImageEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from image, spatial location (omit now) and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v_feature_size, v_hidden_size, hidden_dropout_prob):\n",
    "        super(BertImageEmbeddings, self).__init__()\n",
    "\n",
    "        self.image_embeddings = nn.Linear(v_feature_size, v_hidden_size)\n",
    "        self.image_location_embeddings = nn.Linear(5, v_hidden_size)\n",
    "        self.LayerNorm = LayerNorm(v_hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, input_loc):\n",
    "\n",
    "        img_embeddings = self.image_embeddings(input_ids)\n",
    "        loc_embeddings = self.image_location_embeddings(input_loc)\n",
    "\n",
    "        # TODO: we want to make the padding_idx == 0, however, with custom initilization, it seems it will have a bias.\n",
    "        # Let's do masking for now\n",
    "        embeddings = self.LayerNorm(img_embeddings + loc_embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from vilbert\n",
    "class BertTextPooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(BertTextPooler, self).__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertImagePooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(BertImagePooler, self).__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this is a nonsequential model, will have two cross modal blocks running at same time\n",
    "I have to share the vector after the self attention portion, maybe within the model I get:\n",
    "2 inputs, each input passes through its own self-attention -> add+norm, then get each vector, and pass it as\n",
    "eachothers key & val as its 'memory', may have to make the module for the crossmodal block to have a \n",
    "self attention method and a cross attention module\n",
    "'''\n",
    "\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    \"Full model definition\"\n",
    "    def __init__(self, self_attn_A, crossmodal_A, self_attn_B, crossmodal_B, embracement, terminal, embed_image, embed_text, img_pooler, txt_pooler):\n",
    "        super(MultimodalTransformer, self).__init__()\n",
    "        #cross modal transformer sub blocks\n",
    "        self.self_attn_A = self_attn_A\n",
    "        self.self_attn_B = self_attn_B\n",
    "        self.crossmodal_A = crossmodal_A\n",
    "        self.crossmodal_B = crossmodal_B\n",
    "        self.embrace = embracement\n",
    "        self.terminal = terminal\n",
    "        self.embed_image = embed_image\n",
    "        self.embed_text = embed_text\n",
    "        self.img_pooler = img_pooler\n",
    "        self.txt_pooler = txt_pooler\n",
    "        \n",
    "    def forward(self, x_a, x_b, X_A_mask, X_B_mask):#a and b are modalities A and B\n",
    "        #perform self attention on modalities A and B\n",
    "        \n",
    "        a = self.embed_image(*x_a)\n",
    "        b = self.embed_text(x_b)\n",
    "        X_A = self.self_attn_A(a, X_A_mask)#remove Masks?!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        X_B = self.self_attn_B(b, X_B_mask)\n",
    "        #perform cross modal attention\n",
    "        X_A = self.crossmodal_A(X_A, X_B, X_B_mask)#modality A gets modality B as memory, no masks?\n",
    "        X_B = self.crossmodal_B(X_B, X_A, X_A_mask)#modality B gets modality A as memory\n",
    "        X_A = self.img_pooler(X_A)\n",
    "        X_B = self.txt_pooler(X_B)\n",
    "        #X_A = torch.mean(X_A, axis=1)\n",
    "        #X_B = torch.mean(X_B, axis=1)#text input, maybe just use a special token?\n",
    "        Z = self.embrace(X_A, X_B)#our embraced vector\n",
    "        #Z = X_A*X_B#just multiply them\n",
    "        return self.terminal(Z)#give us output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(d_model = 128, ff_size = 128, src_vocab = 2000, n_out = 2, n_head = 1, Nx = 1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_head,d_model)\n",
    "    ff = FeedForward(d_model, ff_size=ff_size)\n",
    "    self_attn_A = SelfAttentionLayer(d_model, c(attn), 0.1)\n",
    "    crossmodal_A = CrossModal(CrossModalLayer(d_model, c(attn), c(ff), dropout=0.1),Nx)\n",
    "    \n",
    "    self_attn_B = SelfAttentionLayer(d_model, c(attn), 0.1)\n",
    "    crossmodal_B = CrossModal(CrossModalLayer(d_model, c(attn), c(ff), dropout=0.1),Nx)\n",
    "    \n",
    "    tn = TerminalNetwork(d_model, n_out)\n",
    "    embracement = Embracement(d_model, [0.50,0.50])\n",
    "    position = PositionalEncoder(d_model=d_model, dropout=0.1)#max_seq_len must be larger than the longest sequence!\n",
    "    #embeddings are placed in sequential nn so we can train them, we also place the positional encoder so when we pass the input -> emb -> Pos\n",
    "    embed_B = nn.Sequential(Embedder(src_vocab, d_model),c(position))#torch.rand(200, d_model)#this is temporary\n",
    "    embed_A = BertImageEmbeddings(v_feature_size=1024, v_hidden_size=d_model, hidden_dropout_prob=0.1)#v_feature_size is fixed due to feature extractor\n",
    "    \n",
    "    txt_pooler = BertTextPooler(d_model)\n",
    "    img_pooler = BertImagePooler(d_model)\n",
    "    model = MultimodalTransformer(self_attn_A, crossmodal_A, self_attn_B, crossmodal_B, embracement,\n",
    "                                  tn, embed_image=embed_A, embed_text=embed_B, img_pooler = img_pooler, txt_pooler=txt_pooler)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Datasets (features must be extracted using the \"RSNA Pneumonia\" notebook from MIMIC-CXR-JPG prior to using):\n",
    " - MIMIC-CXR-JPG: https://physionet.org/content/mimic-cxr-jpg/2.0.0/\n",
    " - MIMIC-CXR (reports): https://physionet.org/content/mimic-cxr/2.0.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data comes from feature extractor & splits. Feature extractor gives features in .npy file (features, bbox, spatials, class, probs) and \n",
    "csv file containing:\n",
    "    dicom_id\n",
    "    feature_path\n",
    "    report\n",
    "    report_path\n",
    "    Pneumonia/not Pneumonia\n",
    "\n",
    "'''\n",
    "#root dirs\n",
    "root_dir = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/'\n",
    "mimic_loc = '/home/kl533/krauthammer_partition/mimic-cxr-jpg/mimic-cxr-jpg-2.0.0.physionet.org/'\n",
    "#load csv into DF\n",
    "df = pd.read_csv(os.path.join(root_dir, 'img_infos.csv'), index_col=0)\n",
    "splits = pd.read_csv(os.path.join(mimic_loc, \"mimic-cxr-2.0.0-split.csv\"))\n",
    "\n",
    "splits = splits[['dicom_id','split']]\n",
    "df_splits = pd.merge(df, splits, on='dicom_id')\n",
    "train = df_splits[df_splits['split']=='train']\n",
    "validate = df_splits[df_splits['split']=='validate']\n",
    "test = df_splits[df_splits['split']=='test']\n",
    "\n",
    "#save dataframes into train, valid, test\n",
    "train.to_csv('/home/kl533/krauthammer_partition/Weights/transforming_embracement/img_infos_TRAIN.csv')\n",
    "validate.to_csv('/home/kl533/krauthammer_partition/Weights/transforming_embracement/img_infos_VALIDATE.csv')\n",
    "test.to_csv('/home/kl533/krauthammer_partition/Weights/transforming_embracement/img_infos_TEST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset will be coming from already extracted features in .npy files.\n",
    "    Args:\n",
    "        csv_file (string): Contains information about original image and captions\n",
    "        root_dir (string): location to .npy files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, max_seq_length=128, max_image_feats=128):\n",
    "        self.max_image_feats = max_image_feats\n",
    "        self.max_seq_length = max_seq_length#max length of tokens\n",
    "        self.dataset_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.img_feats = pd.DataFrame(list(Path(root_dir).rglob(\"*.npy\")), columns=['feature_path'])#list of features per image\n",
    "        self.img_feats['feature_path'] = self.img_feats['feature_path'].apply(lambda x: str(x))\n",
    "        self.img_feats['dicom_id'] = self.img_feats['feature_path'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "        self.dataset_frame = pd.merge(self.dataset_frame, self.img_feats, on='dicom_id')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "        #one hot encode\n",
    "        #target = self.dataset_frame.y\n",
    "        self.n_classes = self.dataset_frame.y.nunique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #load features from disk\n",
    "        features = np.load(self.dataset_frame.iloc[idx]['feature_path'], allow_pickle=True).item()\n",
    "        tokenized = self.tokenizer(text=self.dataset_frame.iloc[idx]['report'], add_special_tokens=True, max_length=self.max_seq_length,truncation=True,pad_to_max_length=True)\n",
    "        text = torch.tensor(tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        text_attn_mask = torch.tensor(tokenized[\"attention_mask\"])\n",
    "        #text_attn_mask = torch.stack([torch.tensor(text_attn_mask)], dim=0).squeeze(0)\n",
    "        image_id = features['image_id']\n",
    "        image_w = features['image_width']\n",
    "        image_h = features['image_height']\n",
    "        feature = torch.from_numpy(features['features'])\n",
    "        num_boxes = features['num_boxes']\n",
    "        g_feat = torch.sum(feature, dim=0) / num_boxes\n",
    "        g_box = torch.tensor([[0.,0.,0.,0.]])\n",
    "        num_boxes = num_boxes + 1\n",
    "        feature = torch.cat([g_feat.view(1,-1), feature], dim=0)\n",
    "        boxes = torch.from_numpy(features['bbox'])\n",
    "        boxes = torch.cat([g_box,boxes])\n",
    "        image_location = np.zeros((boxes.shape[0], 5), dtype=np.float32)\n",
    "        image_location[:,:4] = boxes\n",
    "        image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "        image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "        image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "        image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "        image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "        g_location = np.array([0,0,1,1,1])\n",
    "        image_location = np.concatenate([np.expand_dims(g_location, axis=0), image_location], axis=0)\n",
    "        image_mask = [1] * (int(num_boxes))\n",
    "        \n",
    "        \n",
    "        #the stacked features probably need to be padded!!!!!!!\n",
    "        #padding added\n",
    "        img_feat_padding = torch.zeros((self.max_image_feats-feature.size(0)),feature.size(1))\n",
    "        img_attn_mask = torch.tensor([1]*len(feature) + [0]*len(img_feat_padding))\n",
    "        #img_attn_mask = torch.stack([torch.tensor(img_attn_mask)], dim=0).squeeze(0)\n",
    "        #print('img_attn_mask')\n",
    "        #print(img_attn_mask)\n",
    "        feature = torch.cat([feature, img_feat_padding], axis=0)\n",
    "        features = torch.stack([feature], dim=0).float().squeeze(0)\n",
    "        location = torch.tensor(image_location).float()\n",
    "        location = torch.cat([location, torch.zeros(self.max_image_feats-location.size(0),location.size(1))], axis=0)\n",
    "        #print(location)\n",
    "        spatials = torch.stack([location], dim=0).float().squeeze(0)\n",
    "        #image_mask = torch.stack([torch.tensor(image_mask)], dim=0).byte()\n",
    "        bboxes = torch.cat([boxes, torch.zeros(self.max_image_feats-boxes.size(0),boxes.size(1))], axis=0)\n",
    "        bboxes = torch.stack([bboxes], dim=0).float().squeeze(0)\n",
    "        y = self.dataset_frame.iloc[idx]['y']\n",
    "        output = {'features': features, 'spatials': spatials, 'text': text, 'boxes': bboxes, \n",
    "                  'image_id':image_id,'width':image_w, 'height':image_h, 'text_attn_mask':text_attn_mask, 'img_attn_mask':img_attn_mask,'y':y}\n",
    "\n",
    "        return output#features, spatials, text, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 16:38:15.424336 47629321225088 tokenization_utils_base.py:1233] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kl533/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0720 16:38:15.824843 47629321225088 tokenization_utils_base.py:1233] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kl533/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0720 16:38:16.297834 47629321225088 tokenization_utils_base.py:1233] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kl533/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "train_csv = os.path.join(root_dir, 'img_infos_TRAIN.csv')\n",
    "valid_csv = os.path.join(root_dir, 'img_infos_VALIDATE.csv')\n",
    "test_csv = os.path.join(root_dir, 'img_infos_TEST.csv')\n",
    "\n",
    "mimic_feature_dataset_train = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=train_csv,root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_train = DataLoader(mimic_feature_dataset_train, batch_size=24, shuffle=True, num_workers=4)\n",
    "\n",
    "mimic_feature_dataset_valid = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=valid_csv, root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_valid = DataLoader(mimic_feature_dataset_valid, batch_size=24, shuffle=False, num_workers=4)\n",
    "\n",
    "mimic_feature_dataset_test = MIMIC_CXR_JPG_BBOX_FEATURES_Dataset(csv_file=test_csv, root_dir=root_dir, max_seq_length=512, max_image_feats=10)\n",
    "mimic_feature_dataloader_test = DataLoader(mimic_feature_dataset_test, batch_size=24, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a model with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=8, Nx=6, d_model=512, ff_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model if you want\n",
    "#net = Net()\n",
    "#net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.5000, 0.5000], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embrace.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attn_score = torch.rand(2,2,5,5)\\nattn_mask = torch.tensor([[1,1,1,0,0],[1,1,0,0,0]])\\nprint(attn_score.size())\\nprint(attn_score[0][0])\\nattn_mask = attn_mask.unsqueeze(1).unsqueeze(2)\\nprint(attn_score.size())\\n### Cross attention\\ncross_attn_score = torch.rand(2,2,10,5)#(batch, head, txt_tokens, img_tokens), this is in the text decoders block\\nimg_attn_mask = torch.tensor([[1,1,1,1,0],[1,0,0,0,0]])#b0: 4 feats, b1: 1 feat...\\nprint(cross_attn_score[0][0])\\nprint(cross_attn_score.size())\\nimg_attn_mask = img_attn_mask.unsqueeze(1).unsqueeze(2)\\ncross_attn_score.masked_fill(img_attn_mask == 0, -1e9)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''attn_score = torch.rand(2,2,5,5)\n",
    "attn_mask = torch.tensor([[1,1,1,0,0],[1,1,0,0,0]])\n",
    "print(attn_score.size())\n",
    "print(attn_score[0][0])\n",
    "attn_mask = attn_mask.unsqueeze(1).unsqueeze(2)\n",
    "print(attn_score.size())\n",
    "### Cross attention\n",
    "cross_attn_score = torch.rand(2,2,10,5)#(batch, head, txt_tokens, img_tokens), this is in the text decoders block\n",
    "img_attn_mask = torch.tensor([[1,1,1,1,0],[1,0,0,0,0]])#b0: 4 feats, b1: 1 feat...\n",
    "print(cross_attn_score[0][0])\n",
    "print(cross_attn_score.size())\n",
    "img_attn_mask = img_attn_mask.unsqueeze(1).unsqueeze(2)\n",
    "cross_attn_score.masked_fill(img_attn_mask == 0, -1e9)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Find learning rate. Not implemented yet\\nfrom torch_lr_finder import LRFinder\\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\\nlr_finder.range_test(mimic_feature_dataloader_train, val_loader=mimic_feature_dataloader_valid, end_lr=1, num_iter=100, step_mode=\"linear\")\\nlr_finder.plot(log_lr=False)\\nlr_finder.reset()'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Find learning rate. Not implemented yet\n",
    "from torch_lr_finder import LRFinder\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(mimic_feature_dataloader_train, val_loader=mimic_feature_dataloader_valid, end_lr=1, num_iter=100, step_mode=\"linear\")\n",
    "lr_finder.plot(log_lr=False)\n",
    "lr_finder.reset()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.917\n",
      "[1,   200] loss: 0.681\n",
      "[1,   300] loss: 0.507\n",
      "[1,   400] loss: 0.359\n",
      "[1,   500] loss: 0.339\n",
      "[1,   600] loss: 0.298\n",
      "[1,   700] loss: 0.310\n",
      "[1,   800] loss: 0.283\n",
      "[1,   900] loss: 0.282\n",
      "[1,  1000] loss: 0.257\n",
      "[1,  1100] loss: 0.247\n",
      "[1,  1200] loss: 0.247\n",
      "Validation loss: 0.013646476902067661\n",
      "[2,   100] loss: 0.222\n",
      "[2,   200] loss: 0.219\n",
      "[2,   300] loss: 0.213\n",
      "[2,   400] loss: 0.227\n",
      "[2,   500] loss: 0.206\n",
      "[2,   600] loss: 0.200\n",
      "[2,   700] loss: 0.195\n",
      "[2,   800] loss: 0.194\n",
      "[2,   900] loss: 0.228\n",
      "[2,  1000] loss: 0.201\n",
      "[2,  1100] loss: 0.184\n",
      "[2,  1200] loss: 0.190\n",
      "Validation loss: 0.009639862924814224\n",
      "[3,   100] loss: 0.164\n",
      "[3,   200] loss: 0.139\n",
      "[3,   300] loss: 0.165\n",
      "[3,   400] loss: 0.162\n",
      "[3,   500] loss: 0.172\n",
      "[3,   600] loss: 0.157\n",
      "[3,   700] loss: 0.176\n",
      "[3,   800] loss: 0.160\n",
      "[3,   900] loss: 0.169\n",
      "[3,  1000] loss: 0.177\n",
      "[3,  1100] loss: 0.141\n",
      "[3,  1200] loss: 0.160\n",
      "Validation loss: 0.01092276070266962\n",
      "[4,   100] loss: 0.127\n",
      "[4,   200] loss: 0.134\n",
      "[4,   300] loss: 0.117\n",
      "[4,   400] loss: 0.143\n",
      "[4,   500] loss: 0.126\n",
      "[4,   600] loss: 0.136\n",
      "[4,   700] loss: 0.124\n",
      "[4,   800] loss: 0.140\n",
      "[4,   900] loss: 0.130\n",
      "[4,  1000] loss: 0.114\n",
      "[4,  1100] loss: 0.134\n",
      "[4,  1200] loss: 0.127\n",
      "Validation loss: 0.016113370656967163\n",
      "[5,   100] loss: 0.095\n",
      "[5,   200] loss: 0.085\n",
      "[5,   300] loss: 0.090\n",
      "[5,   400] loss: 0.094\n",
      "[5,   500] loss: 0.101\n",
      "[5,   600] loss: 0.116\n",
      "[5,   700] loss: 0.101\n",
      "[5,   800] loss: 0.091\n",
      "[5,   900] loss: 0.121\n",
      "[5,  1000] loss: 0.106\n",
      "[5,  1100] loss: 0.109\n",
      "[5,  1200] loss: 0.108\n",
      "Validation loss: 0.012131469324231148\n",
      "Epoch     5: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[6,   100] loss: 0.078\n",
      "[6,   200] loss: 0.054\n",
      "[6,   300] loss: 0.062\n",
      "[6,   400] loss: 0.047\n",
      "[6,   500] loss: 0.057\n",
      "[6,   600] loss: 0.067\n",
      "[6,   700] loss: 0.060\n",
      "[6,   800] loss: 0.061\n",
      "[6,   900] loss: 0.053\n",
      "[6,  1000] loss: 0.061\n",
      "[6,  1100] loss: 0.056\n",
      "[6,  1200] loss: 0.053\n",
      "Validation loss: 0.01518023107200861\n",
      "[7,   100] loss: 0.040\n",
      "[7,   200] loss: 0.040\n",
      "[7,   300] loss: 0.047\n",
      "[7,   400] loss: 0.055\n",
      "[7,   500] loss: 0.042\n",
      "[7,   600] loss: 0.057\n",
      "[7,   700] loss: 0.052\n",
      "[7,   800] loss: 0.052\n",
      "[7,   900] loss: 0.040\n",
      "[7,  1000] loss: 0.044\n",
      "[7,  1100] loss: 0.046\n",
      "[7,  1200] loss: 0.056\n",
      "Validation loss: 0.015416808426380157\n",
      "[8,   100] loss: 0.044\n",
      "[8,   200] loss: 0.045\n",
      "[8,   300] loss: 0.040\n",
      "[8,   400] loss: 0.040\n",
      "[8,   500] loss: 0.046\n",
      "[8,   600] loss: 0.045\n",
      "[8,   700] loss: 0.026\n",
      "[8,   800] loss: 0.043\n",
      "[8,   900] loss: 0.027\n",
      "[8,  1000] loss: 0.059\n",
      "[8,  1100] loss: 0.036\n",
      "[8,  1200] loss: 0.052\n",
      "Validation loss: 0.016067788004875183\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-06.\n",
      "[9,   100] loss: 0.039\n",
      "[9,   200] loss: 0.043\n",
      "[9,   300] loss: 0.037\n",
      "[9,   400] loss: 0.044\n",
      "[9,   500] loss: 0.031\n",
      "[9,   600] loss: 0.023\n",
      "[9,   700] loss: 0.033\n",
      "[9,   800] loss: 0.038\n",
      "[9,   900] loss: 0.031\n",
      "[9,  1000] loss: 0.037\n",
      "[9,  1100] loss: 0.042\n",
      "[9,  1200] loss: 0.037\n",
      "Validation loss: 0.018317004665732384\n",
      "[10,   100] loss: 0.026\n",
      "[10,   200] loss: 0.034\n",
      "[10,   300] loss: 0.036\n",
      "[10,   400] loss: 0.038\n",
      "[10,   500] loss: 0.034\n",
      "[10,   600] loss: 0.039\n",
      "[10,   700] loss: 0.027\n",
      "[10,   800] loss: 0.039\n",
      "[10,   900] loss: 0.033\n",
      "[10,  1000] loss: 0.037\n",
      "[10,  1100] loss: 0.046\n",
      "[10,  1200] loss: 0.031\n",
      "Validation loss: 0.01595245860517025\n",
      "[11,   100] loss: 0.034\n",
      "[11,   200] loss: 0.034\n",
      "[11,   300] loss: 0.028\n",
      "[11,   400] loss: 0.026\n",
      "[11,   500] loss: 0.034\n",
      "[11,   600] loss: 0.041\n",
      "[11,   700] loss: 0.037\n",
      "[11,   800] loss: 0.046\n",
      "[11,   900] loss: 0.050\n",
      "[11,  1000] loss: 0.023\n",
      "[11,  1100] loss: 0.033\n",
      "[11,  1200] loss: 0.026\n",
      "Validation loss: 0.019914474338293076\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-07.\n",
      "[12,   100] loss: 0.026\n",
      "[12,   200] loss: 0.033\n",
      "[12,   300] loss: 0.033\n",
      "[12,   400] loss: 0.030\n",
      "[12,   500] loss: 0.042\n",
      "[12,   600] loss: 0.026\n",
      "[12,   700] loss: 0.026\n",
      "[12,   800] loss: 0.034\n",
      "[12,   900] loss: 0.033\n",
      "[12,  1000] loss: 0.031\n",
      "[12,  1100] loss: 0.045\n",
      "[12,  1200] loss: 0.038\n",
      "Validation loss: 0.01852874644100666\n",
      "[13,   100] loss: 0.035\n",
      "[13,   200] loss: 0.037\n",
      "[13,   300] loss: 0.036\n",
      "[13,   400] loss: 0.031\n",
      "[13,   500] loss: 0.042\n",
      "[13,   600] loss: 0.038\n",
      "[13,   700] loss: 0.028\n",
      "[13,   800] loss: 0.050\n",
      "[13,   900] loss: 0.032\n",
      "[13,  1000] loss: 0.027\n",
      "[13,  1100] loss: 0.024\n",
      "[13,  1200] loss: 0.032\n",
      "Validation loss: 0.01963713951408863\n",
      "[14,   100] loss: 0.033\n",
      "[14,   200] loss: 0.035\n",
      "[14,   300] loss: 0.043\n",
      "[14,   400] loss: 0.036\n",
      "[14,   500] loss: 0.024\n",
      "[14,   600] loss: 0.035\n",
      "[14,   700] loss: 0.041\n",
      "[14,   800] loss: 0.033\n",
      "[14,   900] loss: 0.041\n",
      "[14,  1000] loss: 0.029\n",
      "[14,  1100] loss: 0.028\n",
      "[14,  1200] loss: 0.035\n",
      "Validation loss: 0.018723972141742706\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-08.\n",
      "[15,   100] loss: 0.031\n",
      "[15,   200] loss: 0.034\n",
      "[15,   300] loss: 0.033\n",
      "[15,   400] loss: 0.026\n",
      "[15,   500] loss: 0.033\n",
      "[15,   600] loss: 0.032\n",
      "[15,   700] loss: 0.041\n",
      "[15,   800] loss: 0.039\n",
      "[15,   900] loss: 0.041\n",
      "[15,  1000] loss: 0.024\n",
      "[15,  1100] loss: 0.020\n",
      "[15,  1200] loss: 0.050\n",
      "Validation loss: 0.017484141513705254\n",
      "[16,   100] loss: 0.036\n",
      "[16,   200] loss: 0.033\n",
      "[16,   300] loss: 0.040\n",
      "[16,   400] loss: 0.051\n",
      "[16,   500] loss: 0.026\n",
      "[16,   600] loss: 0.033\n",
      "[16,   700] loss: 0.039\n",
      "[16,   800] loss: 0.036\n",
      "[16,   900] loss: 0.025\n",
      "[16,  1000] loss: 0.035\n",
      "[16,  1100] loss: 0.043\n",
      "[16,  1200] loss: 0.021\n",
      "Validation loss: 0.01937282457947731\n",
      "[17,   100] loss: 0.031\n",
      "[17,   200] loss: 0.048\n",
      "[17,   300] loss: 0.031\n",
      "[17,   400] loss: 0.036\n",
      "[17,   500] loss: 0.025\n",
      "[17,   600] loss: 0.040\n",
      "[17,   700] loss: 0.036\n",
      "[17,   800] loss: 0.035\n",
      "[17,   900] loss: 0.026\n",
      "[17,  1000] loss: 0.032\n",
      "[17,  1100] loss: 0.031\n",
      "[17,  1200] loss: 0.040\n",
      "Validation loss: 0.019874393939971924\n",
      "[18,   100] loss: 0.030\n",
      "[18,   200] loss: 0.040\n",
      "[18,   300] loss: 0.044\n",
      "[18,   400] loss: 0.041\n",
      "[18,   500] loss: 0.032\n",
      "[18,   600] loss: 0.039\n",
      "[18,   700] loss: 0.032\n",
      "[18,   800] loss: 0.032\n",
      "[18,   900] loss: 0.033\n",
      "[18,  1000] loss: 0.037\n",
      "[18,  1100] loss: 0.031\n",
      "[18,  1200] loss: 0.024\n",
      "Validation loss: 0.017946213483810425\n",
      "[19,   100] loss: 0.026\n",
      "[19,   200] loss: 0.033\n",
      "[19,   300] loss: 0.035\n",
      "[19,   400] loss: 0.038\n",
      "[19,   500] loss: 0.036\n",
      "[19,   600] loss: 0.035\n",
      "[19,   700] loss: 0.040\n",
      "[19,   800] loss: 0.024\n",
      "[19,   900] loss: 0.037\n",
      "[19,  1000] loss: 0.035\n",
      "[19,  1100] loss: 0.037\n",
      "[19,  1200] loss: 0.028\n",
      "Validation loss: 0.019389649853110313\n",
      "[20,   100] loss: 0.031\n",
      "[20,   200] loss: 0.025\n",
      "[20,   300] loss: 0.026\n",
      "[20,   400] loss: 0.035\n",
      "[20,   500] loss: 0.029\n",
      "[20,   600] loss: 0.039\n",
      "[20,   700] loss: 0.031\n",
      "[20,   800] loss: 0.040\n",
      "[20,   900] loss: 0.034\n",
      "[20,  1000] loss: 0.036\n",
      "[20,  1100] loss: 0.030\n",
      "[20,  1200] loss: 0.040\n",
      "Validation loss: 0.01867940090596676\n",
      "[21,   100] loss: 0.038\n",
      "[21,   200] loss: 0.035\n",
      "[21,   300] loss: 0.022\n",
      "[21,   400] loss: 0.038\n",
      "[21,   500] loss: 0.036\n",
      "[21,   600] loss: 0.030\n",
      "[21,   700] loss: 0.036\n",
      "[21,   800] loss: 0.038\n",
      "[21,   900] loss: 0.036\n",
      "[21,  1000] loss: 0.040\n",
      "[21,  1100] loss: 0.032\n",
      "[21,  1200] loss: 0.019\n",
      "Validation loss: 0.01858624629676342\n",
      "[22,   100] loss: 0.023\n",
      "[22,   200] loss: 0.035\n",
      "[22,   300] loss: 0.043\n",
      "[22,   400] loss: 0.031\n",
      "[22,   500] loss: 0.040\n",
      "[22,   600] loss: 0.026\n",
      "[22,   700] loss: 0.032\n",
      "[22,   800] loss: 0.036\n",
      "[22,   900] loss: 0.044\n",
      "[22,  1000] loss: 0.038\n",
      "[22,  1100] loss: 0.032\n",
      "[22,  1200] loss: 0.035\n",
      "Validation loss: 0.019277218729257584\n",
      "[23,   100] loss: 0.034\n",
      "[23,   200] loss: 0.036\n",
      "[23,   300] loss: 0.034\n",
      "[23,   400] loss: 0.033\n",
      "[23,   500] loss: 0.023\n",
      "[23,   600] loss: 0.034\n",
      "[23,   700] loss: 0.039\n",
      "[23,   800] loss: 0.036\n",
      "[23,   900] loss: 0.035\n",
      "[23,  1000] loss: 0.035\n",
      "[23,  1100] loss: 0.035\n",
      "[23,  1200] loss: 0.039\n",
      "Validation loss: 0.018020983785390854\n",
      "[24,   100] loss: 0.037\n",
      "[24,   200] loss: 0.029\n",
      "[24,   300] loss: 0.045\n",
      "[24,   400] loss: 0.032\n",
      "[24,   500] loss: 0.029\n",
      "[24,   600] loss: 0.039\n",
      "[24,   700] loss: 0.037\n",
      "[24,   800] loss: 0.030\n",
      "[24,   900] loss: 0.038\n",
      "[24,  1000] loss: 0.041\n",
      "[24,  1100] loss: 0.028\n",
      "[24,  1200] loss: 0.025\n",
      "Validation loss: 0.019329721108078957\n",
      "[25,   100] loss: 0.037\n",
      "[25,   200] loss: 0.032\n",
      "[25,   300] loss: 0.031\n",
      "[25,   400] loss: 0.028\n",
      "[25,   500] loss: 0.026\n",
      "[25,   600] loss: 0.033\n",
      "[25,   700] loss: 0.036\n",
      "[25,   800] loss: 0.040\n",
      "[25,   900] loss: 0.040\n",
      "[25,  1000] loss: 0.030\n",
      "[25,  1100] loss: 0.038\n",
      "[25,  1200] loss: 0.032\n",
      "Validation loss: 0.01980837807059288\n",
      "[26,   100] loss: 0.028\n",
      "[26,   200] loss: 0.033\n",
      "[26,   300] loss: 0.050\n",
      "[26,   400] loss: 0.034\n",
      "[26,   500] loss: 0.023\n",
      "[26,   600] loss: 0.042\n",
      "[26,   700] loss: 0.034\n",
      "[26,   800] loss: 0.030\n",
      "[26,   900] loss: 0.033\n",
      "[26,  1000] loss: 0.038\n",
      "[26,  1100] loss: 0.031\n",
      "[26,  1200] loss: 0.028\n",
      "Validation loss: 0.018846716731786728\n",
      "[27,   100] loss: 0.027\n",
      "[27,   200] loss: 0.031\n",
      "[27,   300] loss: 0.030\n",
      "[27,   400] loss: 0.050\n",
      "[27,   500] loss: 0.027\n",
      "[27,   600] loss: 0.032\n",
      "[27,   700] loss: 0.032\n",
      "[27,   800] loss: 0.032\n",
      "[27,   900] loss: 0.035\n",
      "[27,  1000] loss: 0.035\n",
      "[27,  1100] loss: 0.044\n",
      "[27,  1200] loss: 0.025\n",
      "Validation loss: 0.018971597775816917\n",
      "[28,   100] loss: 0.035\n",
      "[28,   200] loss: 0.030\n",
      "[28,   300] loss: 0.034\n",
      "[28,   400] loss: 0.038\n",
      "[28,   500] loss: 0.022\n",
      "[28,   600] loss: 0.034\n",
      "[28,   700] loss: 0.024\n",
      "[28,   800] loss: 0.041\n",
      "[28,   900] loss: 0.032\n",
      "[28,  1000] loss: 0.029\n",
      "[28,  1100] loss: 0.046\n",
      "[28,  1200] loss: 0.037\n",
      "Validation loss: 0.019463393837213516\n",
      "[29,   100] loss: 0.026\n",
      "[29,   200] loss: 0.027\n",
      "[29,   300] loss: 0.041\n",
      "[29,   400] loss: 0.041\n",
      "[29,   500] loss: 0.027\n",
      "[29,   600] loss: 0.032\n",
      "[29,   700] loss: 0.037\n",
      "[29,   800] loss: 0.024\n",
      "[29,   900] loss: 0.033\n",
      "[29,  1000] loss: 0.041\n",
      "[29,  1100] loss: 0.037\n",
      "[29,  1200] loss: 0.035\n",
      "Validation loss: 0.01619752123951912\n",
      "[30,   100] loss: 0.047\n",
      "[30,   200] loss: 0.042\n",
      "[30,   300] loss: 0.033\n",
      "[30,   400] loss: 0.028\n",
      "[30,   500] loss: 0.039\n",
      "[30,   600] loss: 0.029\n",
      "[30,   700] loss: 0.027\n",
      "[30,   800] loss: 0.026\n",
      "[30,   900] loss: 0.031\n",
      "[30,  1000] loss: 0.031\n",
      "[30,  1100] loss: 0.034\n",
      "[30,  1200] loss: 0.039\n",
      "Validation loss: 0.01777382381260395\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2)\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    torch.cuda.empty_cache()\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    for i, data in enumerate(mimic_feature_dataloader_train, 0):\n",
    "        # get the inputs;\n",
    "        features = data['features'].to(device)\n",
    "        spatials = data['spatials'].to(device)\n",
    "        text = data['text'].to(device)\n",
    "        labels = data['y'].to(device)\n",
    "        img_attn_mask = data['img_attn_mask'].to(device)\n",
    "        text_attn_mask = data['text_attn_mask'].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model([features, spatials], text, img_attn_mask, text_attn_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    #Check point each epoch\n",
    "    PATH = '/home/kl533/krauthammer_partition/Weights/transforming_embracement/weights/transforming_embracement_leaky_varEMB_epoch_{}.pth'.format(epoch)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_data in mimic_feature_dataloader_valid:\n",
    "            val_features = val_data['features'].to(device)\n",
    "            val_spatials = val_data['spatials'].to(device)\n",
    "            val_text = val_data['text'].to(device)\n",
    "            val_labels = val_data['y'].to(device)\n",
    "            val_img_attn_mask = val_data['img_attn_mask'].to(device)\n",
    "            val_text_attn_mask = val_data['text_attn_mask'].to(device)\n",
    "            val_outputs = model([val_features, val_spatials], val_text, val_img_attn_mask, val_text_attn_mask)\n",
    "            val_loss += criterion(val_outputs, val_labels)\n",
    "    #reduce LR\n",
    "    print('Validation loss: {}'.format(val_loss / len(mimic_feature_dataset_valid)))\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (self_attn_A): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (self_attn_B): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 512)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load weights\n",
    "PATH = './weights/transforming_embracement_multiplied_leaky.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 491 test samples: 89.20570264765784\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_list = []\n",
    "output_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in mimic_feature_dataloader_test:\n",
    "        features = data['features'].to(device)\n",
    "        spatials = data['spatials'].to(device)\n",
    "        text = data['text'].to(device)\n",
    "        labels = data['y'].to(device)\n",
    "        img_attn_mask = data['img_attn_mask'].to(device)\n",
    "        text_attn_mask = data['text_attn_mask'].to(device)\n",
    "        outputs = model([features, spatials], text,img_attn_mask, text_attn_mask)\n",
    "        #outputs = model([torch.rand_like(features), torch.rand_like(spatials)], text, img_attn_mask, text_attn_mask)#noise\n",
    "        #outputs = model([features, spatials], torch.randint_like(text, 30000), img_attn_mask, text_attn_mask)\n",
    "        output_list.extend(outputs.tolist())\n",
    "        label_list.extend(labels.tolist())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the {} test samples: {}'.format(len(mimic_feature_dataset_test), (100 * correct / total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe = OneHotEncoder()\n",
    "#label_list = ohe.fit_transform(np.asarray(label_list).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(label_list, np.asarray(output_list)[:,1])\n",
    "AUC = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507211538461539"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Micro AUC because there is only one class!\n",
    "plt.figure(figsize=(10,10))\n",
    "lw = 5\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % AUC)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('MATE ROC w/ Multiplied Hidden states for Pneumonia')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       192\n",
      "           1       0.92      0.90      0.91       299\n",
      "\n",
      "    accuracy                           0.89       491\n",
      "   macro avg       0.89      0.89      0.89       491\n",
      "weighted avg       0.89      0.89      0.89       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_list, np.argmax(F.softmax(torch.as_tensor(output_list), dim=1), axis=1).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.918367\n"
     ]
    }
   ],
   "source": [
    "precision = metrics.precision_score(label_list, np.argmax(output_list, axis=1))\n",
    "print('Precision: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9183673469387755, 0.903010033444816, 0.9106239460370994, None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(label_list, np.argmax(output_list, axis=1), average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(zip(np.argmax(output_list, axis=1), label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = counts[1,1]\n",
    "tn = counts[0,0]\n",
    "fp = counts[1,0]\n",
    "fn = counts[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9183673469387755\n",
      "0.903010033444816\n",
      "0.9106239460370994\n"
     ]
    }
   ],
   "source": [
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*((precision*recall)/(precision+recall))\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = F.softmax(torch.as_tensor(output_list),dim=1)[:,1].tolist()\n",
    "plt.hist(x, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = './weights/transforming_embracement_leaky.pth'\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (self_attn_A): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (self_attn_B): SelfAttentionLayer(\n",
       "    (self_attn): MultiHeadedAttention(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (sublayer): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (crossmodal_A): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (crossmodal_B): CrossModal(\n",
       "    (layers): ModuleList(\n",
       "      (0): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CrossModalLayer(\n",
       "        (cross_attn): MultiHeadedAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (embrace): Embracement()\n",
       "  (terminal): TerminalNetwork(\n",
       "    (hidden): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layernorm): LayerNorm()\n",
       "  )\n",
       "  (embed_image): BertImageEmbeddings(\n",
       "    (image_embeddings): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (image_location_embeddings): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (LayerNorm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embed_text): Sequential(\n",
       "    (0): Embedder(\n",
       "      (embed): Embedding(30522, 512)\n",
       "    )\n",
       "    (1): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (img_pooler): BertImagePooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (txt_pooler): BertTextPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './weights/transforming_NO_embracement_multiplied_leaky.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only hear to find original files\n",
    "df = pd.read_csv('mimic_outputs.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(mimic_feature_dataloader_test):\n",
    "        features = batch['features'].to(device)\n",
    "        spatials = batch['spatials'].to(device)\n",
    "        text = batch['text'].to(device)\n",
    "        labels = batch['y'].to(device)\n",
    "        img_attn_mask = batch['img_attn_mask'].to(device)\n",
    "        text_attn_mask = batch['text_attn_mask'].to(device)\n",
    "        \n",
    "        bboxs = batch['boxes']\n",
    "        image_id = batch['image_id']\n",
    "        width = batch['width']\n",
    "        height = batch['height']\n",
    "        if i > 3:\n",
    "            #model.to(device)\n",
    "            model.eval()\n",
    "            outputs = model([features, spatials], text, img_attn_mask, text_attn_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "#im_path = str(df[df['dicom_id'] == image_id[0]]['path'].values[0])\n",
    "#im = np.array(Image.open(im_path), dtype=np.uint8)\n",
    "\n",
    "def show_img():\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1,figsize=(10,10))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    max_val = max(head[token_num][idx])\n",
    "    for ix, (box, score) in enumerate(zip(bboxs[idx], head[token_num][idx])):\n",
    "        #print(box)\n",
    "        x0,y0, x1, y1 = box     \n",
    "        rect = patches.Rectangle((x0,y0),x1-x0,y1-y0,linewidth=3,edgecolor=cm.hot(score.item()/max_val),facecolor='none',label='float(score)', alpha = score.item()/max_val)\n",
    "        \n",
    "        ax.add_artist(rect)\n",
    "        rx, ry = rect.get_xy()\n",
    "        cx = rx + rect.get_width()/2.0\n",
    "        cy = ry + rect.get_height()/2.0\n",
    "        ax.annotate('{0:.2f}'.format(score.item()) , (cx, cy), color='w', weight='bold', \n",
    "                    fontsize=12, ha='center', va='center')     \n",
    "        \n",
    "        # Add the patch to the Axes\n",
    "        #ax.add_patch(rect)\n",
    "    fig.colorbar(cm.ScalarMappable(cmap=cm.hot), ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef4e1e59b2d4f149cc1e815ec5af146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Image:', options=('Select Image', 0, 1, 2, 3, 4, 5, 6, 7, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "img_num = 0\n",
    "btn_clr = {'active':'lightgreen', 'inactive':'lightgray'}\n",
    "#text\n",
    "tokenizer = mimic_feature_dataset_test.tokenizer\n",
    "txt = tokenizer.decode(text[img_num])\n",
    "lookup = { v:k for (k,v) in enumerate(txt.split(' '))} \n",
    "\n",
    "#image\n",
    "im_path = str(df[df['dicom_id'] == image_id[0]]['path'].values[0])\n",
    "im = np.array(Image.open(im_path), dtype=np.uint8)\n",
    "global boxes, target, layer_num, head_num, head\n",
    "layer_num = 0\n",
    "batch_num = 0\n",
    "head_num = 0\n",
    "boxes = []#init_buttons(txt)\n",
    "bboxs = batch['boxes'][img_num]\n",
    "global token_num\n",
    "token_num = 0\n",
    "head = model.crossmodal_B.layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "\n",
    "#functions\n",
    "def dd_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        if change['new'] == 'Select Image':\n",
    "            return 0\n",
    "        global im, head, txt, lookup, boxes, bboxs, target, img_num\n",
    "        img_num = change['new']\n",
    "        im_path = str(df[df['dicom_id'] == image_id[img_num]]['path'].values[0])\n",
    "        im = np.array(Image.open(im_path), dtype=np.uint8)\n",
    "        head = model.crossmodal_B.layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "        txt = tokenizer.decode(batch['text'][img_num])\n",
    "        lookup = { v:k for (k,v) in enumerate(txt.split(' '))}\n",
    "        boxes = init_buttons(txt)\n",
    "        btn_container.children = boxes\n",
    "        bboxs = batch['boxes'][img_num]\n",
    "        target.value=\"Ground Truth: {}, Predicted: {}\".format(batch['y'][img_num], predicted[img_num])\n",
    "        with output:\n",
    "            clear_output()\n",
    "            show_img()\n",
    "            #display(target,widgets.GridBox(boxes, layout=widgets.Layout(grid_template_columns=\"repeat(10, auto)\")))\n",
    "def on_button_clicked(b):\n",
    "    global idx, boxes, token_num\n",
    "    tok_id = lookup[b.description]#looks up the word to get the token id        \n",
    "    token_num = tok_id\n",
    "    idx = (-head[token_num]).argsort().cpu()\n",
    "    clear_output()\n",
    "    #color red when selected!\n",
    "    for i in range(len(boxes)):\n",
    "        if boxes[i].description == b.description:\n",
    "            boxes[i].style.button_color = btn_clr['active']\n",
    "        else:\n",
    "            boxes[i].style.button_color = btn_clr['inactive']\n",
    "    btn_container.children = boxes\n",
    "    with output:\n",
    "        clear_output()\n",
    "        show_img()\n",
    "def init_buttons(txt):\n",
    "    #set up buttons\n",
    "    #text = set(txt.split(' '))\n",
    "    boxes = []\n",
    "    tokens = txt.split(' ')\n",
    "    stopwords = ['[PAD]', '[SEP]', '_', ':', '.', ]\n",
    "    tokens = set(tokens)\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    for token in tokens:\n",
    "        b = widgets.Button(\n",
    "            description=token,\n",
    "            layout=widgets.Layout(width='auto'))\n",
    "        b.style.button_color = 'lightgray'\n",
    "        b.on_click(on_button_clicked)\n",
    "        boxes.append(b)\n",
    "    return boxes\n",
    "\n",
    "def layerDD_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global layer_num, head, head_num\n",
    "        layer_num = change['new']\n",
    "        head = model.crossmodal_B.layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "        with output:\n",
    "            clear_output()\n",
    "            show_img()\n",
    "\n",
    "def headDD_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global head_num, head, layer_num\n",
    "        head_num = change['new']\n",
    "        head = model.crossmodal_B.layers[layer_num].cross_attn.attn_scores[img_num][head_num].cpu()\n",
    "        with output:\n",
    "            clear_output()\n",
    "            show_img()\n",
    "#dropdown\n",
    "dd = widgets.Dropdown(\n",
    "    options=['Select Image']+[x for x in range(len(batch['y']))],\n",
    "    description='Image:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "layerDD = widgets.Dropdown(\n",
    "    options=[x for x in range(len(model.crossmodal_B.layers))],\n",
    "    description='Layer:',\n",
    "    disabled=False,\n",
    ")\n",
    "headDD = widgets.Dropdown(\n",
    "    options=[x for x in range(len(model.crossmodal_B.layers[layer_num].cross_attn.attn_scores[img_num]))],\n",
    "    description='Head:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "target = widgets.Label(value=\"Ground Truth: {}, Predicted: {}\".format(batch['y'][img_num], predicted[img_num])\n",
    ")\n",
    "#output\n",
    "output = widgets.Output()\n",
    "dd_container = widgets.HBox(children=[dd, layerDD, headDD], layout=widgets.Layout(width='100%'))\n",
    "btn_container = widgets.GridBox(boxes, layout=widgets.Layout(grid_template_columns=\"repeat(5, auto)\"))\n",
    "btn_fig_container = widgets.HBox(children=[btn_container, output], layout=widgets.Layout(height='700px'))\n",
    "container = widgets.VBox(children=[dd_container,target,btn_fig_container])\n",
    "\n",
    "#display\n",
    "display(container)\n",
    "\n",
    "dd.observe(dd_on_change,names = \"value\", type = \"change\")\n",
    "layerDD.observe(layerDD_on_change,names = \"value\", type = \"change\")\n",
    "headDD.observe(headDD_on_change,names = \"value\", type = \"change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(layer_num)\n",
    "print(head_num)\n",
    "print(img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9640, 0.6248, 0.0131, 0.8533, 0.7309, 0.8748, 0.1154, 0.6604, 0.5496,\n",
      "        0.2828], device='cuda:0')\n",
      "tensor([0.9864, 0.8722, 0.8620, 0.6079, 0.0801, 0.7999, 0.4206, 0.2023, 0.6078,\n",
      "        0.9604], device='cuda:0')\n",
      "tensor([[0.9864, 0.6248, 0.8620, 0.6079, 0.0801, 0.7999, 0.4206, 0.2023, 0.6078,\n",
      "         0.9604]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_ = gen_model(src_vocab=mimic_feature_dataset_train.tokenizer.vocab_size, n_head=2, Nx=2, d_model=10, ff_size=20)\n",
    "a = torch.rand(10).to('cuda')\n",
    "b = torch.rand(10).to('cuda')\n",
    "c = model_.embrace(a, b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 == 1\n",
      "doesnt work\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert 1 == 0, '1 == 1'\n",
    "    print('works')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('doesnt work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnn",
   "language": "python",
   "name": "dlnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
